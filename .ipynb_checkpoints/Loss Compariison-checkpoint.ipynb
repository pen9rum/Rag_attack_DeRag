{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b1e090-96f9-44b6-aa71-b6f80afb880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install datasets transformers scipy tqdm --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76921e-aa9b-4840-af8b-789921005fe7",
   "metadata": {},
   "source": [
    "\n",
    "**MS MARCO & SciFact & NQ**\n",
    "\n",
    "This notebook presents an ablation study on DeRAG attacks using Differential Evolution (DE) under a controlled setting. Specifically, we evaluate how suffix length (`L ∈ {1..10}`) and retrieval depth (`Top-K ∈ {1, 10}`) affect the effectiveness of adversarial prompt suffixes.\n",
    "\n",
    "###  Experiment Setup\n",
    "\n",
    "- **Corpus**: Random sample of 1,000 documents per dataset\n",
    "- **Queries**: 120 filtered questions per dataset (tokens ∈ [20, 500], must contain `?`)\n",
    "- **Target Document**: Document ranked **800th** under baseline retrieval\n",
    "- **Encoder**: `bert-base-uncased` (evaluated in `fp16` on GPU)\n",
    "- **Suffix**: Appended at the end of the query, of length L ∈ [1, 10]\n",
    " \n",
    "\n",
    "Each query is paired with one incorrect target passage from the 800th position. The goal is to use a DE-optimized suffix to **promote** that document into the **Top-K** positions. Suffixes are optimized using **hinge loss** over cosine similarity between query and document embeddings.\n",
    "\n",
    "###  Evaluation Metrics\n",
    "For each `(query, L, K)` setting, we report:\n",
    "- **Suffix tokens** (`suffix`, `suffix_len`)\n",
    "- **Final rank of target document** (`final_rank`)\n",
    "- **Change in cosine similarity** (`Δcos`)\n",
    "- **Change in nDCG** (`ΔnDCG@K`)\n",
    "- **Success rate** (whether `rank ≤ K`)\n",
    "- **Iteration count** (used by DE optimizer)\n",
    "\n",
    "###  Output\n",
    "Results are saved to:\n",
    "```bash\n",
    "exp_results/{dataset}/ablation/ablation_results.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dbf489b-5eac-4f05-8eca-878016d2f99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "msmarco-CLS: 100%|██████████████████████████████████████████████████████████████████████| 63/63 [03:13<00:00,  3.08s/it]\n",
      "msmarco  K=1 L=1: 100%|███████████████████████████████████████████████████████████████| 120/120 [12:51<00:00,  6.43s/it]\n",
      "msmarco  K=1 L=2: 100%|███████████████████████████████████████████████████████████████| 120/120 [31:14<00:00, 15.62s/it]\n",
      "msmarco  K=1 L=3: 100%|███████████████████████████████████████████████████████████████| 120/120 [45:53<00:00, 22.94s/it]\n",
      "msmarco  K=1 L=4: 100%|███████████████████████████████████████████████████████████████| 120/120 [57:14<00:00, 28.62s/it]\n",
      "msmarco  K=1 L=5: 100%|█████████████████████████████████████████████████████████████| 120/120 [1:12:20<00:00, 36.17s/it]\n",
      "msmarco  K=1 L=6: 100%|█████████████████████████████████████████████████████████████| 120/120 [1:31:35<00:00, 45.79s/it]\n",
      "msmarco  K=1 L=7: 100%|█████████████████████████████████████████████████████████████| 120/120 [1:50:54<00:00, 55.45s/it]\n",
      "msmarco  K=1 L=8: 100%|█████████████████████████████████████████████████████████████| 120/120 [1:59:57<00:00, 59.98s/it]\n",
      "msmarco  K=1 L=9: 100%|█████████████████████████████████████████████████████████████| 120/120 [2:21:17<00:00, 70.65s/it]\n",
      "msmarco  K=1 L=10: 100%|████████████████████████████████████████████████████████████| 120/120 [2:36:31<00:00, 78.26s/it]\n",
      "msmarco  K=10 L=1: 100%|██████████████████████████████████████████████████████████████| 120/120 [11:55<00:00,  5.96s/it]\n",
      "msmarco  K=10 L=2: 100%|██████████████████████████████████████████████████████████████| 120/120 [29:17<00:00, 14.65s/it]\n",
      "msmarco  K=10 L=3: 100%|██████████████████████████████████████████████████████████████| 120/120 [44:31<00:00, 22.26s/it]\n",
      "msmarco  K=10 L=4: 100%|██████████████████████████████████████████████████████████████| 120/120 [54:55<00:00, 27.46s/it]\n",
      "msmarco  K=10 L=5: 100%|████████████████████████████████████████████████████████████| 120/120 [1:08:30<00:00, 34.26s/it]\n",
      "msmarco  K=10 L=6: 100%|████████████████████████████████████████████████████████████| 120/120 [1:27:29<00:00, 43.74s/it]\n",
      "msmarco  K=10 L=7: 100%|████████████████████████████████████████████████████████████| 120/120 [1:42:49<00:00, 51.41s/it]\n",
      "msmarco  K=10 L=8: 100%|████████████████████████████████████████████████████████████| 120/120 [1:53:37<00:00, 56.81s/it]\n",
      "msmarco  K=10 L=9: 100%|████████████████████████████████████████████████████████████| 120/120 [2:08:37<00:00, 64.31s/it]\n",
      "msmarco  K=10 L=10: 100%|███████████████████████████████████████████████████████████| 120/120 [2:23:16<00:00, 71.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ msmarco ablation 完成 — rows = 2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, random, math, json, warnings, numpy as np, pandas as pd, torch, tqdm\n",
    "from datasets     import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED             = 42\n",
    "DEVICE           = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_DOCS, N_Q      = 1_000, 120\n",
    "TAIL_MAX         = 10\n",
    "RANK_TARGET      = 800\n",
    "POP, MAXITER     = 20, 2_000\n",
    "PATIENCE         = 20\n",
    "BATCH_CLS        = 16          \n",
    "DATASETS         = [\"msmarco\",\"fiqa\",\"NQ\"]   \n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "tok  = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "VOC  = tok.vocab_size\n",
    "\n",
    "\n",
    "def enc(txts, dev):\n",
    "    return tok(txts, padding=\"max_length\", truncation=True,\n",
    "               max_length=512, return_tensors=\"pt\").to(dev)\n",
    "\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1)\n",
    "\n",
    "def dcg(rank:int)->float:\n",
    "    return 1 / math.log2(rank + 1)\n",
    "\n",
    "def is_good_q(t:str)->bool:\n",
    "    return \"?\" in t and 20 < len(tok(t)[\"input_ids\"])-2 < 500\n",
    "\n",
    "def suffix_str(ids):            # pretty print suffix\n",
    "    return \" \".join(tok.convert_ids_to_tokens(ids))\n",
    "\n",
    "def de_opt(ids, msk, CP, tgt_cls, L, topk):\n",
    "    pos, bounds = list(range(512-L,512)), [(0,VOC-1)]*L\n",
    "    def loss_from_cls(c):\n",
    "        kth = torch.topk(cos_row(c,CP),topk).values[-1]\n",
    "        sim = torch.nn.functional.cosine_similarity(c,tgt_cls)[0]\n",
    "        return max(0., (kth-sim).item())\n",
    "    def obj(v):\n",
    "        v=[int(round(x)) for x in v]\n",
    "        p,m=ids.clone(),msk.clone(); m[pos]=1\n",
    "        for i,t in zip(pos,v): p[i]=t\n",
    "        cls=bert(input_ids=p.unsqueeze(0),\n",
    "                 attention_mask=m.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "        return loss_from_cls(cls)\n",
    "    best,stale=1e9,0\n",
    "    def cb(xk,_):\n",
    "        nonlocal best,stale\n",
    "        cur=obj(xk)\n",
    "        if cur<best: best,stale=cur,0\n",
    "        else: stale+=1\n",
    "        return cur==0 or stale>=PATIENCE\n",
    "    res=differential_evolution(obj,bounds,popsize=POP,maxiter=MAXITER,\n",
    "                               tol=0,polish=False,seed=SEED,callback=cb)\n",
    "    suf=[int(round(x)) for x in res.x]\n",
    "    p,m=ids.clone(),msk.clone(); m[pos]=1\n",
    "    for i,t in zip(pos,suf): p[i]=t\n",
    "    cls=bert(input_ids=p.unsqueeze(0),\n",
    "             attention_mask=m.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "    return suf,cls,res.nfev\n",
    "\n",
    "def prepare(ds:str):\n",
    "    corpus  = load_dataset(f\"BeIR/{ds}\", \"corpus\",  split=\"corpus\")\n",
    "    queries = load_dataset(f\"BeIR/{ds}\", \"queries\", split=\"queries\")\n",
    "    docs = random.sample(list(corpus), N_DOCS)\n",
    "\n",
    "    pool = [q for q in queries if is_good_q(q[\"text\"])]\n",
    "    if len(pool) < N_Q:\n",
    "        raise ValueError(f\"{ds}: less than 120\")\n",
    "    qs = random.sample(pool, N_Q)\n",
    "\n",
    "    cpu_bert = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(\"cpu\")\n",
    "    CLS=[]\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(0,N_DOCS,BATCH_CLS), desc=f\"{ds}-CLS\"):\n",
    "            batch = [d[\"text\"] for d in docs[i:i+BATCH_CLS]]\n",
    "            CLS.append(cpu_bert(**enc(batch,\"cpu\")).last_hidden_state[:,0,:])\n",
    "    C_CLS = torch.cat(CLS)\n",
    "    del cpu_bert; torch.cuda.empty_cache()\n",
    "\n",
    "    return docs, qs, C_CLS\n",
    "\n",
    "def run_ablation(ds:str):\n",
    "    out_dir = f\"exp_results/{ds}/ablation\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    docs, qs, C_CLS = prepare(ds)\n",
    "\n",
    "    global bert\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\",\n",
    "                                     torch_dtype=torch.float16\n",
    "                                    ).to(DEVICE).eval()\n",
    "\n",
    "    rec=[]\n",
    "    for K in (1,10):\n",
    "        for L in range(1, TAIL_MAX+1):\n",
    "            for q in tqdm.tqdm(qs, desc=f\"{ds}  K={K} L={L}\"):\n",
    "                qtxt = q[\"text\"]\n",
    "                ids,msk = enc([qtxt],DEVICE)[\"input_ids\"][0], enc([qtxt],DEVICE)[\"attention_mask\"][0]\n",
    "                with torch.no_grad():\n",
    "                    qcls = bert(**enc([qtxt],DEVICE)).last_hidden_state[:,0,:]\n",
    "\n",
    "                sims = cos_row(qcls.cpu(), C_CLS)\n",
    "                order = torch.argsort(sims, descending=True)\n",
    "                tgt   = int(order[RANK_TARGET-1])\n",
    "                tgt_txt = docs[tgt][\"text\"]\n",
    "                if len(tok(tgt_txt)[\"input_ids\"]) > 510: \n",
    "                    continue\n",
    "\n",
    "                baseline_sim = sims[tgt].item()\n",
    "                orig_top1    = docs[int(order[0])][\"text\"]\n",
    "                tgt_cls      = C_CLS[tgt:tgt+1].to(DEVICE)\n",
    "                CP           = C_CLS[[i for i in range(N_DOCS) if i != tgt]]\n",
    "\n",
    "                suf, cls, it = de_opt(ids, msk, CP, tgt_cls, L, K)\n",
    "                new_sims = cos_row(cls.cpu(), C_CLS)\n",
    "                fr = (new_sims > new_sims[tgt]).sum().item() + 1\n",
    "\n",
    "                rec.append(dict(dataset=ds, topK=K, L=L,\n",
    "                                query=qtxt,\n",
    "                                target_excerpt=tgt_txt[:120].replace(\"\\n\",\" \"),\n",
    "                                orig_top1_excerpt=orig_top1[:120].replace(\"\\n\",\" \"),\n",
    "                                suffix=suffix_str(suf),\n",
    "                                suffix_len=len(suf),\n",
    "                                suffix_token_ids=json.dumps(suf),\n",
    "                                baseline_rank=RANK_TARGET,\n",
    "                                final_rank=fr,\n",
    "                                delta_rank=RANK_TARGET-fr,\n",
    "                                delta_cos=new_sims[tgt].item()-baseline_sim,\n",
    "                                delta_ndcg=dcg(fr)-dcg(RANK_TARGET),\n",
    "                                success=int(fr<=K),\n",
    "                                iter_used=it))\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    pd.DataFrame(rec).to_csv(f\"{out_dir}/ablation_results.csv\",\n",
    "                             index=False, encoding=\"utf-8\")\n",
    "    print(f\"✓ {ds} ablation FINISHED — rows = {len(rec)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"exp_results\", exist_ok=True)\n",
    "    for ds in DATASETS:\n",
    "        run_ablation(ds)\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87fc529-d9d5-4449-acf8-fae139a7ec6c",
   "metadata": {},
   "source": [
    "#  DeRAG Loss Function Comparison\n",
    "\n",
    "This repository evaluates the effectiveness of **Differential Evolution (DE)** optimized tail-patch attacks under different **loss objectives** (`hinge` vs. `cosine`) across four BEIR datasets.\n",
    "\n",
    "##  Datasets\n",
    "\n",
    "- **MS MARCO** (Open-domain QA)\n",
    "- **FiQA** (Financial QA)\n",
    "- **SciFact** (Scientific Fact Verification)\n",
    "- **FEVER** (Fact Extraction and Verification)\n",
    "\n",
    "## Experiment Setup\n",
    "\n",
    "- **Corpus**: 1,000 documents per dataset\n",
    "- **Queries**: 100 filtered questions per dataset  \n",
    "  - Must contain a `?`\n",
    "  - Token length ∈ [20, 500]\n",
    "- **Target Document**: The **100th-ranked** document under baseline similarity (cosine).\n",
    "- **Model**: `bert-base-uncased`\n",
    "- **Device**: GPU with `float16` for inference\n",
    "- **Optimization**: Differential Evolution (DE)\n",
    "  - Population: 20\n",
    "  - Max Iterations: 2,000\n",
    "  - Suffix Length: `L = 5` tokens\n",
    "\n",
    "##  Objective\n",
    "\n",
    "For each `(query, target document)` pair, we use DE to generate a **5-token suffix** that is appended to the query to **promote the target document's rank**.\n",
    "\n",
    "We compare two loss modes:\n",
    "\n",
    "- `hinge`: Max-margin loss between target and top distractor\n",
    "- `cos`: Cosine similarity loss with target document only\n",
    "\n",
    "##  Evaluation Metrics\n",
    "\n",
    "For each query and loss mode:\n",
    "\n",
    "- `suffix` (token sequence)\n",
    "- `suffix_len`\n",
    "- `suffix_token_ids`\n",
    "- `baseline_rank` (always 100)\n",
    "- `final_rank`\n",
    "- `delta_rank`\n",
    "- `delta_cos`\n",
    "- `delta_nDCG`\n",
    "- `success` (whether final rank == 1)\n",
    "- `iter_used` (optimizer steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f649703-4847-4a8a-8206-70c8fbdb72a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Preparing FEVER ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS-fever: 100%|████████████████████████████████████████████████████████████████████████| 63/63 [03:01<00:00,  2.89s/it]\n",
      "fever-hinge: 100%|██████████████████████████████████████████████████████████████████| 100/100 [1:12:16<00:00, 43.37s/it]\n",
      "fever-cos: 100%|████████████████████████████████████████████████████████████████████| 100/100 [1:22:05<00:00, 49.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ fever done  –  hinge success = 26.00%,  cos success = 1.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_DOCS, N_Q  = 1_000, 100         \n",
    "TAIL_L       = 5                 \n",
    "RANK_TARGET  = 100              \n",
    "POP, MAXITER = 20, 2_000        \n",
    "PATIENCE     = 20                \n",
    "BATCH_CLS    = 16               \n",
    "\n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "VOC = tok.vocab_size \n",
    "\n",
    "def enc(txts, dev):\n",
    "    return tok(\n",
    "        txts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(dev)\n",
    "\n",
    "def get_ids_msk(text, dev):\n",
    "    e = enc([text], dev)\n",
    "    return e[\"input_ids\"][0], e[\"attention_mask\"][0]\n",
    "\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1\n",
    "    )\n",
    "\n",
    "def dcg(rank: int) -> float:\n",
    "    return 1.0 / math.log2(rank + 1)\n",
    "\n",
    "def is_good_q(text: str) -> bool:\n",
    "   \n",
    "    n = len(tok(text)[\"input_ids\"]) - 2\n",
    "    return (10 < n < 500)\n",
    "\n",
    "def suffix_str(ids):\n",
    "    return \" \".join(tok.convert_ids_to_tokens(ids))\n",
    "\n",
    "def de_opt(ids, msk, CP, tgt_cls, loss_mode=\"hinge\"):\n",
    "\n",
    "    L = TAIL_L\n",
    "    pos = list(range(512 - L, 512))\n",
    "    bounds = [(0, VOC - 1)] * L\n",
    "\n",
    "    def loss_from_cls(c):\n",
    "        # c: shape [1, dim]\n",
    "        if loss_mode == \"cos\":\n",
    "            # cos loss = - cosine_similarity(c, tgt_cls)\n",
    "            return -(torch.nn.functional.cosine_similarity(c, tgt_cls)[0]).item()\n",
    "        kth = torch.topk(cos_row(c, CP), 1).values[-1]\n",
    "        sim = torch.nn.functional.cosine_similarity(c, tgt_cls)[0]\n",
    "        return max(0.0, (kth - sim).item())\n",
    "\n",
    "    def obj(v):\n",
    "        v = [int(round(x)) for x in v]\n",
    "        p, m = ids.clone(), msk.clone()\n",
    "        for i, t in zip(pos, v):\n",
    "            p[i] = t\n",
    "        m[pos] = 1\n",
    "        cls = bert(input_ids=p.unsqueeze(0),\n",
    "                   attention_mask=m.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "        return loss_from_cls(cls)\n",
    "\n",
    "    best, stale = 1e9, 0\n",
    "    def cb(xk, _):\n",
    "        nonlocal best, stale\n",
    "        cur = obj(xk)\n",
    "        if cur < best:\n",
    "            best, stale = cur, 0\n",
    "        else:\n",
    "            stale += 1\n",
    "        return (cur == 0) or (stale >= PATIENCE)\n",
    "\n",
    "    res = differential_evolution(\n",
    "        obj,\n",
    "        bounds,\n",
    "        popsize=POP,\n",
    "        maxiter=MAXITER,\n",
    "        tol=0,\n",
    "        polish=False,\n",
    "        seed=SEED,\n",
    "        callback=cb\n",
    "    )\n",
    "    suf = [int(round(x)) for x in res.x]\n",
    "    p, m = ids.clone(), msk.clone()\n",
    "    m[pos] = 1\n",
    "    for i, t in zip(pos, suf):\n",
    "        p[i] = t\n",
    "    cls = bert(input_ids=p.unsqueeze(0),\n",
    "               attention_mask=m.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "    return suf, cls, res.nfev\n",
    "\n",
    "def load_qrels_fever():\n",
    "    qrels_all = load_dataset(\"BeIR/fever-qrels\", split=\"train\")\n",
    "    sample = qrels_all[0]\n",
    "    r_qid_key   = next((k for k in sample.keys() if \"query\" in k.lower()), None)\n",
    "    r_doc_key   = next((k for k in sample.keys() if \"corpus\" in k.lower() or \"doc\" in k.lower()), None)\n",
    "    r_score_key = next((k for k in sample.keys() if k.lower() in {\"score\",\"label\",\"relevance\"}), None)\n",
    "    if not (r_qid_key and r_doc_key and r_score_key):\n",
    "        raise RuntimeError(\"fever:No qrels。\")\n",
    "    pos_dict = {}\n",
    "    for r in qrels_all:\n",
    "        if int(r[r_score_key]) > 0:\n",
    "            qid = str(r[r_qid_key])\n",
    "            did = str(r[r_doc_key])\n",
    "            pos_dict.setdefault(qid, []).append(did)\n",
    "    if not pos_dict:\n",
    "        raise RuntimeError(\"fever: qrels no ans！\")\n",
    "    return pos_dict\n",
    "\n",
    "def prepare_fever():\n",
    "    print(\"\\n### Preparing FEVER ###\")\n",
    "\n",
    "    queries = load_dataset(\"BeIR/fever\", \"queries\", split=\"queries\")\n",
    "    corpus  = load_dataset(\"BeIR/fever\", \"corpus\",  split=\"corpus\")\n",
    "    pos_dict = load_qrels_fever()\n",
    "    sample_q = queries[0]\n",
    "    qid_key  = next((k for k in sample_q.keys() if \"id\" in k.lower()), None)\n",
    "    text_key = next((k for k in sample_q.keys() if k.lower() in {\"text\",\"query\",\"body\"}), None)\n",
    "    if not (qid_key and text_key):\n",
    "        raise RuntimeError(\"fever: 无法检测到 queries 中的字段。\")\n",
    "    sample_d = corpus[0]\n",
    "    doc_id_key   = next((k for k in sample_d.keys() if \"id\" in k.lower()), None)\n",
    "    doc_text_key = next((k for k in sample_d.keys()\n",
    "                         if \"text\" in k.lower() or \"body\" in k.lower() or \"passage\" in k.lower()), None)\n",
    "    if not (doc_id_key and doc_text_key):\n",
    "        raise RuntimeError(\"fever: 无法检测到 corpus 中的字段。\")\n",
    "    cand = [\n",
    "        {\"id\": str(q[qid_key]), \"text\": q[text_key]}\n",
    "        for q in queries\n",
    "        if (str(q[qid_key]) in pos_dict) and isinstance(q.get(text_key), str) and is_good_q(q[text_key])\n",
    "    ]\n",
    "    if len(cand) == 0:\n",
    "        raise RuntimeError(\"fever: no query！\")\n",
    "    if len(cand) < N_Q:\n",
    "        qs = random.choices(cand, k=N_Q)\n",
    "    else:\n",
    "        qs = random.sample(cand, N_Q)\n",
    "\n",
    "    pos_ids = { pos_dict[q[\"id\"]][0] for q in qs }\n",
    "    all_doc_ids = [str(d[doc_id_key]) for d in corpus]\n",
    "    other_ids = [did for did in all_doc_ids if did not in pos_ids]\n",
    "    if len(other_ids) < N_DOCS - len(pos_ids):\n",
    "        raise RuntimeError(f\"fever: 填充文档不足（需要 {N_DOCS - len(pos_ids)}, 只有 {len(other_ids)}）。\")\n",
    "    fillers = random.sample(other_ids, N_DOCS - len(pos_ids))\n",
    "    sel_ids = list(pos_ids) + fillers\n",
    "    id2text = { str(d[doc_id_key]): d[doc_text_key] for d in corpus }\n",
    "    docs = [{\"id\": did, \"text\": id2text[did]} for did in sel_ids]\n",
    "    pos_text = { did: id2text[did][:120].replace(\"\\n\", \" \") for did in pos_ids }\n",
    "    cpu_bert = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(\"cpu\")\n",
    "    CLS_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(0, N_DOCS, BATCH_CLS), desc=\"CLS-fever\"):\n",
    "            batch = docs[i : i + BATCH_CLS]\n",
    "            batch_texts = [item[\"text\"] for item in batch]\n",
    "            out = cpu_bert(**enc(batch_texts, \"cpu\")).last_hidden_state[:, 0, :]\n",
    "            CLS_list.append(out)\n",
    "    C_CLS = torch.cat(CLS_list, dim=0)\n",
    "    del cpu_bert\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return docs, qs, pos_dict, pos_text, C_CLS\n",
    "\n",
    "def run_loss_compare_fever():\n",
    "    out_dir = \"exp_results/loss_compare/fever\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    docs, qs, pos_dict, pos_text, C_CLS = prepare_fever()\n",
    "\n",
    "    global bert\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\",\n",
    "                                     torch_dtype=torch.float16).to(DEVICE).eval()\n",
    "\n",
    "    rec = []  \n",
    "    for mode in (\"hinge\", \"cos\"):\n",
    "        pbar = tqdm.tqdm(qs, desc=f\"fever-{mode}\")\n",
    "        for q in pbar:\n",
    "            qtxt = q[\"text\"]\n",
    "            qid  = q[\"id\"]\n",
    "            ids, msk = get_ids_msk(qtxt, DEVICE)\n",
    "            with torch.no_grad():\n",
    "                qcls = bert(**enc([qtxt], DEVICE)).last_hidden_state[:, 0, :]\n",
    "\n",
    "            sims = cos_row(qcls.cpu(), C_CLS)\n",
    "            order = torch.argsort(sims, descending=True)\n",
    "            true_pos_id      = pos_dict[qid][0]\n",
    "            true_pos_excerpt = pos_text[true_pos_id]\n",
    "            tgt_idx       = int(order[RANK_TARGET - 1].item())\n",
    "            tgt_id        = docs[tgt_idx][\"id\"]\n",
    "            tgt_excerpt   = docs[tgt_idx][\"text\"][:120].replace(\"\\n\", \" \")\n",
    "            baseline_sim  = sims[tgt_idx].item()\n",
    "            pos_rank    = None\n",
    "            pos_excerpt = None\n",
    "            tgt_cls = C_CLS[tgt_idx : tgt_idx + 1].to(DEVICE)\n",
    "            CP = torch.cat([C_CLS[:tgt_idx], C_CLS[tgt_idx+1:]], dim=0)\n",
    "\n",
    "            suf, cls_adv, iters = de_opt(ids, msk, CP, tgt_cls, loss_mode=mode)\n",
    "            new_sims = cos_row(cls_adv.cpu(), C_CLS)\n",
    "            fr = (new_sims > new_sims[tgt_idx]).sum().item() + 1\n",
    "            entry = {\n",
    "                \"dataset\":             \"fever\",\n",
    "                \"loss\":                mode,\n",
    "                \"query\":               qtxt,\n",
    "                \"orig_answer_id\":      true_pos_id,\n",
    "                \"orig_answer_excerpt\": true_pos_excerpt,\n",
    "                \"tgt_id\":              tgt_id,\n",
    "                \"tgt_excerpt\":         tgt_excerpt,\n",
    "                \"suffix\":              suffix_str(suf),\n",
    "                \"suffix_len\":          len(suf),\n",
    "                \"suffix_token_ids\":    json.dumps(suf),\n",
    "                \"baseline_rank\":       RANK_TARGET,\n",
    "                \"final_rank\":          fr,\n",
    "                \"delta_rank\":          RANK_TARGET - fr,\n",
    "                \"delta_cos\":           new_sims[tgt_idx].item() - baseline_sim,\n",
    "                \"delta_ndcg\":          dcg(fr) - dcg(RANK_TARGET),\n",
    "                \"success\":             int(fr == 1),\n",
    "                \"iter_used\":           iters,\n",
    "                \"pos_rank\":            pos_rank,\n",
    "                \"pos_excerpt\":         pos_excerpt,\n",
    "            }\n",
    "            rec.append(entry)\n",
    "\n",
    "    df = pd.DataFrame(rec)\n",
    "    df.to_csv(f\"{out_dir}/loss_detail.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    agg = df.groupby(\"loss\").agg(\n",
    "        success_rate   = (\"success\",   \"mean\"),\n",
    "        avg_iters      = (\"iter_used\", \"mean\"),\n",
    "        avg_delta_rank = (\"delta_rank\",\"mean\"),\n",
    "        avg_delta_cos  = (\"delta_cos\", \"mean\"),\n",
    "    ).reset_index()\n",
    "    agg.to_csv(f\"{out_dir}/loss_aggregate.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    h = agg.loc[agg.loss == \"hinge\", \"success_rate\"].values[0]\n",
    "    c = agg.loc[agg.loss == \"cos\",   \"success_rate\"].values[0]\n",
    "    print(f\"✓ fever done  –  hinge success = {h:.2%},  cos success = {c:.2%}\")\n",
    "    del bert\n",
    "    torch.cuda.empty_cache()\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"exp_results/loss_compare/fever\", exist_ok=True)\n",
    "    run_loss_compare_fever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2211713-3a3c-41cc-8639-1a19bd2c5d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Preparing FiQA-2018 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS-fiqa: 100%|█████████████████████████████████████████████████████████████████████████| 63/63 [03:24<00:00,  3.25s/it]\n",
      "fiqa-hinge: 100%|███████████████████████████████████████████████████████████████████| 100/100 [1:17:11<00:00, 46.31s/it]\n",
      "fiqa-cos: 100%|█████████████████████████████████████████████████████████████████████| 100/100 [1:09:47<00:00, 41.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ fiqa done  –  hinge success = 23.00%,  cos success = 2.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_DOCS, N_Q  = 1_000, 100       \n",
    "TAIL_L       = 5                 \n",
    "RANK_TARGET  = 100               \n",
    "POP, MAXITER = 20, 2_000         \n",
    "PATIENCE     = 20                \n",
    "BATCH_CLS    = 16               \n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "VOC = tok.vocab_size \n",
    "def enc(txts, dev):\n",
    "\n",
    "    return tok(\n",
    "        txts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(dev)\n",
    "\n",
    "def get_ids_msk(text, dev):\n",
    "    e = enc([text], dev)\n",
    "    return e[\"input_ids\"][0], e[\"attention_mask\"][0]\n",
    "\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1\n",
    "    )\n",
    "\n",
    "def dcg(rank: int) -> float:\n",
    "    return 1.0 / math.log2(rank + 1)\n",
    "\n",
    "def is_good_q(text: str) -> bool:\n",
    "    if \"?\" not in text:\n",
    "        return False\n",
    "    n = len(tok(text)[\"input_ids\"]) - 2\n",
    "    return (15 < n < 500)\n",
    "\n",
    "def suffix_str(ids):\n",
    "    return \" \".join(tok.convert_ids_to_tokens(ids))\n",
    "\n",
    "def de_opt(ids, msk, CP, tgt_cls, loss_mode=\"hinge\"):\n",
    "\n",
    "    L = TAIL_L\n",
    "    pos = list(range(512 - L, 512))\n",
    "    bounds = [(0, VOC - 1)] * L\n",
    "\n",
    "    def loss_from_cls(c):\n",
    "        # c: shape [1, dim]\n",
    "        if loss_mode == \"cos\":\n",
    "            return -(torch.nn.functional.cosine_similarity(c, tgt_cls)[0]).item()\n",
    "        kth = torch.topk(cos_row(c, CP), 1).values[-1]\n",
    "        sim = torch.nn.functional.cosine_similarity(c, tgt_cls)[0]\n",
    "        return max(0.0, (kth - sim).item())\n",
    "\n",
    "    def obj(v):\n",
    "        \n",
    "        v = [int(round(x)) for x in v]\n",
    "        p, m = ids.clone(), msk.clone()\n",
    "        for i, t in zip(pos, v):\n",
    "            p[i] = t\n",
    "        m[pos] = 1\n",
    "        cls = bert(input_ids=p.unsqueeze(0),\n",
    "                   attention_mask=m.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "        return loss_from_cls(cls)\n",
    "\n",
    "    best, stale = 1e9, 0\n",
    "    def cb(xk, _):\n",
    "        nonlocal best, stale\n",
    "        cur = obj(xk)\n",
    "        if cur < best:\n",
    "            best, stale = cur, 0\n",
    "        else:\n",
    "            stale += 1\n",
    "        return (cur == 0) or (stale >= PATIENCE)\n",
    "\n",
    "    res = differential_evolution(\n",
    "        obj,\n",
    "        bounds,\n",
    "        popsize=POP,\n",
    "        maxiter=MAXITER,\n",
    "        tol=0,\n",
    "        polish=False,\n",
    "        seed=SEED,\n",
    "        callback=cb\n",
    "    )\n",
    "    suf = [int(round(x)) for x in res.x]\n",
    "    p, m = ids.clone(), msk.clone()\n",
    "    m[pos] = 1\n",
    "    for i, t in zip(pos, suf):\n",
    "        p[i] = t\n",
    "    cls = bert(input_ids=p.unsqueeze(0),\n",
    "               attention_mask=m.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "    return suf, cls, res.nfev\n",
    "\n",
    "def load_qrels_fiqa():\n",
    "    qrels_all = load_dataset(\"BeIR/fiqa-qrels\")\n",
    "    qrels = qrels_all[\"test\"]\n",
    "    sample = qrels[0]\n",
    "    r_qid_key   = next((k for k in sample.keys() if \"query\" in k.lower()), None)\n",
    "    r_doc_key   = next((k for k in sample.keys() if \"corpus\" in k.lower() or \"doc\" in k.lower()), None)\n",
    "    r_score_key = next((k for k in sample.keys() if k.lower() in {\"score\",\"label\",\"relevance\"}), None)\n",
    "    if not (r_qid_key and r_doc_key and r_score_key):\n",
    "        raise RuntimeError(\"fiqa: NOT ACCEPT。\")\n",
    "    pos_dict = {}\n",
    "    for r in qrels:\n",
    "        if int(r[r_score_key]) > 0:\n",
    "            qid = str(r[r_qid_key])\n",
    "            did = str(r[r_doc_key])\n",
    "            pos_dict.setdefault(qid, []).append(did)\n",
    "    if not pos_dict:\n",
    "        raise RuntimeError(\"fiqa: qrels NO！\")\n",
    "    return pos_dict\n",
    "\n",
    "def prepare_fiqa():\n",
    "    print(\"\\n### Preparing FiQA-2018 ###\")\n",
    "    queries = load_dataset(\"BeIR/fiqa\", \"queries\", split=\"queries\")\n",
    "    corpus  = load_dataset(\"BeIR/fiqa\", \"corpus\",  split=\"corpus\")\n",
    "    pos_dict = load_qrels_fiqa()\n",
    "    sample_q = queries[0]\n",
    "    qid_key  = next((k for k in sample_q.keys() if \"id\" in k.lower()), None)\n",
    "    text_key = next((k for k in sample_q.keys() if k.lower() in {\"text\",\"query\",\"body\"}), None)\n",
    "    if not (qid_key and text_key):\n",
    "        raise RuntimeError(\"fiqa: NO QUERY。\")\n",
    "    sample_d = corpus[0]\n",
    "    doc_id_key   = next((k for k in sample_d.keys() if \"id\" in k.lower()), None)\n",
    "    doc_text_key = next((k for k in sample_d.keys() \n",
    "                         if \"text\" in k.lower() or \"body\" in k.lower() or \"passage\" in k.lower()), None)\n",
    "    if not (doc_id_key and doc_text_key):\n",
    "        raise RuntimeError(\"fiqa: NO CORPUS。\")\n",
    "    cand = [\n",
    "        {\"id\": str(q[qid_key]), \"text\": q[text_key]}\n",
    "        for q in queries\n",
    "        if (str(q[qid_key]) in pos_dict) and isinstance(q.get(text_key), str) and is_good_q(q[text_key])\n",
    "    ]\n",
    "    if len(cand) == 0:\n",
    "        raise RuntimeError(\"fiqa: OUT OF query！\")\n",
    "    if len(cand) < N_Q:\n",
    "        qs = random.choices(cand, k=N_Q)\n",
    "    else:\n",
    "        qs = random.sample(cand, N_Q)\n",
    "    pos_ids = { pos_dict[q[\"id\"]][0] for q in qs }\n",
    "    all_doc_ids = [str(d[doc_id_key]) for d in corpus]\n",
    "    other_ids = [did for did in all_doc_ids if did not in pos_ids]\n",
    "    if len(other_ids) < N_DOCS - len(pos_ids):\n",
    "        raise RuntimeError(f\"fiqa: NO（,NEED {N_DOCS - len(pos_ids)}, ONLY {len(other_ids)}）。\")\n",
    "    fillers = random.sample(other_ids, N_DOCS - len(pos_ids))\n",
    "    sel_ids = list(pos_ids) + fillers\n",
    "    id2text = { str(d[doc_id_key]): d[doc_text_key] for d in corpus }\n",
    "    docs = [{\"id\": did, \"text\": id2text[did]} for did in sel_ids]\n",
    "    pos_text = { did: id2text[did][:120].replace(\"\\n\", \" \") for did in pos_ids }\n",
    "    cpu_bert = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(\"cpu\")\n",
    "    CLS_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(0, N_DOCS, BATCH_CLS), desc=\"CLS-fiqa\"):\n",
    "            batch = docs[i : i + BATCH_CLS]\n",
    "            batch_texts = [item[\"text\"] for item in batch]\n",
    "            out = cpu_bert(**enc(batch_texts, \"cpu\")).last_hidden_state[:, 0, :]\n",
    "            CLS_list.append(out)\n",
    "    C_CLS = torch.cat(CLS_list, dim=0)\n",
    "    del cpu_bert\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return docs, qs, pos_dict, pos_text, C_CLS\n",
    "def run_loss_compare_fiqa():\n",
    "    out_dir = \"exp_results/loss_compare/fiqa\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    " \n",
    "    docs, qs, pos_dict, pos_text, C_CLS = prepare_fiqa()\n",
    "\n",
    "    global bert\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\",\n",
    "                                     torch_dtype=torch.float16).to(DEVICE).eval()\n",
    "\n",
    "    rec = [] \n",
    "\n",
    "    for mode in (\"hinge\", \"cos\"):\n",
    "        pbar = tqdm.tqdm(qs, desc=f\"fiqa-{mode}\")\n",
    "        for q in pbar:\n",
    "            qtxt = q[\"text\"]\n",
    "            qid  = q[\"id\"]\n",
    "\n",
    "            ids, msk = get_ids_msk(qtxt, DEVICE)\n",
    "            with torch.no_grad():\n",
    "                qcls = bert(**enc([qtxt], DEVICE)).last_hidden_state[:, 0, :]\n",
    "\n",
    "            sims = cos_row(qcls.cpu(), C_CLS)\n",
    "            order = torch.argsort(sims, descending=True)\n",
    "\n",
    "            true_pos_id      = pos_dict[qid][0]\n",
    "            true_pos_excerpt = pos_text[true_pos_id]\n",
    "\n",
    "            tgt_idx       = int(order[RANK_TARGET - 1].item())\n",
    "            tgt_id        = docs[tgt_idx][\"id\"]\n",
    "            tgt_excerpt   = docs[tgt_idx][\"text\"][:120].replace(\"\\n\", \" \")\n",
    "            baseline_sim  = sims[tgt_idx].item()\n",
    "            pos_rank    = None\n",
    "            pos_excerpt = None\n",
    "            tgt_cls = C_CLS[tgt_idx : tgt_idx + 1].to(DEVICE)\n",
    "            CP = torch.cat([C_CLS[:tgt_idx], C_CLS[tgt_idx+1:]], dim=0)\n",
    "\n",
    "            suf, cls_adv, iters = de_opt(ids, msk, CP, tgt_cls, loss_mode=mode)\n",
    "            new_sims = cos_row(cls_adv.cpu(), C_CLS)\n",
    "            fr = (new_sims > new_sims[tgt_idx]).sum().item() + 1\n",
    "            entry = {\n",
    "                \"dataset\":            \"fiqa\",\n",
    "                \"loss\":               mode,\n",
    "                \"query\":              qtxt,\n",
    "\n",
    "                \"orig_answer_id\":      true_pos_id,\n",
    "                \"orig_answer_excerpt\": true_pos_excerpt,\n",
    "                \"tgt_id\":             tgt_id,\n",
    "                \"tgt_excerpt\":        tgt_excerpt,\n",
    "\n",
    "                \"suffix\":             suffix_str(suf),\n",
    "                \"suffix_len\":         len(suf),\n",
    "                \"suffix_token_ids\":   json.dumps(suf),\n",
    "                \"baseline_rank\":      RANK_TARGET,\n",
    "                \"final_rank\":         fr,\n",
    "                \"delta_rank\":         RANK_TARGET - fr,\n",
    "                \"delta_cos\":          new_sims[tgt_idx].item() - baseline_sim,\n",
    "                \"delta_ndcg\":         dcg(fr) - dcg(RANK_TARGET),\n",
    "                \"success\":            int(fr == 1),\n",
    "                \"iter_used\":          iters,\n",
    "                \"pos_rank\":           pos_rank,\n",
    "                \"pos_excerpt\":        pos_excerpt,\n",
    "            }\n",
    "\n",
    "            rec.append(entry)\n",
    "\n",
    "    df = pd.DataFrame(rec)\n",
    "    df.to_csv(f\"{out_dir}/loss_detail.csv\", index=False, encoding=\"utf-8\")\n",
    "    agg = df.groupby(\"loss\").agg(\n",
    "        success_rate   = (\"success\",   \"mean\"),\n",
    "        avg_iters      = (\"iter_used\", \"mean\"),\n",
    "        avg_delta_rank = (\"delta_rank\",\"mean\"),\n",
    "        avg_delta_cos  = (\"delta_cos\", \"mean\"),\n",
    "    ).reset_index()\n",
    "    agg.to_csv(f\"{out_dir}/loss_aggregate.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    h = agg.loc[agg.loss == \"hinge\", \"success_rate\"].values[0]\n",
    "    c = agg.loc[agg.loss == \"cos\",   \"success_rate\"].values[0]\n",
    "    print(f\"✓ fiqa done  –  hinge success = {h:.2%},  cos success = {c:.2%}\")\n",
    "    del bert\n",
    "    torch.cuda.empty_cache()\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"exp_results/loss_compare/fiqa\", exist_ok=True)\n",
    "    run_loss_compare_fiqa()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b8fac-d591-4517-b9ba-3a06b1d1ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for ds_name in FILES.keys():\n",
    "    for topk, linestyle, marker in [(1, '-', 'o'), (10, '--', 's')]:\n",
    "        sub = summary_df[\n",
    "            (summary_df['Dataset'] == ds_name) &\n",
    "            (summary_df['TopK'] == topk)\n",
    "        ].sort_values('suffix_len')\n",
    "        plt.plot(\n",
    "            sub['suffix_len'],\n",
    "            sub['mean_delta_rank'],\n",
    "            label=f\"{ds_name} Top-{topk}\",\n",
    "            color=COLORS[ds_name],\n",
    "            linestyle=linestyle,\n",
    "            marker=marker,\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "plt.axvline(x=4.5, color='gray', linestyle='--', linewidth=1)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.fill_betweenx([ymin, ymax], 4.5, 10, color='gray', alpha=0.15)\n",
    "plt.text(5.2, ymax * 0.9, 'Marginal gains ≈ 0 for L ≥ 5', fontsize=9, color='gray')\n",
    "\n",
    "plt.xlabel('Suffix Length (L)')\n",
    "plt.ylabel('Mean ΔRank')\n",
    "plt.title('Mean ΔRank vs. Suffix Length – MSMARCO/Fiqa/nq (Top-1 & Top-10)')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('delta_rank.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for ds_name in FILES.keys():\n",
    "    for topk, linestyle, marker in [(1, '-', 'o'), (10, '--', 's')]:\n",
    "        sub = summary_df[\n",
    "            (summary_df['Dataset'] == ds_name) &\n",
    "            (summary_df['TopK'] == topk)\n",
    "        ].sort_values('suffix_len')\n",
    "        plt.plot(\n",
    "            sub['suffix_len'],\n",
    "            sub['success_rate'],\n",
    "            label=f\"{ds_name} Top-{topk}\",\n",
    "            color=COLORS[ds_name],\n",
    "            linestyle=linestyle,\n",
    "            marker=marker,\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "plt.xlabel('Suffix Length (L)')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.title('Success Rate vs. Suffix Length – MSMARCO/Fiqa/nq (Top-1 & Top-10)')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc='lower right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig('success_rate.png', dpi=300)\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 5))\n",
    "for ds_name in FILES.keys():\n",
    "    for topk, linestyle, marker in [(1, '-', 'o'), (10, '--', 's')]:\n",
    "        mg_df = marginal_dict[(ds_name, topk)]\n",
    "        plt.plot(\n",
    "            mg_df['suffix_len'],\n",
    "            mg_df['marginal_gain'],\n",
    "            label=f\"{ds_name} Top-{topk}\",\n",
    "            color=COLORS[ds_name],\n",
    "            linestyle=linestyle,\n",
    "            marker=marker,\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "plt.axhline(y=0, color='gray', linestyle='-', linewidth=1)\n",
    "plt.axvline(x=5, color='red', linestyle='--', linewidth=1)\n",
    "plt.text(5.2, plt.ylim()[1] * 0.8, 'L = 5 cutoff', color='red', fontsize=9)\n",
    "\n",
    "plt.xlabel('Suffix Length (L)')\n",
    "plt.ylabel('Marginal Gain (ΔRank difference)')\n",
    "plt.title('Marginal Gain vs. Suffix Length – MSMARCO/Fiqa/nq (Top-1 & Top-10)')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc='upper right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('marginal_gain.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdd5b14-6930-4e23-9d52-8b507ef27693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "FILES = {\n",
    "    'MSMARCO': 'exp_results/msmarco/ablation/ablation_results.csv',\n",
    "    'Fiqa':    'exp_results/ablation/ablation_results.csv',\n",
    "    'nq':      'exp_results/nq/ablation/ablation_results.csv'\n",
    "}\n",
    "COLORS = {\n",
    "    'MSMARCO': 'tab:blue',\n",
    "    'Fiqa':    'tab:orange',\n",
    "    'nq':      'tab:green'\n",
    "}\n",
    "def load_and_truncate(file_path, dataset_name, topk_focus, keep_per_group=100):\n",
    "    if not pathlib.Path(file_path).exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'topK' in df.columns:\n",
    "        topk_col = 'topK'\n",
    "    elif 'top_k' in df.columns:\n",
    "        topk_col = 'top_k'\n",
    "    else:\n",
    "        raise KeyError(f\"Cannot find 'topK' or 'top_k' column in {file_path}\")\n",
    "\n",
    "    df['Dataset'] = dataset_name\n",
    "    df_trunc = (\n",
    "        df\n",
    "        .groupby([topk_col, 'suffix_len'], group_keys=False)\n",
    "        .apply(lambda grp: grp.iloc[:keep_per_group])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    df_focus = df_trunc[df_trunc[topk_col] == topk_focus].copy()\n",
    "    df_focus['TopK'] = topk_focus\n",
    "    return df_focus\n",
    "data_frames = []\n",
    "for ds_name, path in FILES.items():\n",
    "    for topk in [1, 10]:\n",
    "        df_focus = load_and_truncate(path, ds_name, topk_focus=topk, keep_per_group=100)\n",
    "        data_frames.append(df_focus)\n",
    "combined = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "def compute_summary(df):\n",
    "    summary = (\n",
    "        df\n",
    "        .groupby(['Dataset', 'TopK', 'suffix_len'], as_index=False)\n",
    "        .agg(\n",
    "            mean_delta_rank=('delta_rank', 'mean'),\n",
    "            success_rate   =('success',    'mean'),\n",
    "            mean_delta_cos =('delta_cos',  'mean'),\n",
    "            mean_delta_ndcg=('delta_ndcg', 'mean')\n",
    "        )\n",
    "    )\n",
    "    return summary\n",
    "\n",
    "summary_df = compute_summary(combined)\n",
    "\n",
    "def compute_marginal_gain(summary_df, metric_key):\n",
    "    df = summary_df.copy().sort_values('suffix_len').reset_index(drop=True)\n",
    "    gains = [np.nan] \n",
    "    for i in range(1, len(df)):\n",
    "        gains.append(df.loc[i, metric_key] - df.loc[i-1, metric_key])\n",
    "    df['marginal_gain'] = gains\n",
    "    return df\n",
    "marginal_dict = {}\n",
    "for ds_name in FILES.keys():\n",
    "    for topk in [1, 10]:\n",
    "        sub = summary_df[\n",
    "            (summary_df['Dataset'] == ds_name) &\n",
    "            (summary_df['TopK'] == topk)\n",
    "        ][['suffix_len', 'mean_delta_rank']].copy()\n",
    "        mg = compute_marginal_gain(sub, 'mean_delta_rank')\n",
    "        marginal_dict[(ds_name, topk)] = mg\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for ds_name in FILES.keys():\n",
    "    for topk, linestyle, marker in [(1, '-', 'o'), (10, '--', 's')]:\n",
    "        sub = summary_df[\n",
    "            (summary_df['Dataset'] == ds_name) &\n",
    "            (summary_df['TopK'] == topk)\n",
    "        ].sort_values('suffix_len')\n",
    "        plt.plot(\n",
    "            sub['suffix_len'],\n",
    "            sub['mean_delta_rank'],\n",
    "            label=f\"{ds_name} Top-{topk}\",\n",
    "            color=COLORS[ds_name],\n",
    "            linestyle=linestyle,\n",
    "            marker=marker,\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "plt.axvline(x=4.5, color='gray', linestyle='--', linewidth=1)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.fill_betweenx([ymin, ymax], 4.5, 10, color='gray', alpha=0.15)\n",
    "plt.text(5.2, ymax * 0.9, 'Marginal gains ≈ 0 for L ≥ 5', fontsize=9, color='gray')\n",
    "\n",
    "plt.xlabel('Suffix Length (L)')\n",
    "plt.ylabel('Mean ΔRank')\n",
    "plt.title('Mean ΔRank vs. Suffix Length – MSMARCO/Fiqa/nq (Top-1 & Top-10)')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 5))\n",
    "for ds_name in FILES.keys():\n",
    "    for topk, linestyle, marker in [(1, '-', 'o'), (10, '--', 's')]:\n",
    "        sub = summary_df[\n",
    "            (summary_df['Dataset'] == ds_name) &\n",
    "            (summary_df['TopK'] == topk)\n",
    "        ].sort_values('suffix_len')\n",
    "        plt.plot(\n",
    "            sub['suffix_len'],\n",
    "            sub['success_rate'],\n",
    "            label=f\"{ds_name} Top-{topk}\",\n",
    "            color=COLORS[ds_name],\n",
    "            linestyle=linestyle,\n",
    "            marker=marker,\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "plt.xlabel('Suffix Length (L)')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.title('Success Rate vs. Suffix Length – MSMARCO/Fiqa/nq (Top-1 & Top-10)')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc='lower right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 5))\n",
    "for ds_name in FILES.keys():\n",
    "    for topk, linestyle, marker in [(1, '-', 'o'), (10, '--', 's')]:\n",
    "        mg_df = marginal_dict[(ds_name, topk)]\n",
    "        plt.plot(\n",
    "            mg_df['suffix_len'],\n",
    "            mg_df['marginal_gain'],\n",
    "            label=f\"{ds_name} Top-{topk}\",\n",
    "            color=COLORS[ds_name],\n",
    "            linestyle=linestyle,\n",
    "            marker=marker,\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "plt.axhline(y=0, color='gray', linestyle='-', linewidth=1)\n",
    "plt.axvline(x=5, color='red', linestyle='--', linewidth=1)\n",
    "plt.text(5.2, plt.ylim()[1] * 0.8, 'L = 5 cutoff', color='red', fontsize=9)\n",
    "\n",
    "plt.xlabel('Suffix Length (L)')\n",
    "plt.ylabel('Marginal Gain (ΔRank difference)')\n",
    "plt.title('Marginal Gain vs. Suffix Length – MSMARCO/Fiqa/nq (Top-1 & Top-10)')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc='upper right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9786c22e-0b16-43ac-8993-45ac78e85f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ─────────────── 全局配置 ───────────────\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_DOCS, N_Q  = 1_000, 100        \n",
    "TAIL_L       = 5                \n",
    "RANK_TARGET  = 100              \n",
    "POP, MAXITER = 20, 2_000         \n",
    "PATIENCE     = 20                \n",
    "BATCH_CLS    = 16               \n",
    "\n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "VOC = tok.vocab_size  \n",
    "def enc(txts, dev):\n",
    "    return tok(\n",
    "        txts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(dev)\n",
    "\n",
    "def get_ids_msk(text, dev):\n",
    "    e = enc([text], dev)\n",
    "    return e[\"input_ids\"][0], e[\"attention_mask\"][0]\n",
    "\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1\n",
    "    )\n",
    "\n",
    "def dcg(rank: int) -> float:\n",
    "    return 1.0 / math.log2(rank + 1)\n",
    "\n",
    "def is_good_q(text: str) -> bool:\n",
    "\n",
    "    n = len(tok(text)[\"input_ids\"]) - 2\n",
    "    return (10 < n < 500)\n",
    "\n",
    "def suffix_str(ids):\n",
    "\n",
    "    return \" \".join(tok.convert_ids_to_tokens(ids))\n",
    "\n",
    "def de_opt(ids, msk, CP, tgt_cls, loss_mode=\"hinge\"):\n",
    " \n",
    "    L = TAIL_L\n",
    "    pos = list(range(512 - L, 512))\n",
    "    bounds = [(0, VOC - 1)] * L\n",
    "\n",
    "    def loss_from_cls(c):\n",
    "        # c: shape [1, dim]\n",
    "        if loss_mode == \"cos\":\n",
    "            # cos loss = - cosine_similarity(c, tgt_cls)\n",
    "            return -(torch.nn.functional.cosine_similarity(c, tgt_cls)[0]).item()\n",
    "    \n",
    "        kth = torch.topk(cos_row(c, CP), 1).values[-1]\n",
    "        sim = torch.nn.functional.cosine_similarity(c, tgt_cls)[0]\n",
    "        return max(0.0, (kth - sim).item())\n",
    "\n",
    "    def obj(v):\n",
    "      \n",
    "        v = [int(round(x)) for x in v]\n",
    "        p, m = ids.clone(), msk.clone()\n",
    "        for i, t in zip(pos, v):\n",
    "            p[i] = t\n",
    "        m[pos] = 1\n",
    "        cls = bert(input_ids=p.unsqueeze(0),\n",
    "                   attention_mask=m.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "        return loss_from_cls(cls)\n",
    "\n",
    "    best, stale = 1e9, 0\n",
    "    def cb(xk, _):\n",
    "        nonlocal best, stale\n",
    "        cur = obj(xk)\n",
    "        if cur < best:\n",
    "            best, stale = cur, 0\n",
    "        else:\n",
    "            stale += 1\n",
    "        return (cur == 0) or (stale >= PATIENCE)\n",
    "\n",
    "    res = differential_evolution(\n",
    "        obj,\n",
    "        bounds,\n",
    "        popsize=POP,\n",
    "        maxiter=MAXITER,\n",
    "        tol=0,\n",
    "        polish=False,\n",
    "        seed=SEED,\n",
    "        callback=cb\n",
    "    )\n",
    "    suf = [int(round(x)) for x in res.x]\n",
    "\n",
    " \n",
    "    p, m = ids.clone(), msk.clone()\n",
    "    m[pos] = 1\n",
    "    for i, t in zip(pos, suf):\n",
    "        p[i] = t\n",
    "    cls = bert(input_ids=p.unsqueeze(0),\n",
    "               attention_mask=m.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "    return suf, cls, res.nfev\n",
    "\n",
    "def load_qrels_scifact():\n",
    "   \n",
    "    qrels_all = load_dataset(\"BeIR/scifact-qrels\", split=\"train\")\n",
    "    sample = qrels_all[0]\n",
    "    r_qid_key   = next((k for k in sample.keys() if \"query\" in k.lower()), None)\n",
    "    r_doc_key   = next((k for k in sample.keys() if \"corpus\" in k.lower() or \"doc\" in k.lower()), None)\n",
    "    r_score_key = next((k for k in sample.keys() if k.lower() in {\"score\",\"label\",\"relevance\"}), None)\n",
    "    if not (r_qid_key and r_doc_key and r_score_key):\n",
    "        raise RuntimeError(\"scifact: 无法检测到 qrels 字段。\")\n",
    "    pos_dict = {}\n",
    "    for r in qrels_all:\n",
    "        if int(r[r_score_key]) > 0:\n",
    "            qid = str(r[r_qid_key])\n",
    "            did = str(r[r_doc_key])\n",
    "            pos_dict.setdefault(qid, []).append(did)\n",
    "    if not pos_dict:\n",
    "        raise RuntimeError(\"scifact: qrels 中没有任何正例！\")\n",
    "    return pos_dict\n",
    "\n",
    "def prepare_scifact():\n",
    "    queries = load_dataset(\"BeIR/scifact\", \"queries\", split=\"queries\")\n",
    "    corpus  = load_dataset(\"BeIR/scifact\", \"corpus\",  split=\"corpus\")\n",
    "    pos_dict = load_qrels_scifact()\n",
    "    sample_q = queries[0]\n",
    "    qid_key  = next((k for k in sample_q.keys() if \"id\" in k.lower()), None)\n",
    "    text_key = next((k for k in sample_q.keys() if k.lower() in {\"text\",\"query\",\"body\"}), None)\n",
    "    if not (qid_key and text_key):\n",
    "        raise RuntimeError(\"scifact: not able to detect。\")\n",
    "\n",
    "    sample_d = corpus[0]\n",
    "    doc_id_key   = next((k for k in sample_d.keys() if \"id\" in k.lower()), None)\n",
    "    doc_text_key = next((k for k in sample_d.keys()\n",
    "                         if \"text\" in k.lower() or \"body\" in k.lower() or \"passage\" in k.lower()), None)\n",
    "    if not (doc_id_key and doc_text_key):\n",
    "        raise RuntimeError(\"scifact: corpus error。\")\n",
    "\n",
    "    cand = [\n",
    "        {\"id\": str(q[qid_key]), \"text\": q[text_key]}\n",
    "        for q in queries\n",
    "        if (str(q[qid_key]) in pos_dict) and isinstance(q.get(text_key), str) and is_good_q(q[text_key])\n",
    "    ]\n",
    "    if len(cand) == 0:\n",
    "        raise RuntimeError(\"scifact: no query！\")\n",
    "    if len(cand) < N_Q:\n",
    "        \n",
    "        qs = random.choices(cand, k=N_Q)\n",
    "    else:\n",
    "        qs = random.sample(cand, N_Q)\n",
    "    pos_ids = { pos_dict[q[\"id\"]][0] for q in qs }\n",
    "    all_doc_ids = [str(d[doc_id_key]) for d in corpus]\n",
    "    other_ids = [did for did in all_doc_ids if did not in pos_ids]\n",
    "    if len(other_ids) < N_DOCS - len(pos_ids):\n",
    "        raise RuntimeError(f\"scifact: 填充文档不足（需要 {N_DOCS - len(pos_ids)}, 只有 {len(other_ids)}）。\")\n",
    "    fillers = random.sample(other_ids, N_DOCS - len(pos_ids))\n",
    "    sel_ids = list(pos_ids) + fillers\n",
    "    id2text = { str(d[doc_id_key]): d[doc_text_key] for d in corpus }\n",
    "    docs = [{\"id\": did, \"text\": id2text[did]} for did in sel_ids]\n",
    "    pos_text = { did: id2text[did][:120].replace(\"\\n\", \" \") for did in pos_ids }\n",
    "    cpu_bert = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(\"cpu\")\n",
    "    CLS_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(0, N_DOCS, BATCH_CLS), desc=\"CLS-scifact\"):\n",
    "            batch = docs[i : i + BATCH_CLS]\n",
    "            batch_texts = [item[\"text\"] for item in batch]\n",
    "            out = cpu_bert(**enc(batch_texts, \"cpu\")).last_hidden_state[:, 0, :]\n",
    "            CLS_list.append(out)\n",
    "    C_CLS = torch.cat(CLS_list, dim=0)\n",
    "    del cpu_bert\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return docs, qs, pos_dict, pos_text, C_CLS\n",
    "def run_loss_compare_scifact():\n",
    "    out_dir = \"exp_results/loss_compare/scifact\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    docs, qs, pos_dict, pos_text, C_CLS = prepare_scifact()\n",
    "    global bert\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\",\n",
    "                                     torch_dtype=torch.float16).to(DEVICE).eval()\n",
    "\n",
    "    rec = []  \n",
    "    for mode in (\"hinge\", \"cos\"):\n",
    "        pbar = tqdm.tqdm(qs, desc=f\"scifact-{mode}\")\n",
    "        for q in pbar:\n",
    "            qtxt = q[\"text\"]\n",
    "            qid  = q[\"id\"]\n",
    "            ids, msk = get_ids_msk(qtxt, DEVICE)\n",
    "            with torch.no_grad():\n",
    "                qcls = bert(**enc([qtxt], DEVICE)).last_hidden_state[:, 0, :]\n",
    "\n",
    "            sims = cos_row(qcls.cpu(), C_CLS)\n",
    "            order = torch.argsort(sims, descending=True)\n",
    "            true_pos_id      = pos_dict[qid][0]\n",
    "            true_pos_excerpt = pos_text[true_pos_id]\n",
    "\n",
    "            tgt_idx       = int(order[RANK_TARGET - 1].item())\n",
    "            tgt_id        = docs[tgt_idx][\"id\"]\n",
    "            tgt_excerpt   = docs[tgt_idx][\"text\"][:120].replace(\"\\n\", \" \")\n",
    "            baseline_sim  = sims[tgt_idx].item()\n",
    "            pos_rank    = None\n",
    "            pos_excerpt = None\n",
    "            tgt_cls = C_CLS[tgt_idx : tgt_idx + 1].to(DEVICE)\n",
    "            CP = torch.cat([C_CLS[:tgt_idx], C_CLS[tgt_idx+1:]], dim=0)\n",
    "\n",
    "            suf, cls_adv, iters = de_opt(ids, msk, CP, tgt_cls, loss_mode=mode)\n",
    "            new_sims = cos_row(cls_adv.cpu(), C_CLS)\n",
    "            fr = (new_sims > new_sims[tgt_idx]).sum().item() + 1\n",
    "            entry = {\n",
    "                \"dataset\":             \"scifact\",\n",
    "                \"loss\":                mode,\n",
    "                \"query\":               qtxt,\n",
    "                \"orig_answer_id\":      true_pos_id,\n",
    "                \"orig_answer_excerpt\": true_pos_excerpt,\n",
    "                \"tgt_id\":              tgt_id,\n",
    "                \"tgt_excerpt\":         tgt_excerpt,\n",
    "                \"suffix\":              suffix_str(suf),\n",
    "                \"suffix_len\":          len(suf),\n",
    "                \"suffix_token_ids\":    json.dumps(suf),\n",
    "                \"baseline_rank\":       RANK_TARGET,\n",
    "                \"final_rank\":          fr,\n",
    "                \"delta_rank\":          RANK_TARGET - fr,\n",
    "                \"delta_cos\":           new_sims[tgt_idx].item() - baseline_sim,\n",
    "                \"delta_ndcg\":          dcg(fr) - dcg(RANK_TARGET),\n",
    "                \"success\":             int(fr == 1),\n",
    "                \"iter_used\":           iters,\n",
    "                \"pos_rank\":            pos_rank,\n",
    "                \"pos_excerpt\":         pos_excerpt,\n",
    "            }\n",
    "            rec.append(entry)\n",
    "    df = pd.DataFrame(rec)\n",
    "    df.to_csv(f\"{out_dir}/loss_detail.csv\", index=False, encoding=\"utf-8\")\n",
    "    agg = df.groupby(\"loss\").agg(\n",
    "        success_rate   = (\"success\",   \"mean\"),\n",
    "        avg_iters      = (\"iter_used\", \"mean\"),\n",
    "        avg_delta_rank = (\"delta_rank\",\"mean\"),\n",
    "        avg_delta_cos  = (\"delta_cos\", \"mean\"),\n",
    "    ).reset_index()\n",
    "    agg.to_csv(f\"{out_dir}/loss_aggregate.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    h = agg.loc[agg.loss == \"hinge\", \"success_rate\"].values[0]\n",
    "    c = agg.loc[agg.loss == \"cos\",   \"success_rate\"].values[0]\n",
    "    print(f\"✓ scifact done  –  hinge success = {h:.2%},  cos success = {c:.2%}\")\n",
    "    del bert\n",
    "    torch.cuda.empty_cache()\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"exp_results/loss_compare/scifact\", exist_ok=True)\n",
    "    run_loss_compare_scifact()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
