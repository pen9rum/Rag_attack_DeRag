{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb524e98-2aad-41dd-9e16-cf47531a8b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Tail-Patch 8 方法 × Top-{1,10,20}\n",
    "#  - 包含 plateau-stop 版本 (DE_fixed_stop, DE_seq_stop)\n",
    "# ============================================================\n",
    "\n",
    "!pip -q install datasets transformers scipy tqdm --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd159326-2a82-4954-be0d-d6d60895c14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 63/63 [00:28<00:00,  2.23it/s]\n",
      "Top-1:   0%|                                                                                    | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 308\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;66;03m# ------------------ 執行 3 組 k -------------------\u001b[39;00m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m K \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m1\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m20\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[43mrun_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopk_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresults_top\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mK\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m     torch.cuda.empty_cache()          \u001b[38;5;66;03m# 下一輪前清顯存\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 146\u001b[39m, in \u001b[36mrun_all\u001b[39m\u001b[34m(topk_loss, save_dir)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    145\u001b[39m     qcls = bert(**qenc).last_hidden_state[:, \u001b[32m0\u001b[39m, :]\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m base = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcos_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCP\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mqcls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m rank_b = (base > base[-\u001b[32m1\u001b[39m]).sum().item() + \u001b[32m1\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mtd \u001b[38;5;129;01min\u001b[39;00m METHODS:\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#  Tail-Patch Attack Experiments\n",
    "#  - 8 methods × {Top-1, Top-10, Top-20}\n",
    "#  - corpus 1 000 docs , 100 queries  (BeIR/scifact)\n",
    "#  - BERT-base-uncased encoder\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "import os, random, math, tqdm, numpy as np, pandas as pd, torch, warnings\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_DOCS     = 1_000\n",
    "N_Q        = 100\n",
    "TAIL_L     = 5         # 固定補丁長度\n",
    "BUDGET     = 150       # 固定預算 (= f-eval 或 epoch 次數)\n",
    "PATIENCE   = 20        # plateau 20 代早停\n",
    "BATCH_CLS  = 16        # 計算 CLS 時的批次，大幅影響顯存\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1)\n",
    "\n",
    "def dcg(rank): return 1 / math.log2(rank + 1)\n",
    "\n",
    "def load_subset():\n",
    "    corpus  = load_dataset(\"BeIR/scifact\", \"corpus\",  split=\"corpus\")\n",
    "    queries = load_dataset(\"BeIR/scifact\", \"queries\", split=\"queries\")\n",
    "    docs = random.sample(list(corpus),  N_DOCS)\n",
    "    qs   = random.sample(list(queries), N_Q)\n",
    "    return [d[\"text\"] for d in docs], [q[\"text\"] for q in qs]\n",
    "\n",
    "# ------------------------- GGPP ---------------------------------\n",
    "def ggpp_full(tok, bert, ids, msk, CP, tgt_cls, tgt_txt,\n",
    "              cap_L=5, max_epoch=150, topk_loss=20):\n",
    "    # --- Algorithm-1: token importance for prefix init ---\n",
    "    body = tok(tgt_txt, add_special_tokens=False,\n",
    "               truncation=True, max_length=510)[\"input_ids\"]\n",
    "    body += [tok.unk_token_id] * max(0, cap_L - len(body))\n",
    "    base = bert(**tok(tgt_txt, return_tensors=\"pt\",\n",
    "                      truncation=True, max_length=512).to(DEVICE)\n",
    "                ).last_hidden_state[0, 0]\n",
    "    imp = []\n",
    "    for i in range(cap_L):\n",
    "        tmp = body.copy(); tmp[i] = tok.mask_token_id\n",
    "        tens = torch.tensor([tok.cls_token_id]+tmp+[tok.sep_token_id]\n",
    "                            ).unsqueeze(0).to(DEVICE)\n",
    "        emb = bert(input_ids=tens,\n",
    "                   attention_mask=torch.ones_like(tens)).last_hidden_state[0, 0]\n",
    "        imp.append(1 - torch.nn.functional.cosine_similarity(base, emb, dim=0).item())\n",
    "    prefix = [body[i] for i in np.argsort(imp)[-cap_L:]]\n",
    "\n",
    "    # --- prepare patch & helper ---\n",
    "    pos = list(range(502, 502+cap_L))\n",
    "    patch = ids.clone()\n",
    "    for p, v in zip(pos, prefix): patch[p] = v\n",
    "    am = msk.clone(); am[pos] = 1\n",
    "    W = bert.embeddings.word_embeddings.weight\n",
    "\n",
    "    def loss_fn(cls_vec):\n",
    "        kth = torch.topk(cos_row(cls_vec, CP), topk_loss).values[-1]\n",
    "        sim = torch.nn.functional.cosine_similarity(cls_vec, tgt_cls)[0]\n",
    "        return max(0., (kth - sim).item())\n",
    "\n",
    "    best_cls = bert(input_ids=patch.unsqueeze(0),\n",
    "                    attention_mask=am.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "    best_loss = loss_fn(best_cls); used_iter = 0\n",
    "\n",
    "    # --- gradient-guided local search ---\n",
    "    for _ in range(max_epoch):\n",
    "        used_iter += 1\n",
    "        emb = bert.embeddings.word_embeddings(\n",
    "            patch.unsqueeze(0)).detach().clone().requires_grad_(True)\n",
    "        cls = bert(inputs_embeds=emb,\n",
    "                   attention_mask=am.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "        loss = loss_fn(cls); loss.backward()\n",
    "        if loss == 0: best_cls = cls; break\n",
    "\n",
    "        grad = emb.grad[0, pos]                     # (L, 768)\n",
    "        score = torch.matmul(grad, W.t())           # (L, V)\n",
    "        cand_ids = score.topk(5, largest=False, dim=1).indices.cpu()\n",
    "\n",
    "        improved = False\n",
    "        for mask in range(1, 1 << cap_L):           # 子集窮舉\n",
    "            cand = patch.clone()\n",
    "            for i in range(cap_L):\n",
    "                if mask & (1 << i):\n",
    "                    cand[pos[i]] = random.choice(cand_ids[i]).item()\n",
    "            cls2 = bert(input_ids=cand.unsqueeze(0),\n",
    "                        attention_mask=am.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "            l2 = loss_fn(cls2)\n",
    "            if l2 < best_loss:\n",
    "                best_loss = l2; patch = cand; best_cls = cls2; improved = True\n",
    "            if l2 == 0: break\n",
    "        if not improved: break\n",
    "    return best_cls, cap_L, used_iter\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# ---------------------- 主流程 -----------------------\n",
    "def run_all(topk_loss, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(DEVICE)\n",
    "    VOC  = tok.vocab_size\n",
    "    def enc(txt):\n",
    "        return tok(txt, padding=\"max_length\", truncation=True,\n",
    "                   max_length=512, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # ------- prepare corpus CLS vectors (on CPU) -------\n",
    "    docs, qs = load_subset()\n",
    "    CLS = []\n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm.tqdm(range(0, len(docs), BATCH_CLS), desc=\"CLS\"):\n",
    "            bt = enc(docs[i:i+BATCH_CLS])\n",
    "            cls = bert(**bt).last_hidden_state[:, 0, :].cpu()\n",
    "            CLS.append(cls)\n",
    "            del bt; torch.cuda.empty_cache()\n",
    "    C_CLS = torch.cat(CLS)\n",
    "\n",
    "    METHODS = (\n",
    "        \"none\", \"random\", \"greedy\", \"ggpp\",\n",
    "        \"DE_fixed\", \"DE_seq\",\n",
    "        \"DE_fixed_stop\", \"DE_seq_stop\"\n",
    "    )\n",
    "    records = []\n",
    "\n",
    "    # ------- iterate queries -------\n",
    "    for qtxt in tqdm.tqdm(qs, desc=f\"Top-{topk_loss}\"):\n",
    "        tgt = random.randrange(len(docs))\n",
    "        tgt_txt = docs[tgt]\n",
    "        if len(tok(tgt_txt)[\"input_ids\"]) > 510: continue\n",
    "        tgt_cls = C_CLS[tgt:tgt+1].to(DEVICE)\n",
    "        CP = C_CLS[[i for i in range(len(docs)) if i != tgt]]\n",
    "\n",
    "        qenc = enc(qtxt); ids = qenc[\"input_ids\"][0]; msk = qenc[\"attention_mask\"][0]\n",
    "        with torch.no_grad():\n",
    "            qcls = bert(**qenc).last_hidden_state[:, 0, :]\n",
    "        base = torch.cat([cos_row(qcls, CP),\n",
    "                          torch.nn.functional.cosine_similarity(\n",
    "                              qcls.cpu(), tgt_cls.cpu())])\n",
    "        rank_b = (base > base[-1]).sum().item() + 1\n",
    "\n",
    "        for mtd in METHODS:\n",
    "            success = False; used_L = 0; used_iter = 0; adv_cls = qcls\n",
    "\n",
    "            # === none ===\n",
    "            if mtd == \"none\":\n",
    "                pass\n",
    "\n",
    "            # === random ===\n",
    "            elif mtd == \"random\":\n",
    "                best_loss = 1e9\n",
    "                for _ in range(BUDGET):\n",
    "                    patch = ids.clone()\n",
    "                    for p in range(502, 502+TAIL_L):\n",
    "                        patch[p] = random.randrange(VOC)\n",
    "                    am = msk.clone(); am[502:502+TAIL_L] = 1\n",
    "                    cls = bert(input_ids=patch.unsqueeze(0),\n",
    "                               attention_mask=am.unsqueeze(0)\n",
    "                               ).last_hidden_state[:, 0, :]\n",
    "                    kth = torch.topk(cos_row(cls, CP), topk_loss).values[-1]\n",
    "                    loss = max(0., (kth - torch.nn.functional.cosine_similarity(\n",
    "                                         cls, tgt_cls)[0]).item())\n",
    "                    used_iter += 1\n",
    "                    if loss < best_loss:\n",
    "                        best_loss = loss; adv_cls = cls; success = (loss == 0)\n",
    "                    if success: break\n",
    "                used_L = TAIL_L\n",
    "\n",
    "            # === greedy (HotFlip-like) ===\n",
    "            elif mtd == \"greedy\":\n",
    "                pos = list(range(502, 502+TAIL_L)); patch = ids.clone()\n",
    "                best_loss = 1e9\n",
    "                for _ in range(BUDGET):\n",
    "                    used_iter += 1; improved = False\n",
    "                    for p in pos:\n",
    "                        best_id = patch[p].item()\n",
    "                        for cand in random.sample(range(VOC), 512):\n",
    "                            patch[p] = cand\n",
    "                            am = msk.clone(); am[pos] = 1\n",
    "                            cls = bert(input_ids=patch.unsqueeze(0),\n",
    "                                       attention_mask=am.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "                            kth = torch.topk(cos_row(cls, CP), topk_loss).values[-1]\n",
    "                            loss = max(0., (kth - torch.nn.functional.cosine_similarity(\n",
    "                                                 cls, tgt_cls)[0]).item())\n",
    "                            if loss < best_loss:\n",
    "                                best_loss = loss; best_id = cand; adv_cls = cls; improved = True\n",
    "                            if loss == 0: success = True; break\n",
    "                        patch[p] = best_id\n",
    "                        if success: break\n",
    "                    if success or not improved: break\n",
    "                used_L = TAIL_L\n",
    "\n",
    "            # === ggpp ===\n",
    "            elif mtd == \"ggpp\":\n",
    "                adv_cls, used_L, used_iter = ggpp_full(\n",
    "                    tok, bert, ids, msk, CP, tgt_cls, tgt_txt,\n",
    "                    cap_L=TAIL_L, max_epoch=BUDGET, topk_loss=topk_loss\n",
    "                )\n",
    "                kth = torch.topk(cos_row(adv_cls, CP), topk_loss).values[-1]\n",
    "                success = (kth <= torch.nn.functional.cosine_similarity(\n",
    "                                      adv_cls, tgt_cls)[0])\n",
    "\n",
    "            # === 共用 DE 執行器 ===\n",
    "            def de_run(L, max_iter, plateau):\n",
    "                pos = list(range(502, 502+L)); bounds = [(0, VOC-1)] * L\n",
    "                gens = max_iter // 20 if max_iter else 1000\n",
    "                stop = [0]; best = [1e9]\n",
    "\n",
    "                def obj(v):\n",
    "                    v = [int(round(x)) for x in v]\n",
    "                    patch = ids.clone()\n",
    "                    for p, t in zip(pos, v): patch[p] = t\n",
    "                    am = msk.clone(); am[pos] = 1\n",
    "                    cls = bert(input_ids=patch.unsqueeze(0),\n",
    "                               attention_mask=am.unsqueeze(0)\n",
    "                               ).last_hidden_state[:, 0, :]\n",
    "                    kth = torch.topk(cos_row(cls, CP), topk_loss).values[-1]\n",
    "                    sim = torch.nn.functional.cosine_similarity(cls, tgt_cls)[0]\n",
    "                    return max(0., (kth - sim).item())\n",
    "\n",
    "                def cb(xk, _):\n",
    "                    if not plateau: return False\n",
    "                    cur = obj(xk)\n",
    "                    stop[0] = 0 if cur < best[0] else stop[0] + 1\n",
    "                    best[0] = min(best[0], cur)\n",
    "                    return stop[0] >= PATIENCE or cur == 0\n",
    "\n",
    "                res = differential_evolution(obj, bounds, popsize=20,\n",
    "                                             maxiter=gens, tol=0,\n",
    "                                             polish=False, seed=SEED,\n",
    "                                             callback=cb)\n",
    "                v = [int(round(x)) for x in res.x]\n",
    "                patch = ids.clone()\n",
    "                for p, t in zip(pos, v): patch[p] = t\n",
    "                am = msk.clone(); am[pos] = 1\n",
    "                cls = bert(input_ids=patch.unsqueeze(0),\n",
    "                           attention_mask=am.unsqueeze(0)\n",
    "                           ).last_hidden_state[:, 0, :]\n",
    "                return cls, res.nfev, res.fun == 0\n",
    "\n",
    "            # === DE_fixed ===\n",
    "            if mtd == \"DE_fixed\":\n",
    "                adv_cls, used_iter, success = de_run(TAIL_L, BUDGET, plateau=False)\n",
    "                used_L = TAIL_L\n",
    "\n",
    "            # === DE_fixed_stop ===\n",
    "            if mtd == \"DE_fixed_stop\":\n",
    "                adv_cls, used_iter, success = de_run(TAIL_L, None, plateau=True)\n",
    "                used_L = TAIL_L\n",
    "\n",
    "            # === DE_seq ===\n",
    "            if mtd == \"DE_seq\":\n",
    "                for L in range(1, TAIL_L+1):\n",
    "                    cls, iters, ok = de_run(L, BUDGET, plateau=False)\n",
    "                    used_iter += iters\n",
    "                    if ok:\n",
    "                        adv_cls = cls; used_L = L; success = True; break\n",
    "                else: used_L = TAIL_L\n",
    "\n",
    "            # === DE_seq_stop ===\n",
    "            if mtd == \"DE_seq_stop\":\n",
    "                for L in range(1, TAIL_L+1):\n",
    "                    cls, iters, ok = de_run(L, None, plateau=True)\n",
    "                    used_iter += iters\n",
    "                    if ok:\n",
    "                        adv_cls = cls; used_L = L; success = True; break\n",
    "                else: used_L = TAIL_L\n",
    "\n",
    "            # ========== 指標 ==========\n",
    "            sims = torch.cat([cos_row(adv_cls.cpu(), CP),\n",
    "                              torch.nn.functional.cosine_similarity(\n",
    "                                  adv_cls.cpu(), tgt_cls.cpu())])\n",
    "            rank_a = (sims > sims[-1]).sum().item() + 1\n",
    "            d_mrr  = 1/rank_a - 1/rank_b\n",
    "            d_ndcg = ((dcg(rank_a) if rank_a <= 20 else 0) -\n",
    "                      (dcg(rank_b) if rank_b <= 20 else 0))\n",
    "            d_cos  = (torch.nn.functional.cosine_similarity(\n",
    "                        adv_cls, tgt_cls)[0] -\n",
    "                      torch.nn.functional.cosine_similarity(\n",
    "                        qcls, tgt_cls)[0]).item()\n",
    "\n",
    "            records.append(dict(\n",
    "                top_k       = topk_loss,\n",
    "                method      = mtd,\n",
    "                success     = int(rank_a <= topk_loss),\n",
    "                token_used  = used_L,\n",
    "                iter_used   = used_iter,\n",
    "                delta_mrr   = d_mrr,\n",
    "                delta_ndcg  = d_ndcg,\n",
    "                delta_cos   = d_cos\n",
    "            ))\n",
    "        torch.cuda.empty_cache()   # 每 query 釋放暫存\n",
    "\n",
    "    pd.DataFrame(records).to_csv(f\"{save_dir}/records.csv\", index=False)\n",
    "    print(f\"✓ {save_dir}  rows = {len(records)}\")\n",
    "\n",
    "# ------------------ 執行 3 組 k -------------------\n",
    "for K in (1, 10, 20):\n",
    "    run_all(topk_loss=K, save_dir=f\"results_top{K}\")\n",
    "    torch.cuda.empty_cache()          # 下一輪前清顯存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd1d5a-d05c-47d1-a21b-a66678756844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
