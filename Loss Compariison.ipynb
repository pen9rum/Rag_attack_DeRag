{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b1e090-96f9-44b6-aa71-b6f80afb880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install datasets transformers scipy tqdm --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76921e-aa9b-4840-af8b-789921005fe7",
   "metadata": {},
   "source": [
    "\n",
    "**MS MARCO & SciFact & NQ**\n",
    "\n",
    "This notebook presents an ablation study on DeRAG attacks using Differential Evolution (DE) under a controlled setting. Specifically, we evaluate how suffix length (`L ∈ {1..10}`) and retrieval depth (`Top-K ∈ {1, 10}`) affect the effectiveness of adversarial prompt suffixes.\n",
    "\n",
    "###  Experiment Setup\n",
    "\n",
    "- **Corpus**: Random sample of 1,000 documents per dataset\n",
    "- **Queries**: 120 filtered questions per dataset (tokens ∈ [20, 500], must contain `?`)\n",
    "- **Target Document**: Document ranked **800th** under baseline retrieval\n",
    "- **Encoder**: `bert-base-uncased` (evaluated in `fp16` on GPU)\n",
    "- **Suffix**: Appended at the end of the query, of length L ∈ [1, 10]\n",
    " \n",
    "\n",
    "Each query is paired with one incorrect target passage from the 800th position. The goal is to use a DE-optimized suffix to **promote** that document into the **Top-K** positions. Suffixes are optimized using **hinge loss** over cosine similarity between query and document embeddings.\n",
    "\n",
    "###  Evaluation Metrics\n",
    "For each `(query, L, K)` setting, we report:\n",
    "- **Suffix tokens** (`suffix`, `suffix_len`)\n",
    "- **Final rank of target document** (`final_rank`)\n",
    "- **Change in cosine similarity** (`Δcos`)\n",
    "- **Change in nDCG** (`ΔnDCG@K`)\n",
    "- **Success rate** (whether `rank ≤ K`)\n",
    "- **Iteration count** (used by DE optimizer)\n",
    "\n",
    "###  Output\n",
    "Results are saved to:\n",
    "```bash\n",
    "exp_results/{dataset}/ablation/ablation_results.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dbf489b-5eac-4f05-8eca-878016d2f99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "msmarco-CLS: 100%|██████████████████████████████████████████████████████████████████████| 63/63 [03:13<00:00,  3.08s/it]\n",
      "msmarco  K=1 L=1: 100%|███████████████████████████████████████████████████████████████| 120/120 [12:51<00:00,  6.43s/it]\n",
      "msmarco  K=1 L=2: 100%|███████████████████████████████████████████████████████████████| 120/120 [31:14<00:00, 15.62s/it]\n",
      "msmarco  K=1 L=3: 100%|███████████████████████████████████████████████████████████████| 120/120 [45:53<00:00, 22.94s/it]\n",
      "msmarco  K=1 L=4: 100%|███████████████████████████████████████████████████████████████| 120/120 [57:14<00:00, 28.62s/it]\n",
      "msmarco  K=1 L=5: 100%|█████████████████████████████████████████████████████████████| 120/120 [1:12:20<00:00, 36.17s/it]\n",
      "msmarco  K=1 L=6: 100%|█████████████████████████████████████████████████████████████| 120/120 [1:31:35<00:00, 45.79s/it]\n",
      "msmarco  K=1 L=7: 100%|█████████████████████████████████████████████████████████████| 120/120 [1:50:54<00:00, 55.45s/it]\n",
      "msmarco  K=1 L=8: 100%|█████████████████████████████████████████████████████████████| 120/120 [1:59:57<00:00, 59.98s/it]\n",
      "msmarco  K=1 L=9: 100%|█████████████████████████████████████████████████████████████| 120/120 [2:21:17<00:00, 70.65s/it]\n",
      "msmarco  K=1 L=10: 100%|████████████████████████████████████████████████████████████| 120/120 [2:36:31<00:00, 78.26s/it]\n",
      "msmarco  K=10 L=1: 100%|██████████████████████████████████████████████████████████████| 120/120 [11:55<00:00,  5.96s/it]\n",
      "msmarco  K=10 L=2: 100%|██████████████████████████████████████████████████████████████| 120/120 [29:17<00:00, 14.65s/it]\n",
      "msmarco  K=10 L=3: 100%|██████████████████████████████████████████████████████████████| 120/120 [44:31<00:00, 22.26s/it]\n",
      "msmarco  K=10 L=4: 100%|██████████████████████████████████████████████████████████████| 120/120 [54:55<00:00, 27.46s/it]\n",
      "msmarco  K=10 L=5: 100%|████████████████████████████████████████████████████████████| 120/120 [1:08:30<00:00, 34.26s/it]\n",
      "msmarco  K=10 L=6: 100%|████████████████████████████████████████████████████████████| 120/120 [1:27:29<00:00, 43.74s/it]\n",
      "msmarco  K=10 L=7: 100%|████████████████████████████████████████████████████████████| 120/120 [1:42:49<00:00, 51.41s/it]\n",
      "msmarco  K=10 L=8: 100%|████████████████████████████████████████████████████████████| 120/120 [1:53:37<00:00, 56.81s/it]\n",
      "msmarco  K=10 L=9: 100%|████████████████████████████████████████████████████████████| 120/120 [2:08:37<00:00, 64.31s/it]\n",
      "msmarco  K=10 L=10: 100%|███████████████████████████████████████████████████████████| 120/120 [2:23:16<00:00, 71.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ msmarco ablation 完成 — rows = 2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, random, math, json, warnings, numpy as np, pandas as pd, torch, tqdm\n",
    "from datasets     import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED             = 42\n",
    "DEVICE           = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_DOCS, N_Q      = 1_000, 120\n",
    "TAIL_MAX         = 10\n",
    "RANK_TARGET      = 800\n",
    "POP, MAXITER     = 20, 2_000\n",
    "PATIENCE         = 20\n",
    "BATCH_CLS        = 16          \n",
    "DATASETS         = [\"msmarco\",\"fiqa\",\"NQ\"]   \n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "tok  = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "VOC  = tok.vocab_size\n",
    "\n",
    "\n",
    "def enc(txts, dev):\n",
    "    return tok(txts, padding=\"max_length\", truncation=True,\n",
    "               max_length=512, return_tensors=\"pt\").to(dev)\n",
    "\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1)\n",
    "\n",
    "def dcg(rank:int)->float:\n",
    "    return 1 / math.log2(rank + 1)\n",
    "\n",
    "def is_good_q(t:str)->bool:\n",
    "    return \"?\" in t and 20 < len(tok(t)[\"input_ids\"])-2 < 500\n",
    "\n",
    "def suffix_str(ids):            # pretty print suffix\n",
    "    return \" \".join(tok.convert_ids_to_tokens(ids))\n",
    "\n",
    "def de_opt(ids, msk, CP, tgt_cls, L, topk):\n",
    "    pos, bounds = list(range(512-L,512)), [(0,VOC-1)]*L\n",
    "    def loss_from_cls(c):\n",
    "        kth = torch.topk(cos_row(c,CP),topk).values[-1]\n",
    "        sim = torch.nn.functional.cosine_similarity(c,tgt_cls)[0]\n",
    "        return max(0., (kth-sim).item())\n",
    "    def obj(v):\n",
    "        v=[int(round(x)) for x in v]\n",
    "        p,m=ids.clone(),msk.clone(); m[pos]=1\n",
    "        for i,t in zip(pos,v): p[i]=t\n",
    "        cls=bert(input_ids=p.unsqueeze(0),\n",
    "                 attention_mask=m.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "        return loss_from_cls(cls)\n",
    "    best,stale=1e9,0\n",
    "    def cb(xk,_):\n",
    "        nonlocal best,stale\n",
    "        cur=obj(xk)\n",
    "        if cur<best: best,stale=cur,0\n",
    "        else: stale+=1\n",
    "        return cur==0 or stale>=PATIENCE\n",
    "    res=differential_evolution(obj,bounds,popsize=POP,maxiter=MAXITER,\n",
    "                               tol=0,polish=False,seed=SEED,callback=cb)\n",
    "    suf=[int(round(x)) for x in res.x]\n",
    "    p,m=ids.clone(),msk.clone(); m[pos]=1\n",
    "    for i,t in zip(pos,suf): p[i]=t\n",
    "    cls=bert(input_ids=p.unsqueeze(0),\n",
    "             attention_mask=m.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "    return suf,cls,res.nfev\n",
    "\n",
    "def prepare(ds:str):\n",
    "    corpus  = load_dataset(f\"BeIR/{ds}\", \"corpus\",  split=\"corpus\")\n",
    "    queries = load_dataset(f\"BeIR/{ds}\", \"queries\", split=\"queries\")\n",
    "    docs = random.sample(list(corpus), N_DOCS)\n",
    "\n",
    "    pool = [q for q in queries if is_good_q(q[\"text\"])]\n",
    "    if len(pool) < N_Q:\n",
    "        raise ValueError(f\"{ds}: 問句僅 {len(pool)} 條，少於 120\")\n",
    "    qs = random.sample(pool, N_Q)\n",
    "\n",
    "    cpu_bert = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(\"cpu\")\n",
    "    CLS=[]\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(0,N_DOCS,BATCH_CLS), desc=f\"{ds}-CLS\"):\n",
    "            batch = [d[\"text\"] for d in docs[i:i+BATCH_CLS]]\n",
    "            CLS.append(cpu_bert(**enc(batch,\"cpu\")).last_hidden_state[:,0,:])\n",
    "    C_CLS = torch.cat(CLS)\n",
    "    del cpu_bert; torch.cuda.empty_cache()\n",
    "\n",
    "    return docs, qs, C_CLS\n",
    "\n",
    "def run_ablation(ds:str):\n",
    "    out_dir = f\"exp_results/{ds}/ablation\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    docs, qs, C_CLS = prepare(ds)\n",
    "\n",
    "    global bert\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\",\n",
    "                                     torch_dtype=torch.float16\n",
    "                                    ).to(DEVICE).eval()\n",
    "\n",
    "    rec=[]\n",
    "    for K in (1,10):\n",
    "        for L in range(1, TAIL_MAX+1):\n",
    "            for q in tqdm.tqdm(qs, desc=f\"{ds}  K={K} L={L}\"):\n",
    "                qtxt = q[\"text\"]\n",
    "                ids,msk = enc([qtxt],DEVICE)[\"input_ids\"][0], enc([qtxt],DEVICE)[\"attention_mask\"][0]\n",
    "                with torch.no_grad():\n",
    "                    qcls = bert(**enc([qtxt],DEVICE)).last_hidden_state[:,0,:]\n",
    "\n",
    "                sims = cos_row(qcls.cpu(), C_CLS)\n",
    "                order = torch.argsort(sims, descending=True)\n",
    "                tgt   = int(order[RANK_TARGET-1])\n",
    "                tgt_txt = docs[tgt][\"text\"]\n",
    "                if len(tok(tgt_txt)[\"input_ids\"]) > 510: \n",
    "                    continue\n",
    "\n",
    "                baseline_sim = sims[tgt].item()\n",
    "                orig_top1    = docs[int(order[0])][\"text\"]\n",
    "                tgt_cls      = C_CLS[tgt:tgt+1].to(DEVICE)\n",
    "                CP           = C_CLS[[i for i in range(N_DOCS) if i != tgt]]\n",
    "\n",
    "                suf, cls, it = de_opt(ids, msk, CP, tgt_cls, L, K)\n",
    "                new_sims = cos_row(cls.cpu(), C_CLS)\n",
    "                fr = (new_sims > new_sims[tgt]).sum().item() + 1\n",
    "\n",
    "                rec.append(dict(dataset=ds, topK=K, L=L,\n",
    "                                query=qtxt,\n",
    "                                target_excerpt=tgt_txt[:120].replace(\"\\n\",\" \"),\n",
    "                                orig_top1_excerpt=orig_top1[:120].replace(\"\\n\",\" \"),\n",
    "                                suffix=suffix_str(suf),\n",
    "                                suffix_len=len(suf),\n",
    "                                suffix_token_ids=json.dumps(suf),\n",
    "                                baseline_rank=RANK_TARGET,\n",
    "                                final_rank=fr,\n",
    "                                delta_rank=RANK_TARGET-fr,\n",
    "                                delta_cos=new_sims[tgt].item()-baseline_sim,\n",
    "                                delta_ndcg=dcg(fr)-dcg(RANK_TARGET),\n",
    "                                success=int(fr<=K),\n",
    "                                iter_used=it))\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    pd.DataFrame(rec).to_csv(f\"{out_dir}/ablation_results.csv\",\n",
    "                             index=False, encoding=\"utf-8\")\n",
    "    print(f\"✓ {ds} ablation FINISHED — rows = {len(rec)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"exp_results\", exist_ok=True)\n",
    "    for ds in DATASETS:\n",
    "        run_ablation(ds)\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87fc529-d9d5-4449-acf8-fae139a7ec6c",
   "metadata": {},
   "source": [
    "#  DeRAG Loss Function Comparison\n",
    "\n",
    "This repository evaluates the effectiveness of **Differential Evolution (DE)** optimized tail-patch attacks under different **loss objectives** (`hinge` vs. `cosine`) across four BEIR datasets.\n",
    "\n",
    "##  Datasets\n",
    "\n",
    "- **MS MARCO** (Open-domain QA)\n",
    "- **FiQA** (Financial QA)\n",
    "- **SciFact** (Scientific Fact Verification)\n",
    "- **FEVER** (Fact Extraction and Verification)\n",
    "\n",
    "## Experiment Setup\n",
    "\n",
    "- **Corpus**: 1,000 documents per dataset\n",
    "- **Queries**: 100 filtered questions per dataset  \n",
    "  - Must contain a `?`\n",
    "  - Token length ∈ [20, 500]\n",
    "- **Target Document**: The **100th-ranked** document under baseline similarity (cosine).\n",
    "- **Model**: `bert-base-uncased`\n",
    "- **Device**: GPU with `float16` for inference\n",
    "- **Optimization**: Differential Evolution (DE)\n",
    "  - Population: 20\n",
    "  - Max Iterations: 2,000\n",
    "  - Suffix Length: `L = 5` tokens\n",
    "\n",
    "##  Objective\n",
    "\n",
    "For each `(query, target document)` pair, we use DE to generate a **5-token suffix** that is appended to the query to **promote the target document's rank**.\n",
    "\n",
    "We compare two loss modes:\n",
    "\n",
    "- `hinge`: Max-margin loss between target and top distractor\n",
    "- `cos`: Cosine similarity loss with target document only\n",
    "\n",
    "##  Evaluation Metrics\n",
    "\n",
    "For each query and loss mode:\n",
    "\n",
    "- `suffix` (token sequence)\n",
    "- `suffix_len`\n",
    "- `suffix_token_ids`\n",
    "- `baseline_rank` (always 100)\n",
    "- `final_rank`\n",
    "- `delta_rank`\n",
    "- `delta_cos`\n",
    "- `delta_nDCG`\n",
    "- `success` (whether final rank == 1)\n",
    "- `iter_used` (optimizer steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f649703-4847-4a8a-8206-70c8fbdb72a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Preparing FEVER ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS-fever: 100%|████████████████████████████████████████████████████████████████████████| 63/63 [03:01<00:00,  2.89s/it]\n",
      "fever-hinge: 100%|██████████████████████████████████████████████████████████████████| 100/100 [1:12:16<00:00, 43.37s/it]\n",
      "fever-cos: 100%|████████████████████████████████████████████████████████████████████| 100/100 [1:22:05<00:00, 49.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ fever done  –  hinge success = 26.00%,  cos success = 1.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_DOCS, N_Q  = 1_000, 100         \n",
    "TAIL_L       = 5                 \n",
    "RANK_TARGET  = 100              \n",
    "POP, MAXITER = 20, 2_000        \n",
    "PATIENCE     = 20                \n",
    "BATCH_CLS    = 16               \n",
    "\n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "VOC = tok.vocab_size \n",
    "\n",
    "def enc(txts, dev):\n",
    "    return tok(\n",
    "        txts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(dev)\n",
    "\n",
    "def get_ids_msk(text, dev):\n",
    "    e = enc([text], dev)\n",
    "    return e[\"input_ids\"][0], e[\"attention_mask\"][0]\n",
    "\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1\n",
    "    )\n",
    "\n",
    "def dcg(rank: int) -> float:\n",
    "    return 1.0 / math.log2(rank + 1)\n",
    "\n",
    "def is_good_q(text: str) -> bool:\n",
    "   \n",
    "    n = len(tok(text)[\"input_ids\"]) - 2\n",
    "    return (10 < n < 500)\n",
    "\n",
    "def suffix_str(ids):\n",
    "    return \" \".join(tok.convert_ids_to_tokens(ids))\n",
    "\n",
    "# ─────────────── DE 优化器 ───────────────\n",
    "def de_opt(ids, msk, CP, tgt_cls, loss_mode=\"hinge\"):\n",
    "\n",
    "    L = TAIL_L\n",
    "    pos = list(range(512 - L, 512))\n",
    "    bounds = [(0, VOC - 1)] * L\n",
    "\n",
    "    def loss_from_cls(c):\n",
    "        # c: shape [1, dim]\n",
    "        if loss_mode == \"cos\":\n",
    "            # cos loss = - cosine_similarity(c, tgt_cls)\n",
    "            return -(torch.nn.functional.cosine_similarity(c, tgt_cls)[0]).item()\n",
    "        kth = torch.topk(cos_row(c, CP), 1).values[-1]\n",
    "        sim = torch.nn.functional.cosine_similarity(c, tgt_cls)[0]\n",
    "        return max(0.0, (kth - sim).item())\n",
    "\n",
    "    def obj(v):\n",
    "        v = [int(round(x)) for x in v]\n",
    "        p, m = ids.clone(), msk.clone()\n",
    "        for i, t in zip(pos, v):\n",
    "            p[i] = t\n",
    "        m[pos] = 1\n",
    "        cls = bert(input_ids=p.unsqueeze(0),\n",
    "                   attention_mask=m.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "        return loss_from_cls(cls)\n",
    "\n",
    "    best, stale = 1e9, 0\n",
    "    def cb(xk, _):\n",
    "        nonlocal best, stale\n",
    "        cur = obj(xk)\n",
    "        if cur < best:\n",
    "            best, stale = cur, 0\n",
    "        else:\n",
    "            stale += 1\n",
    "        return (cur == 0) or (stale >= PATIENCE)\n",
    "\n",
    "    res = differential_evolution(\n",
    "        obj,\n",
    "        bounds,\n",
    "        popsize=POP,\n",
    "        maxiter=MAXITER,\n",
    "        tol=0,\n",
    "        polish=False,\n",
    "        seed=SEED,\n",
    "        callback=cb\n",
    "    )\n",
    "    suf = [int(round(x)) for x in res.x]\n",
    "    p, m = ids.clone(), msk.clone()\n",
    "    m[pos] = 1\n",
    "    for i, t in zip(pos, suf):\n",
    "        p[i] = t\n",
    "    cls = bert(input_ids=p.unsqueeze(0),\n",
    "               attention_mask=m.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "    return suf, cls, res.nfev\n",
    "\n",
    "def load_qrels_fever():\n",
    "    qrels_all = load_dataset(\"BeIR/fever-qrels\", split=\"train\")\n",
    "    # 动态探测字段：通常是 'query-id', 'corpus-id', 'score'\n",
    "    sample = qrels_all[0]\n",
    "    r_qid_key   = next((k for k in sample.keys() if \"query\" in k.lower()), None)\n",
    "    r_doc_key   = next((k for k in sample.keys() if \"corpus\" in k.lower() or \"doc\" in k.lower()), None)\n",
    "    r_score_key = next((k for k in sample.keys() if k.lower() in {\"score\",\"label\",\"relevance\"}), None)\n",
    "    if not (r_qid_key and r_doc_key and r_score_key):\n",
    "        raise RuntimeError(\"fever:No qrels。\")\n",
    "    pos_dict = {}\n",
    "    for r in qrels_all:\n",
    "        if int(r[r_score_key]) > 0:\n",
    "            qid = str(r[r_qid_key])\n",
    "            did = str(r[r_doc_key])\n",
    "            pos_dict.setdefault(qid, []).append(did)\n",
    "    if not pos_dict:\n",
    "        raise RuntimeError(\"fever: qrels no ans！\")\n",
    "    return pos_dict\n",
    "\n",
    "# ─────────────── 数据集准备 ───────────────\n",
    "def prepare_fever():\n",
    "    print(\"\\n### Preparing FEVER ###\")\n",
    "\n",
    "    queries = load_dataset(\"BeIR/fever\", \"queries\", split=\"queries\")\n",
    "    corpus  = load_dataset(\"BeIR/fever\", \"corpus\",  split=\"corpus\")\n",
    "    pos_dict = load_qrels_fever()\n",
    "    sample_q = queries[0]\n",
    "    qid_key  = next((k for k in sample_q.keys() if \"id\" in k.lower()), None)\n",
    "    text_key = next((k for k in sample_q.keys() if k.lower() in {\"text\",\"query\",\"body\"}), None)\n",
    "    if not (qid_key and text_key):\n",
    "        raise RuntimeError(\"fever: 无法检测到 queries 中的字段。\")\n",
    "    sample_d = corpus[0]\n",
    "    doc_id_key   = next((k for k in sample_d.keys() if \"id\" in k.lower()), None)\n",
    "    doc_text_key = next((k for k in sample_d.keys()\n",
    "                         if \"text\" in k.lower() or \"body\" in k.lower() or \"passage\" in k.lower()), None)\n",
    "    if not (doc_id_key and doc_text_key):\n",
    "        raise RuntimeError(\"fever: 无法检测到 corpus 中的字段。\")\n",
    "    cand = [\n",
    "        {\"id\": str(q[qid_key]), \"text\": q[text_key]}\n",
    "        for q in queries\n",
    "        if (str(q[qid_key]) in pos_dict) and isinstance(q.get(text_key), str) and is_good_q(q[text_key])\n",
    "    ]\n",
    "    if len(cand) == 0:\n",
    "        raise RuntimeError(\"fever: no query！\")\n",
    "    if len(cand) < N_Q:\n",
    "        qs = random.choices(cand, k=N_Q)\n",
    "    else:\n",
    "        qs = random.sample(cand, N_Q)\n",
    "\n",
    "    pos_ids = { pos_dict[q[\"id\"]][0] for q in qs }\n",
    "    all_doc_ids = [str(d[doc_id_key]) for d in corpus]\n",
    "    other_ids = [did for did in all_doc_ids if did not in pos_ids]\n",
    "    if len(other_ids) < N_DOCS - len(pos_ids):\n",
    "        raise RuntimeError(f\"fever: 填充文档不足（需要 {N_DOCS - len(pos_ids)}, 只有 {len(other_ids)}）。\")\n",
    "    fillers = random.sample(other_ids, N_DOCS - len(pos_ids))\n",
    "    sel_ids = list(pos_ids) + fillers\n",
    "    id2text = { str(d[doc_id_key]): d[doc_text_key] for d in corpus }\n",
    "    docs = [{\"id\": did, \"text\": id2text[did]} for did in sel_ids]\n",
    "    pos_text = { did: id2text[did][:120].replace(\"\\n\", \" \") for did in pos_ids }\n",
    "    cpu_bert = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(\"cpu\")\n",
    "    CLS_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(0, N_DOCS, BATCH_CLS), desc=\"CLS-fever\"):\n",
    "            batch = docs[i : i + BATCH_CLS]\n",
    "            batch_texts = [item[\"text\"] for item in batch]\n",
    "            out = cpu_bert(**enc(batch_texts, \"cpu\")).last_hidden_state[:, 0, :]\n",
    "            CLS_list.append(out)\n",
    "    C_CLS = torch.cat(CLS_list, dim=0)\n",
    "    del cpu_bert\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return docs, qs, pos_dict, pos_text, C_CLS\n",
    "\n",
    "# ─────────────── 主实验函数 ───────────────\n",
    "def run_loss_compare_fever():\n",
    "    out_dir = \"exp_results/loss_compare/fever\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # 1. 组装数据\n",
    "    docs, qs, pos_dict, pos_text, C_CLS = prepare_fever()\n",
    "\n",
    "    # 用 GPU 加载 bert（用于对抗）\n",
    "    global bert\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\",\n",
    "                                     torch_dtype=torch.float16).to(DEVICE).eval()\n",
    "\n",
    "    rec = []  \n",
    "    for mode in (\"hinge\", \"cos\"):\n",
    "        pbar = tqdm.tqdm(qs, desc=f\"fever-{mode}\")\n",
    "        for q in pbar:\n",
    "            qtxt = q[\"text\"]\n",
    "            qid  = q[\"id\"]\n",
    "            ids, msk = get_ids_msk(qtxt, DEVICE)\n",
    "            with torch.no_grad():\n",
    "                qcls = bert(**enc([qtxt], DEVICE)).last_hidden_state[:, 0, :]\n",
    "\n",
    "            sims = cos_row(qcls.cpu(), C_CLS)\n",
    "            order = torch.argsort(sims, descending=True)\n",
    "            true_pos_id      = pos_dict[qid][0]\n",
    "            true_pos_excerpt = pos_text[true_pos_id]\n",
    "            tgt_idx       = int(order[RANK_TARGET - 1].item())\n",
    "            tgt_id        = docs[tgt_idx][\"id\"]\n",
    "            tgt_excerpt   = docs[tgt_idx][\"text\"][:120].replace(\"\\n\", \" \")\n",
    "            baseline_sim  = sims[tgt_idx].item()\n",
    "            pos_rank    = None\n",
    "            pos_excerpt = None\n",
    "            tgt_cls = C_CLS[tgt_idx : tgt_idx + 1].to(DEVICE)\n",
    "            CP = torch.cat([C_CLS[:tgt_idx], C_CLS[tgt_idx+1:]], dim=0)\n",
    "\n",
    "            suf, cls_adv, iters = de_opt(ids, msk, CP, tgt_cls, loss_mode=mode)\n",
    "            new_sims = cos_row(cls_adv.cpu(), C_CLS)\n",
    "            fr = (new_sims > new_sims[tgt_idx]).sum().item() + 1\n",
    "            entry = {\n",
    "                \"dataset\":             \"fever\",\n",
    "                \"loss\":                mode,\n",
    "                \"query\":               qtxt,\n",
    "                \"orig_answer_id\":      true_pos_id,\n",
    "                \"orig_answer_excerpt\": true_pos_excerpt,\n",
    "                \"tgt_id\":              tgt_id,\n",
    "                \"tgt_excerpt\":         tgt_excerpt,\n",
    "                \"suffix\":              suffix_str(suf),\n",
    "                \"suffix_len\":          len(suf),\n",
    "                \"suffix_token_ids\":    json.dumps(suf),\n",
    "                \"baseline_rank\":       RANK_TARGET,\n",
    "                \"final_rank\":          fr,\n",
    "                \"delta_rank\":          RANK_TARGET - fr,\n",
    "                \"delta_cos\":           new_sims[tgt_idx].item() - baseline_sim,\n",
    "                \"delta_ndcg\":          dcg(fr) - dcg(RANK_TARGET),\n",
    "                \"success\":             int(fr == 1),\n",
    "                \"iter_used\":           iters,\n",
    "                \"pos_rank\":            pos_rank,\n",
    "                \"pos_excerpt\":         pos_excerpt,\n",
    "            }\n",
    "            rec.append(entry)\n",
    "\n",
    "    df = pd.DataFrame(rec)\n",
    "    df.to_csv(f\"{out_dir}/loss_detail.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    agg = df.groupby(\"loss\").agg(\n",
    "        success_rate   = (\"success\",   \"mean\"),\n",
    "        avg_iters      = (\"iter_used\", \"mean\"),\n",
    "        avg_delta_rank = (\"delta_rank\",\"mean\"),\n",
    "        avg_delta_cos  = (\"delta_cos\", \"mean\"),\n",
    "    ).reset_index()\n",
    "    agg.to_csv(f\"{out_dir}/loss_aggregate.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    h = agg.loc[agg.loss == \"hinge\", \"success_rate\"].values[0]\n",
    "    c = agg.loc[agg.loss == \"cos\",   \"success_rate\"].values[0]\n",
    "    print(f\"✓ fever done  –  hinge success = {h:.2%},  cos success = {c:.2%}\")\n",
    "    del bert\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ─────────────── 主入口 ───────────────\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"exp_results/loss_compare/fever\", exist_ok=True)\n",
    "    run_loss_compare_fever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2211713-3a3c-41cc-8639-1a19bd2c5d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Preparing FiQA-2018 ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS-fiqa: 100%|█████████████████████████████████████████████████████████████████████████| 63/63 [03:24<00:00,  3.25s/it]\n",
      "fiqa-hinge: 100%|███████████████████████████████████████████████████████████████████| 100/100 [1:17:11<00:00, 46.31s/it]\n",
      "fiqa-cos: 100%|█████████████████████████████████████████████████████████████████████| 100/100 [1:09:47<00:00, 41.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ fiqa done  –  hinge success = 23.00%,  cos success = 2.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_DOCS, N_Q  = 1_000, 100       \n",
    "TAIL_L       = 5                 \n",
    "RANK_TARGET  = 100               \n",
    "POP, MAXITER = 20, 2_000         \n",
    "PATIENCE     = 20                \n",
    "BATCH_CLS    = 16               \n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "VOC = tok.vocab_size \n",
    "def enc(txts, dev):\n",
    "\n",
    "    return tok(\n",
    "        txts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(dev)\n",
    "\n",
    "def get_ids_msk(text, dev):\n",
    "    e = enc([text], dev)\n",
    "    return e[\"input_ids\"][0], e[\"attention_mask\"][0]\n",
    "\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1\n",
    "    )\n",
    "\n",
    "def dcg(rank: int) -> float:\n",
    "    return 1.0 / math.log2(rank + 1)\n",
    "\n",
    "def is_good_q(text: str) -> bool:\n",
    "    if \"?\" not in text:\n",
    "        return False\n",
    "    n = len(tok(text)[\"input_ids\"]) - 2\n",
    "    return (15 < n < 500)\n",
    "\n",
    "def suffix_str(ids):\n",
    "    return \" \".join(tok.convert_ids_to_tokens(ids))\n",
    "\n",
    "def de_opt(ids, msk, CP, tgt_cls, loss_mode=\"hinge\"):\n",
    "\n",
    "    L = TAIL_L\n",
    "    pos = list(range(512 - L, 512))\n",
    "    bounds = [(0, VOC - 1)] * L\n",
    "\n",
    "    def loss_from_cls(c):\n",
    "        # c: shape [1, dim]\n",
    "        if loss_mode == \"cos\":\n",
    "            return -(torch.nn.functional.cosine_similarity(c, tgt_cls)[0]).item()\n",
    "        kth = torch.topk(cos_row(c, CP), 1).values[-1]\n",
    "        sim = torch.nn.functional.cosine_similarity(c, tgt_cls)[0]\n",
    "        return max(0.0, (kth - sim).item())\n",
    "\n",
    "    def obj(v):\n",
    "        \n",
    "        v = [int(round(x)) for x in v]\n",
    "        p, m = ids.clone(), msk.clone()\n",
    "        for i, t in zip(pos, v):\n",
    "            p[i] = t\n",
    "        m[pos] = 1\n",
    "        cls = bert(input_ids=p.unsqueeze(0),\n",
    "                   attention_mask=m.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "        return loss_from_cls(cls)\n",
    "\n",
    "    best, stale = 1e9, 0\n",
    "    def cb(xk, _):\n",
    "        nonlocal best, stale\n",
    "        cur = obj(xk)\n",
    "        if cur < best:\n",
    "            best, stale = cur, 0\n",
    "        else:\n",
    "            stale += 1\n",
    "        return (cur == 0) or (stale >= PATIENCE)\n",
    "\n",
    "    res = differential_evolution(\n",
    "        obj,\n",
    "        bounds,\n",
    "        popsize=POP,\n",
    "        maxiter=MAXITER,\n",
    "        tol=0,\n",
    "        polish=False,\n",
    "        seed=SEED,\n",
    "        callback=cb\n",
    "    )\n",
    "    suf = [int(round(x)) for x in res.x]\n",
    "    p, m = ids.clone(), msk.clone()\n",
    "    m[pos] = 1\n",
    "    for i, t in zip(pos, suf):\n",
    "        p[i] = t\n",
    "    cls = bert(input_ids=p.unsqueeze(0),\n",
    "               attention_mask=m.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "    return suf, cls, res.nfev\n",
    "\n",
    "def load_qrels_fiqa():\n",
    "    qrels_all = load_dataset(\"BeIR/fiqa-qrels\")\n",
    "    qrels = qrels_all[\"test\"]\n",
    "    sample = qrels[0]\n",
    "    r_qid_key   = next((k for k in sample.keys() if \"query\" in k.lower()), None)\n",
    "    r_doc_key   = next((k for k in sample.keys() if \"corpus\" in k.lower() or \"doc\" in k.lower()), None)\n",
    "    r_score_key = next((k for k in sample.keys() if k.lower() in {\"score\",\"label\",\"relevance\"}), None)\n",
    "    if not (r_qid_key and r_doc_key and r_score_key):\n",
    "        raise RuntimeError(\"fiqa: NOT ACCEPT。\")\n",
    "    pos_dict = {}\n",
    "    for r in qrels:\n",
    "        if int(r[r_score_key]) > 0:\n",
    "            qid = str(r[r_qid_key])\n",
    "            did = str(r[r_doc_key])\n",
    "            pos_dict.setdefault(qid, []).append(did)\n",
    "    if not pos_dict:\n",
    "        raise RuntimeError(\"fiqa: qrels NO！\")\n",
    "    return pos_dict\n",
    "\n",
    "def prepare_fiqa():\n",
    "    print(\"\\n### Preparing FiQA-2018 ###\")\n",
    "    queries = load_dataset(\"BeIR/fiqa\", \"queries\", split=\"queries\")\n",
    "    corpus  = load_dataset(\"BeIR/fiqa\", \"corpus\",  split=\"corpus\")\n",
    "    pos_dict = load_qrels_fiqa()\n",
    "    sample_q = queries[0]\n",
    "    qid_key  = next((k for k in sample_q.keys() if \"id\" in k.lower()), None)\n",
    "    text_key = next((k for k in sample_q.keys() if k.lower() in {\"text\",\"query\",\"body\"}), None)\n",
    "    if not (qid_key and text_key):\n",
    "        raise RuntimeError(\"fiqa: NO QUERY。\")\n",
    "    sample_d = corpus[0]\n",
    "    doc_id_key   = next((k for k in sample_d.keys() if \"id\" in k.lower()), None)\n",
    "    doc_text_key = next((k for k in sample_d.keys() \n",
    "                         if \"text\" in k.lower() or \"body\" in k.lower() or \"passage\" in k.lower()), None)\n",
    "    if not (doc_id_key and doc_text_key):\n",
    "        raise RuntimeError(\"fiqa: NO CORPUS。\")\n",
    "    cand = [\n",
    "        {\"id\": str(q[qid_key]), \"text\": q[text_key]}\n",
    "        for q in queries\n",
    "        if (str(q[qid_key]) in pos_dict) and isinstance(q.get(text_key), str) and is_good_q(q[text_key])\n",
    "    ]\n",
    "    if len(cand) == 0:\n",
    "        raise RuntimeError(\"fiqa: OUT OF query！\")\n",
    "    if len(cand) < N_Q:\n",
    "        qs = random.choices(cand, k=N_Q)\n",
    "    else:\n",
    "        qs = random.sample(cand, N_Q)\n",
    "    pos_ids = { pos_dict[q[\"id\"]][0] for q in qs }\n",
    "    all_doc_ids = [str(d[doc_id_key]) for d in corpus]\n",
    "    other_ids = [did for did in all_doc_ids if did not in pos_ids]\n",
    "    if len(other_ids) < N_DOCS - len(pos_ids):\n",
    "        raise RuntimeError(f\"fiqa: NO（,NEED {N_DOCS - len(pos_ids)}, ONLY {len(other_ids)}）。\")\n",
    "    fillers = random.sample(other_ids, N_DOCS - len(pos_ids))\n",
    "    sel_ids = list(pos_ids) + fillers\n",
    "    id2text = { str(d[doc_id_key]): d[doc_text_key] for d in corpus }\n",
    "    docs = [{\"id\": did, \"text\": id2text[did]} for did in sel_ids]\n",
    "    pos_text = { did: id2text[did][:120].replace(\"\\n\", \" \") for did in pos_ids }\n",
    "    cpu_bert = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(\"cpu\")\n",
    "    CLS_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(0, N_DOCS, BATCH_CLS), desc=\"CLS-fiqa\"):\n",
    "            batch = docs[i : i + BATCH_CLS]\n",
    "            batch_texts = [item[\"text\"] for item in batch]\n",
    "            out = cpu_bert(**enc(batch_texts, \"cpu\")).last_hidden_state[:, 0, :]\n",
    "            CLS_list.append(out)\n",
    "    C_CLS = torch.cat(CLS_list, dim=0)\n",
    "    del cpu_bert\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return docs, qs, pos_dict, pos_text, C_CLS\n",
    "def run_loss_compare_fiqa():\n",
    "    out_dir = \"exp_results/loss_compare/fiqa\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    " \n",
    "    docs, qs, pos_dict, pos_text, C_CLS = prepare_fiqa()\n",
    "\n",
    "    global bert\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\",\n",
    "                                     torch_dtype=torch.float16).to(DEVICE).eval()\n",
    "\n",
    "    rec = [] \n",
    "\n",
    "    for mode in (\"hinge\", \"cos\"):\n",
    "        pbar = tqdm.tqdm(qs, desc=f\"fiqa-{mode}\")\n",
    "        for q in pbar:\n",
    "            qtxt = q[\"text\"]\n",
    "            qid  = q[\"id\"]\n",
    "\n",
    "            ids, msk = get_ids_msk(qtxt, DEVICE)\n",
    "            with torch.no_grad():\n",
    "                qcls = bert(**enc([qtxt], DEVICE)).last_hidden_state[:, 0, :]\n",
    "\n",
    "            sims = cos_row(qcls.cpu(), C_CLS)\n",
    "            order = torch.argsort(sims, descending=True)\n",
    "\n",
    "            true_pos_id      = pos_dict[qid][0]\n",
    "            true_pos_excerpt = pos_text[true_pos_id]\n",
    "\n",
    "            tgt_idx       = int(order[RANK_TARGET - 1].item())\n",
    "            tgt_id        = docs[tgt_idx][\"id\"]\n",
    "            tgt_excerpt   = docs[tgt_idx][\"text\"][:120].replace(\"\\n\", \" \")\n",
    "            baseline_sim  = sims[tgt_idx].item()\n",
    "            pos_rank    = None\n",
    "            pos_excerpt = None\n",
    "            tgt_cls = C_CLS[tgt_idx : tgt_idx + 1].to(DEVICE)\n",
    "            CP = torch.cat([C_CLS[:tgt_idx], C_CLS[tgt_idx+1:]], dim=0)\n",
    "\n",
    "            suf, cls_adv, iters = de_opt(ids, msk, CP, tgt_cls, loss_mode=mode)\n",
    "            new_sims = cos_row(cls_adv.cpu(), C_CLS)\n",
    "            fr = (new_sims > new_sims[tgt_idx]).sum().item() + 1\n",
    "            entry = {\n",
    "                \"dataset\":            \"fiqa\",\n",
    "                \"loss\":               mode,\n",
    "                \"query\":              qtxt,\n",
    "\n",
    "                \"orig_answer_id\":      true_pos_id,\n",
    "                \"orig_answer_excerpt\": true_pos_excerpt,\n",
    "                \"tgt_id\":             tgt_id,\n",
    "                \"tgt_excerpt\":        tgt_excerpt,\n",
    "\n",
    "                \"suffix\":             suffix_str(suf),\n",
    "                \"suffix_len\":         len(suf),\n",
    "                \"suffix_token_ids\":   json.dumps(suf),\n",
    "                \"baseline_rank\":      RANK_TARGET,\n",
    "                \"final_rank\":         fr,\n",
    "                \"delta_rank\":         RANK_TARGET - fr,\n",
    "                \"delta_cos\":          new_sims[tgt_idx].item() - baseline_sim,\n",
    "                \"delta_ndcg\":         dcg(fr) - dcg(RANK_TARGET),\n",
    "                \"success\":            int(fr == 1),\n",
    "                \"iter_used\":          iters,\n",
    "                \"pos_rank\":           pos_rank,\n",
    "                \"pos_excerpt\":        pos_excerpt,\n",
    "            }\n",
    "\n",
    "            rec.append(entry)\n",
    "\n",
    "    df = pd.DataFrame(rec)\n",
    "    df.to_csv(f\"{out_dir}/loss_detail.csv\", index=False, encoding=\"utf-8\")\n",
    "    agg = df.groupby(\"loss\").agg(\n",
    "        success_rate   = (\"success\",   \"mean\"),\n",
    "        avg_iters      = (\"iter_used\", \"mean\"),\n",
    "        avg_delta_rank = (\"delta_rank\",\"mean\"),\n",
    "        avg_delta_cos  = (\"delta_cos\", \"mean\"),\n",
    "    ).reset_index()\n",
    "    agg.to_csv(f\"{out_dir}/loss_aggregate.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    h = agg.loc[agg.loss == \"hinge\", \"success_rate\"].values[0]\n",
    "    c = agg.loc[agg.loss == \"cos\",   \"success_rate\"].values[0]\n",
    "    print(f\"✓ fiqa done  –  hinge success = {h:.2%},  cos success = {c:.2%}\")\n",
    "    del bert\n",
    "    torch.cuda.empty_cache()\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"exp_results/loss_compare/fiqa\", exist_ok=True)\n",
    "    run_loss_compare_fiqa()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b8fac-d591-4517-b9ba-3a06b1d1ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for ds_name in FILES.keys():\n",
    "    for topk, linestyle, marker in [(1, '-', 'o'), (10, '--', 's')]:\n",
    "        sub = summary_df[\n",
    "            (summary_df['Dataset'] == ds_name) &\n",
    "            (summary_df['TopK'] == topk)\n",
    "        ].sort_values('suffix_len')\n",
    "        plt.plot(\n",
    "            sub['suffix_len'],\n",
    "            sub['mean_delta_rank'],\n",
    "            label=f\"{ds_name} Top-{topk}\",\n",
    "            color=COLORS[ds_name],\n",
    "            linestyle=linestyle,\n",
    "            marker=marker,\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "plt.axvline(x=4.5, color='gray', linestyle='--', linewidth=1)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.fill_betweenx([ymin, ymax], 4.5, 10, color='gray', alpha=0.15)\n",
    "plt.text(5.2, ymax * 0.9, 'Marginal gains ≈ 0 for L ≥ 5', fontsize=9, color='gray')\n",
    "\n",
    "plt.xlabel('Suffix Length (L)')\n",
    "plt.ylabel('Mean ΔRank')\n",
    "plt.title('Mean ΔRank vs. Suffix Length – MSMARCO/Fiqa/nq (Top-1 & Top-10)')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('delta_rank.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for ds_name in FILES.keys():\n",
    "    for topk, linestyle, marker in [(1, '-', 'o'), (10, '--', 's')]:\n",
    "        sub = summary_df[\n",
    "            (summary_df['Dataset'] == ds_name) &\n",
    "            (summary_df['TopK'] == topk)\n",
    "        ].sort_values('suffix_len')\n",
    "        plt.plot(\n",
    "            sub['suffix_len'],\n",
    "            sub['success_rate'],\n",
    "            label=f\"{ds_name} Top-{topk}\",\n",
    "            color=COLORS[ds_name],\n",
    "            linestyle=linestyle,\n",
    "            marker=marker,\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "plt.xlabel('Suffix Length (L)')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.title('Success Rate vs. Suffix Length – MSMARCO/Fiqa/nq (Top-1 & Top-10)')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc='lower right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig('success_rate.png', dpi=300)\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 5))\n",
    "for ds_name in FILES.keys():\n",
    "    for topk, linestyle, marker in [(1, '-', 'o'), (10, '--', 's')]:\n",
    "        mg_df = marginal_dict[(ds_name, topk)]\n",
    "        plt.plot(\n",
    "            mg_df['suffix_len'],\n",
    "            mg_df['marginal_gain'],\n",
    "            label=f\"{ds_name} Top-{topk}\",\n",
    "            color=COLORS[ds_name],\n",
    "            linestyle=linestyle,\n",
    "            marker=marker,\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "plt.axhline(y=0, color='gray', linestyle='-', linewidth=1)\n",
    "plt.axvline(x=5, color='red', linestyle='--', linewidth=1)\n",
    "plt.text(5.2, plt.ylim()[1] * 0.8, 'L = 5 cutoff', color='red', fontsize=9)\n",
    "\n",
    "plt.xlabel('Suffix Length (L)')\n",
    "plt.ylabel('Marginal Gain (ΔRank difference)')\n",
    "plt.title('Marginal Gain vs. Suffix Length – MSMARCO/Fiqa/nq (Top-1 & Top-10)')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc='upper right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('marginal_gain.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdd5b14-6930-4e23-9d52-8b507ef27693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "FILES = {\n",
    "    'MSMARCO': 'exp_results/msmarco/ablation/ablation_results.csv',\n",
    "    'Fiqa':    'exp_results/ablation/ablation_results.csv',\n",
    "    'nq':      'exp_results/nq/ablation/ablation_results.csv'\n",
    "}\n",
    "COLORS = {\n",
    "    'MSMARCO': 'tab:blue',\n",
    "    'Fiqa':    'tab:orange',\n",
    "    'nq':      'tab:green'\n",
    "}\n",
    "def load_and_truncate(file_path, dataset_name, topk_focus, keep_per_group=100):\n",
    "    if not pathlib.Path(file_path).exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'topK' in df.columns:\n",
    "        topk_col = 'topK'\n",
    "    elif 'top_k' in df.columns:\n",
    "        topk_col = 'top_k'\n",
    "    else:\n",
    "        raise KeyError(f\"Cannot find 'topK' or 'top_k' column in {file_path}\")\n",
    "\n",
    "    df['Dataset'] = dataset_name\n",
    "    df_trunc = (\n",
    "        df\n",
    "        .groupby([topk_col, 'suffix_len'], group_keys=False)\n",
    "        .apply(lambda grp: grp.iloc[:keep_per_group])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    df_focus = df_trunc[df_trunc[topk_col] == topk_focus].copy()\n",
    "    df_focus['TopK'] = topk_focus\n",
    "    return df_focus\n",
    "data_frames = []\n",
    "for ds_name, path in FILES.items():\n",
    "    for topk in [1, 10]:\n",
    "        df_focus = load_and_truncate(path, ds_name, topk_focus=topk, keep_per_group=100)\n",
    "        data_frames.append(df_focus)\n",
    "combined = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "def compute_summary(df):\n",
    "    summary = (\n",
    "        df\n",
    "        .groupby(['Dataset', 'TopK', 'suffix_len'], as_index=False)\n",
    "        .agg(\n",
    "            mean_delta_rank=('delta_rank', 'mean'),\n",
    "            success_rate   =('success',    'mean'),\n",
    "            mean_delta_cos =('delta_cos',  'mean'),\n",
    "            mean_delta_ndcg=('delta_ndcg', 'mean')\n",
    "        )\n",
    "    )\n",
    "    return summary\n",
    "\n",
    "summary_df = compute_summary(combined)\n",
    "\n",
    "def compute_marginal_gain(summary_df, metric_key):\n",
    "    df = summary_df.copy().sort_values('suffix_len').reset_index(drop=True)\n",
    "    gains = [np.nan] \n",
    "    for i in range(1, len(df)):\n",
    "        gains.append(df.loc[i, metric_key] - df.loc[i-1, metric_key])\n",
    "    df['marginal_gain'] = gains\n",
    "    return df\n",
    "marginal_dict = {}\n",
    "for ds_name in FILES.keys():\n",
    "    for topk in [1, 10]:\n",
    "        sub = summary_df[\n",
    "            (summary_df['Dataset'] == ds_name) &\n",
    "            (summary_df['TopK'] == topk)\n",
    "        ][['suffix_len', 'mean_delta_rank']].copy()\n",
    "        mg = compute_marginal_gain(sub, 'mean_delta_rank')\n",
    "        marginal_dict[(ds_name, topk)] = mg\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for ds_name in FILES.keys():\n",
    "    for topk, linestyle, marker in [(1, '-', 'o'), (10, '--', 's')]:\n",
    "        sub = summary_df[\n",
    "            (summary_df['Dataset'] == ds_name) &\n",
    "            (summary_df['TopK'] == topk)\n",
    "        ].sort_values('suffix_len')\n",
    "        plt.plot(\n",
    "            sub['suffix_len'],\n",
    "            sub['mean_delta_rank'],\n",
    "            label=f\"{ds_name} Top-{topk}\",\n",
    "            color=COLORS[ds_name],\n",
    "            linestyle=linestyle,\n",
    "            marker=marker,\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "plt.axvline(x=4.5, color='gray', linestyle='--', linewidth=1)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.fill_betweenx([ymin, ymax], 4.5, 10, color='gray', alpha=0.15)\n",
    "plt.text(5.2, ymax * 0.9, 'Marginal gains ≈ 0 for L ≥ 5', fontsize=9, color='gray')\n",
    "\n",
    "plt.xlabel('Suffix Length (L)')\n",
    "plt.ylabel('Mean ΔRank')\n",
    "plt.title('Mean ΔRank vs. Suffix Length – MSMARCO/Fiqa/nq (Top-1 & Top-10)')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 5))\n",
    "for ds_name in FILES.keys():\n",
    "    for topk, linestyle, marker in [(1, '-', 'o'), (10, '--', 's')]:\n",
    "        sub = summary_df[\n",
    "            (summary_df['Dataset'] == ds_name) &\n",
    "            (summary_df['TopK'] == topk)\n",
    "        ].sort_values('suffix_len')\n",
    "        plt.plot(\n",
    "            sub['suffix_len'],\n",
    "            sub['success_rate'],\n",
    "            label=f\"{ds_name} Top-{topk}\",\n",
    "            color=COLORS[ds_name],\n",
    "            linestyle=linestyle,\n",
    "            marker=marker,\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "plt.xlabel('Suffix Length (L)')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.title('Success Rate vs. Suffix Length – MSMARCO/Fiqa/nq (Top-1 & Top-10)')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc='lower right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 5))\n",
    "for ds_name in FILES.keys():\n",
    "    for topk, linestyle, marker in [(1, '-', 'o'), (10, '--', 's')]:\n",
    "        mg_df = marginal_dict[(ds_name, topk)]\n",
    "        plt.plot(\n",
    "            mg_df['suffix_len'],\n",
    "            mg_df['marginal_gain'],\n",
    "            label=f\"{ds_name} Top-{topk}\",\n",
    "            color=COLORS[ds_name],\n",
    "            linestyle=linestyle,\n",
    "            marker=marker,\n",
    "            linewidth=2\n",
    "        )\n",
    "\n",
    "plt.axhline(y=0, color='gray', linestyle='-', linewidth=1)\n",
    "plt.axvline(x=5, color='red', linestyle='--', linewidth=1)\n",
    "plt.text(5.2, plt.ylim()[1] * 0.8, 'L = 5 cutoff', color='red', fontsize=9)\n",
    "\n",
    "plt.xlabel('Suffix Length (L)')\n",
    "plt.ylabel('Marginal Gain (ΔRank difference)')\n",
    "plt.title('Marginal Gain vs. Suffix Length – MSMARCO/Fiqa/nq (Top-1 & Top-10)')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(loc='upper right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9786c22e-0b16-43ac-8993-45ac78e85f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Loss-comparison experiment  –  Hinge vs. Cosine (针对 SciFact)\n",
    "--------------------------------------------------------------\n",
    "Datasets  : SciFact （BEIR/scifact）\n",
    "Corpus    : 1 000 docs · 100 queries（保证每条 query 至少有 1 个正例文档）\n",
    "Target    : baseline rank = 100\n",
    "Tail-patch suffix length = 5 tokens\n",
    "Outputs   : exp_results/loss_compare/scifact/{loss_detail.csv, loss_aggregate.csv}\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ─────────────── 全局配置 ───────────────\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_DOCS, N_Q  = 1_000, 100         # 1 000 篇文档（含每条 query 的首个正例），随机抽 100 条 query\n",
    "TAIL_L       = 5                 # suffix 长度 = 5 tokens\n",
    "RANK_TARGET  = 100               # baseline 排名第 100 的文档作为攻击目标\n",
    "POP, MAXITER = 20, 2_000         # DE 的种群大小、最大迭代次数\n",
    "PATIENCE     = 20                # DE callback 早停阈值\n",
    "BATCH_CLS    = 16                # CPU-BERT 计算 CLS 时的 batch size\n",
    "\n",
    "# 加载 BERT tokenizer\n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "VOC = tok.vocab_size  # 词表大小，用于 DE 优化时的 token bound\n",
    "\n",
    "# ─────────────── 辅助函数 ───────────────\n",
    "def enc(txts, dev):\n",
    "    \"\"\"\n",
    "    对一批文本 txts 进行 BERT 编码（padding & truncation），并返回 dict{'input_ids','attention_mask'}，\n",
    "    其中所有 tensor 都被 .to(dev)。\n",
    "    \"\"\"\n",
    "    return tok(\n",
    "        txts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(dev)\n",
    "\n",
    "def get_ids_msk(text, dev):\n",
    "    \"\"\"\n",
    "    针对单条 text，返回 (input_ids, attention_mask)，每个 shape 都是 (512,)。\n",
    "    \"\"\"\n",
    "    e = enc([text], dev)\n",
    "    return e[\"input_ids\"][0], e[\"attention_mask\"][0]\n",
    "\n",
    "def cos_row(x, Y):\n",
    "    \"\"\"\n",
    "    给定单个 vector x (shape: [1, dim])，和一组 vectors Y (shape: [N, dim])，\n",
    "    返回 x 与 Y 中每一行的 cosine similarity，结果是长度 N 的 tensor。\n",
    "    \"\"\"\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1\n",
    "    )\n",
    "\n",
    "def dcg(rank: int) -> float:\n",
    "    \"\"\"\n",
    "    计算 DCG（Discounted Cumulative Gain）指标的基础函数：1/log2(rank+1)\n",
    "    \"\"\"\n",
    "    return 1.0 / math.log2(rank + 1)\n",
    "\n",
    "def is_good_q(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    过滤 query 的函数：只要 token 长度（扣去 [CLS] [SEP]）在 (10, 500) 之间即可。\n",
    "    \"\"\"\n",
    "    n = len(tok(text)[\"input_ids\"]) - 2\n",
    "    return (10 < n < 500)\n",
    "\n",
    "def suffix_str(ids):\n",
    "    \"\"\"\n",
    "    给定一个 token id 列表 ids（例如 [2134, 765, 56, ...]），\n",
    "    返回对应的 token string：“token1 token2 token3 ...”。\n",
    "    \"\"\"\n",
    "    return \" \".join(tok.convert_ids_to_tokens(ids))\n",
    "\n",
    "# ─────────────── DE 优化器 ───────────────\n",
    "def de_opt(ids, msk, CP, tgt_cls, loss_mode=\"hinge\"):\n",
    "    \"\"\"\n",
    "    对 single-query single-target 进行 Differential Evolution 优化，寻找 L=TAIL_L 个 token，\n",
    "    使得对抗后的 query CLS (cls) 与 target_cls 之间满足 hinge 或 cos loss 最小。\n",
    "    参数：\n",
    "      - ids, msk：query 对应的 input_ids/attention_mask\n",
    "      - CP：排除了目标文档在外的所有 doc CLS（shape: [N_DOCS-1, dim]），用于 hinge “取第 1 阈值”\n",
    "      - tgt_cls：目标文档 CLS（shape: [1, dim]）\n",
    "      - loss_mode： \"hinge\" 或 \"cos\"\n",
    "    返回：\n",
    "      - suf：长度 = L 的 token id 列表\n",
    "      - cls：suffix 拼接后对抗 query 的 CLS embedding\n",
    "      - res.nfev：DE 优化过程中的函数评估次数\n",
    "    \"\"\"\n",
    "    L = TAIL_L\n",
    "    pos = list(range(512 - L, 512))\n",
    "    bounds = [(0, VOC - 1)] * L\n",
    "\n",
    "    def loss_from_cls(c):\n",
    "        # c: shape [1, dim]\n",
    "        if loss_mode == \"cos\":\n",
    "            # cos loss = - cosine_similarity(c, tgt_cls)\n",
    "            return -(torch.nn.functional.cosine_similarity(c, tgt_cls)[0]).item()\n",
    "        # hinge loss: kth = 最接近 c 的“其他文档”得分（top-1），sim = c 与 tgt_cls 的 cos\n",
    "        kth = torch.topk(cos_row(c, CP), 1).values[-1]\n",
    "        sim = torch.nn.functional.cosine_similarity(c, tgt_cls)[0]\n",
    "        return max(0.0, (kth - sim).item())\n",
    "\n",
    "    def obj(v):\n",
    "        # v: 长度 L 的浮点列表（DE 内部传递过来）\n",
    "        v = [int(round(x)) for x in v]\n",
    "        p, m = ids.clone(), msk.clone()\n",
    "        for i, t in zip(pos, v):\n",
    "            p[i] = t\n",
    "        m[pos] = 1\n",
    "        cls = bert(input_ids=p.unsqueeze(0),\n",
    "                   attention_mask=m.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "        return loss_from_cls(cls)\n",
    "\n",
    "    best, stale = 1e9, 0\n",
    "    def cb(xk, _):\n",
    "        nonlocal best, stale\n",
    "        cur = obj(xk)\n",
    "        if cur < best:\n",
    "            best, stale = cur, 0\n",
    "        else:\n",
    "            stale += 1\n",
    "        return (cur == 0) or (stale >= PATIENCE)\n",
    "\n",
    "    res = differential_evolution(\n",
    "        obj,\n",
    "        bounds,\n",
    "        popsize=POP,\n",
    "        maxiter=MAXITER,\n",
    "        tol=0,\n",
    "        polish=False,\n",
    "        seed=SEED,\n",
    "        callback=cb\n",
    "    )\n",
    "    suf = [int(round(x)) for x in res.x]\n",
    "\n",
    "    # 用最优 suf 重新计算一次 CLS，作为返回值\n",
    "    p, m = ids.clone(), msk.clone()\n",
    "    m[pos] = 1\n",
    "    for i, t in zip(pos, suf):\n",
    "        p[i] = t\n",
    "    cls = bert(input_ids=p.unsqueeze(0),\n",
    "               attention_mask=m.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "    return suf, cls, res.nfev\n",
    "\n",
    "# ─────────────── QRELS 加载 ───────────────\n",
    "def load_qrels_scifact():\n",
    "    \"\"\"\n",
    "    为 SciFact 设计的 qrels 加载。返回 pos_dict: { str(query_id) : [ str(doc_id), ... ] }。\n",
    "    只保留 score > 0 的那部分。SciFact 的 qrels 存放在 “BeIR/scifact-qrels” 数据集中，使用 split=\"train\"。\n",
    "    \"\"\"\n",
    "    qrels_all = load_dataset(\"BeIR/scifact-qrels\", split=\"train\")\n",
    "    # 动态探测字段：通常是 'query-id', 'corpus-id', 'score'\n",
    "    sample = qrels_all[0]\n",
    "    r_qid_key   = next((k for k in sample.keys() if \"query\" in k.lower()), None)\n",
    "    r_doc_key   = next((k for k in sample.keys() if \"corpus\" in k.lower() or \"doc\" in k.lower()), None)\n",
    "    r_score_key = next((k for k in sample.keys() if k.lower() in {\"score\",\"label\",\"relevance\"}), None)\n",
    "    if not (r_qid_key and r_doc_key and r_score_key):\n",
    "        raise RuntimeError(\"scifact: 无法检测到 qrels 字段。\")\n",
    "    pos_dict = {}\n",
    "    for r in qrels_all:\n",
    "        if int(r[r_score_key]) > 0:\n",
    "            qid = str(r[r_qid_key])\n",
    "            did = str(r[r_doc_key])\n",
    "            pos_dict.setdefault(qid, []).append(did)\n",
    "    if not pos_dict:\n",
    "        raise RuntimeError(\"scifact: qrels 中没有任何正例！\")\n",
    "    return pos_dict\n",
    "\n",
    "# ─────────────── 数据集准备 ───────────────\n",
    "def prepare_scifact():\n",
    "    \"\"\"\n",
    "    1. 加载 BEIR/scifact 的 queries, corpus, qrels；\n",
    "    2. 从 qrels 构建 pos_dict；筛出 N_Q 条 is_good_q 的 query（且在 pos_dict 里必须有正例）；\n",
    "    3. 构建 docs 列表：保证每条 query 的第一个正例都在 N_DOCS 篇里，再随机补足到 N_DOCS；\n",
    "    4. 用 CPU 上的 BERT 计算 docs 的 CLS embedding → C_CLS。\n",
    "    返回：\n",
    "      - docs: [{\"id\": str_id, \"text\": str_text}, …]（长度 = N_DOCS）\n",
    "      - qs  : [{\"id\": str_id, \"text\": str_text}, …]（长度 = N_Q）\n",
    "      - pos_dict: { query_id: [doc_id1, …], … }\n",
    "      - pos_text: { doc_id: doc_text[:120], … }  （保存每条 query 首个正例的前 120 字作为 excerpt）\n",
    "      - C_CLS: shape = [N_DOCS, hidden_dim]，docs 对应的 CLS embedding\n",
    "    \"\"\"\n",
    "    print(\"\\n### Preparing SciFact ###\")\n",
    "\n",
    "    # ——— 1. 加载 queries / corpus / qrels ———\n",
    "    queries = load_dataset(\"BeIR/scifact\", \"queries\", split=\"queries\")\n",
    "    corpus  = load_dataset(\"BeIR/scifact\", \"corpus\",  split=\"corpus\")\n",
    "    pos_dict = load_qrels_scifact()\n",
    "\n",
    "    # 动态检测 queries 字段\n",
    "    sample_q = queries[0]\n",
    "    qid_key  = next((k for k in sample_q.keys() if \"id\" in k.lower()), None)\n",
    "    text_key = next((k for k in sample_q.keys() if k.lower() in {\"text\",\"query\",\"body\"}), None)\n",
    "    if not (qid_key and text_key):\n",
    "        raise RuntimeError(\"scifact: 无法检测到 queries 中的字段。\")\n",
    "\n",
    "    # 动态检测 corpus 字段\n",
    "    sample_d = corpus[0]\n",
    "    doc_id_key   = next((k for k in sample_d.keys() if \"id\" in k.lower()), None)\n",
    "    doc_text_key = next((k for k in sample_d.keys()\n",
    "                         if \"text\" in k.lower() or \"body\" in k.lower() or \"passage\" in k.lower()), None)\n",
    "    if not (doc_id_key and doc_text_key):\n",
    "        raise RuntimeError(\"scifact: 无法检测到 corpus 中的字段。\")\n",
    "\n",
    "    # ——— 2. 筛选 N_Q 条 query ———\n",
    "    cand = [\n",
    "        {\"id\": str(q[qid_key]), \"text\": q[text_key]}\n",
    "        for q in queries\n",
    "        if (str(q[qid_key]) in pos_dict) and isinstance(q.get(text_key), str) and is_good_q(q[text_key])\n",
    "    ]\n",
    "    if len(cand) == 0:\n",
    "        raise RuntimeError(\"scifact: 没有符合条件的 query！\")\n",
    "    if len(cand) < N_Q:\n",
    "        print(f\"⚠ 可选 query 只有 {len(cand)} 条，不足 {N_Q}，会重复采样。\")\n",
    "        qs = random.choices(cand, k=N_Q)\n",
    "    else:\n",
    "        qs = random.sample(cand, N_Q)\n",
    "\n",
    "    # ——— 3. 构建 docs 列表，确保每条 query 的第一个正例都在 N_DOCS 篇里 ———\n",
    "    pos_ids = { pos_dict[q[\"id\"]][0] for q in qs }\n",
    "    all_doc_ids = [str(d[doc_id_key]) for d in corpus]\n",
    "    other_ids = [did for did in all_doc_ids if did not in pos_ids]\n",
    "    if len(other_ids) < N_DOCS - len(pos_ids):\n",
    "        raise RuntimeError(f\"scifact: 填充文档不足（需要 {N_DOCS - len(pos_ids)}, 只有 {len(other_ids)}）。\")\n",
    "    fillers = random.sample(other_ids, N_DOCS - len(pos_ids))\n",
    "    sel_ids = list(pos_ids) + fillers\n",
    "\n",
    "    # 构建 docs 列表\n",
    "    id2text = { str(d[doc_id_key]): d[doc_text_key] for d in corpus }\n",
    "    docs = [{\"id\": did, \"text\": id2text[did]} for did in sel_ids]\n",
    "    pos_text = { did: id2text[did][:120].replace(\"\\n\", \" \") for did in pos_ids }\n",
    "\n",
    "    # ——— 4. 用 CPU 上的 BERT 计算 docs 的 CLS embedding ———\n",
    "    cpu_bert = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(\"cpu\")\n",
    "    CLS_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(0, N_DOCS, BATCH_CLS), desc=\"CLS-scifact\"):\n",
    "            batch = docs[i : i + BATCH_CLS]\n",
    "            batch_texts = [item[\"text\"] for item in batch]\n",
    "            out = cpu_bert(**enc(batch_texts, \"cpu\")).last_hidden_state[:, 0, :]\n",
    "            CLS_list.append(out)\n",
    "    C_CLS = torch.cat(CLS_list, dim=0)\n",
    "    del cpu_bert\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return docs, qs, pos_dict, pos_text, C_CLS\n",
    "\n",
    "# ─────────────── 主实验函数 ───────────────\n",
    "def run_loss_compare_scifact():\n",
    "    \"\"\"\n",
    "    1. 调用 prepare_scifact() 组装好 docs、qs、pos_dict、pos_text、C_CLS；\n",
    "    2. 对每条 query，先算真正正例作为 orig_top1，再算 baseline rank=100 文档信息，\n",
    "       然后用 hinge/cos 两种 loss mode 对该目标文档做 suffix_5 攻击，\n",
    "       收集记录并输出到 CSV。\n",
    "    \"\"\"\n",
    "    out_dir = \"exp_results/loss_compare/scifact\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # 1. 组装数据\n",
    "    docs, qs, pos_dict, pos_text, C_CLS = prepare_scifact()\n",
    "\n",
    "    # 用 GPU 加载 bert（用于对抗）\n",
    "    global bert\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\",\n",
    "                                     torch_dtype=torch.float16).to(DEVICE).eval()\n",
    "\n",
    "    rec = []  # 存放所有 query + 细节的 list\n",
    "\n",
    "    # 2. 对每条 query，用 hinge + cos 两种 loss mode 分别跑一遍\n",
    "    for mode in (\"hinge\", \"cos\"):\n",
    "        pbar = tqdm.tqdm(qs, desc=f\"scifact-{mode}\")\n",
    "        for q in pbar:\n",
    "            qtxt = q[\"text\"]\n",
    "            qid  = q[\"id\"]\n",
    "\n",
    "            # 2.1 把 query 编码\n",
    "            ids, msk = get_ids_msk(qtxt, DEVICE)\n",
    "            with torch.no_grad():\n",
    "                qcls = bert(**enc([qtxt], DEVICE)).last_hidden_state[:, 0, :]\n",
    "\n",
    "            # 2.2 计算原始与所有 docs 的相似度，得到排序\n",
    "            sims = cos_row(qcls.cpu(), C_CLS)\n",
    "            order = torch.argsort(sims, descending=True)\n",
    "\n",
    "            # —— 抓取“真正正例”信息，作为 orig_top1 —— \n",
    "            true_pos_id      = pos_dict[qid][0]\n",
    "            true_pos_excerpt = pos_text[true_pos_id]\n",
    "\n",
    "            # —— 抓取“baseline Rank=100”目标文档信息 —— \n",
    "            tgt_idx       = int(order[RANK_TARGET - 1].item())\n",
    "            tgt_id        = docs[tgt_idx][\"id\"]\n",
    "            tgt_excerpt   = docs[tgt_idx][\"text\"][:120].replace(\"\\n\", \" \")\n",
    "            baseline_sim  = sims[tgt_idx].item()\n",
    "\n",
    "            # 不需要单独输出 pos_rank/pos_excerpt，这里统一设 None\n",
    "            pos_rank    = None\n",
    "            pos_excerpt = None\n",
    "\n",
    "            # —— 对第 100 名目标文档做 suffix 攻击 —— \n",
    "            tgt_cls = C_CLS[tgt_idx : tgt_idx + 1].to(DEVICE)\n",
    "            CP = torch.cat([C_CLS[:tgt_idx], C_CLS[tgt_idx+1:]], dim=0)\n",
    "\n",
    "            suf, cls_adv, iters = de_opt(ids, msk, CP, tgt_cls, loss_mode=mode)\n",
    "            new_sims = cos_row(cls_adv.cpu(), C_CLS)\n",
    "            fr = (new_sims > new_sims[tgt_idx]).sum().item() + 1\n",
    "\n",
    "            # —— 记录这一条 query + mode 的所有字段 —— \n",
    "            entry = {\n",
    "                \"dataset\":             \"scifact\",\n",
    "                \"loss\":                mode,\n",
    "                \"query\":               qtxt,\n",
    "\n",
    "                # 原本的“真正正例”直接当成 orig_top1\n",
    "                \"orig_answer_id\":      true_pos_id,\n",
    "                \"orig_answer_excerpt\": true_pos_excerpt,\n",
    "\n",
    "                # baseline Rank=100\n",
    "                \"tgt_id\":              tgt_id,\n",
    "                \"tgt_excerpt\":         tgt_excerpt,\n",
    "\n",
    "                # 对抗 patch 细节\n",
    "                \"suffix\":              suffix_str(suf),\n",
    "                \"suffix_len\":          len(suf),\n",
    "                \"suffix_token_ids\":    json.dumps(suf),\n",
    "\n",
    "                # ranking 变化\n",
    "                \"baseline_rank\":       RANK_TARGET,\n",
    "                \"final_rank\":          fr,\n",
    "                \"delta_rank\":          RANK_TARGET - fr,\n",
    "                \"delta_cos\":           new_sims[tgt_idx].item() - baseline_sim,\n",
    "                \"delta_ndcg\":          dcg(fr) - dcg(RANK_TARGET),\n",
    "                \"success\":             int(fr == 1),\n",
    "                \"iter_used\":           iters,\n",
    "\n",
    "                # 统一留 None\n",
    "                \"pos_rank\":            pos_rank,\n",
    "                \"pos_excerpt\":         pos_excerpt,\n",
    "            }\n",
    "            rec.append(entry)\n",
    "\n",
    "    # 3. 写出 loss_detail.csv\n",
    "    df = pd.DataFrame(rec)\n",
    "    df.to_csv(f\"{out_dir}/loss_detail.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # 4. 聚合输出 loss_aggregate.csv\n",
    "    agg = df.groupby(\"loss\").agg(\n",
    "        success_rate   = (\"success\",   \"mean\"),\n",
    "        avg_iters      = (\"iter_used\", \"mean\"),\n",
    "        avg_delta_rank = (\"delta_rank\",\"mean\"),\n",
    "        avg_delta_cos  = (\"delta_cos\", \"mean\"),\n",
    "    ).reset_index()\n",
    "    agg.to_csv(f\"{out_dir}/loss_aggregate.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    h = agg.loc[agg.loss == \"hinge\", \"success_rate\"].values[0]\n",
    "    c = agg.loc[agg.loss == \"cos\",   \"success_rate\"].values[0]\n",
    "    print(f\"✓ scifact done  –  hinge success = {h:.2%},  cos success = {c:.2%}\")\n",
    "\n",
    "    # 释放显存\n",
    "    del bert\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ─────────────── 主入口 ───────────────\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"exp_results/loss_compare/scifact\", exist_ok=True)\n",
    "    run_loss_compare_scifact()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36878cce-b457-4a25-89c5-47406bc26547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 四个数据集对比表格 ===\n",
      "\n",
      "dataset  cos_avg_delta_cos  hinge_avg_delta_cos  cos_avg_delta_rank  hinge_avg_delta_rank  cos_avg_iters  hinge_avg_iters  cos_success_rate  hinge_success_rate\n",
      "  fever             0.0686              -0.0273             59.0400               76.2600      4425.0000        3615.0000            0.0100              0.2600\n",
      "   fiqa             0.0422              -0.0092             63.9200               86.9900      4274.0000        3743.0000            0.0200              0.2300\n",
      "msmarco             0.0634               0.0208             77.6300               88.2500      4346.0000        3499.0000            0.0700              0.2900\n",
      "scifact             0.0577              -0.0203             60.6700               80.9400      4624.0000        3466.0000            0.0200              0.3100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to loss_aggregate.csv for different datasets\n",
    "paths = {\n",
    "    'fever':  'exp_results/loss_compare/fever/loss_aggregate.csv',\n",
    "    'scifact':'exp_results/loss_compare/scifact/loss_aggregate.csv',\n",
    "    'msmarco':'exp_results/loss_compare/msmarco/loss_aggregate.csv',\n",
    "    'fiqa':   'exp_results/loss_compare/fiqa/loss_aggregate.csv',\n",
    "}\n",
    "\n",
    "# 1) 读取并合并所有可用的 loss_aggregate.csv\n",
    "dfs = []\n",
    "for ds, path in paths.items():\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        df['dataset'] = ds\n",
    "        dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        pass  # 如果某个数据集的文件不存在，就跳过\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"找不到任何 loss_aggregate.csv，请检查路径是否正确。\")\n",
    "\n",
    "combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 2) 构建两个 pivot tables\n",
    "#    - 一个针对 success_rate：行=index=loss, 列=dataset, 值=success_rate\n",
    "#    - 一个针对 avg_delta_rank：行=index=loss, 列=dataset, 值=avg_delta_rank\n",
    "pivot_sr = combined.pivot(index='loss', columns='dataset', values='success_rate')\n",
    "pivot_dr = combined.pivot(index='loss', columns='dataset', values='avg_delta_rank')\n",
    "\n",
    "# 3) 确定 loss types & datasets 顺序\n",
    "losses = pivot_sr.index.tolist()         # e.g. ['hinge', 'cos']\n",
    "datasets = pivot_sr.columns.tolist()      # e.g. ['fever','scifact','msmarco','fiqa']\n",
    "\n",
    "x = np.arange(len(losses))\n",
    "width = 0.8 / len(datasets)  # 每个 bar 的宽度\n",
    "\n",
    "# 4) 绘制两个并排子图：左边是 Success Rate，右边是 Avg Delta Rank\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), dpi=100)\n",
    "\n",
    "# —— 左：Success Rate by Loss & Dataset —— \n",
    "for i, ds in enumerate(datasets):\n",
    "    axes[0].bar(x + i*width, pivot_sr[ds], width, label=ds)\n",
    "\n",
    "axes[0].set_xticks(x + width * (len(datasets)-1) / 2)\n",
    "axes[0].set_xticklabels(losses)\n",
    "axes[0].set_xlabel(\"Loss\")\n",
    "axes[0].set_ylabel(\"Success Rate\")\n",
    "axes[0].set_title(\"Success Rate by Loss and Dataset\")\n",
    "axes[0].legend()\n",
    "\n",
    "# —— 右：Avg Delta Rank by Loss & Dataset —— \n",
    "for i, ds in enumerate(datasets):\n",
    "    axes[1].bar(x + i*width, pivot_dr[ds], width, label=ds)\n",
    "\n",
    "axes[1].set_xticks(x + width * (len(datasets)-1) / 2)\n",
    "axes[1].set_xticklabels(losses)\n",
    "axes[1].set_xlabel(\"Loss\")\n",
    "axes[1].set_ylabel(\"Avg Δ-Rank\")\n",
    "axes[1].set_title(\"Avg Δ-Rank by Loss and Dataset\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 1) 读取每个数据集的 loss_aggregate.csv，并将其重塑到同一个表格中\n",
    "records = []\n",
    "for ds, path in paths.items():\n",
    "    df = pd.read_csv(path)\n",
    "    for _, row in df.iterrows():\n",
    "        records.append({\n",
    "            'dataset':          ds,\n",
    "            'loss':             row['loss'],\n",
    "            'success_rate':     row['success_rate'],\n",
    "            'avg_delta_rank':   row['avg_delta_rank'],\n",
    "            'avg_iters':        row['avg_iters'],\n",
    "            'avg_delta_cos':    row['avg_delta_cos']\n",
    "        })\n",
    "\n",
    "combined = pd.DataFrame.from_records(records)\n",
    "\n",
    "# 2) 将表格重塑为“dataset 为行，loss+指标 为列”的格式\n",
    "pivot_table = combined.pivot_table(\n",
    "    index='dataset',\n",
    "    columns='loss',\n",
    "    values=['success_rate', 'avg_delta_rank', 'avg_iters', 'avg_delta_cos']\n",
    ")\n",
    "\n",
    "# 3) 重新命名列：把多级列合并成单级，例如 'success_rate','hinge' → 'hinge_success_rate'\n",
    "pivot_table.columns = [\n",
    "    f\"{loss}_{metric}\" \n",
    "    for metric, loss in pivot_table.columns\n",
    "]\n",
    "pivot_table = pivot_table.reset_index()\n",
    "\n",
    "# 4) 直接打印 DataFrame\n",
    "print(\"\\n=== 四个数据集对比表格 ===\\n\")\n",
    "print(pivot_table.to_string(index=False, float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "54329eae-2588-4410-9589-5852c4eb9a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 32/32 [02:58<00:00,  5.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g= 1 run=01  best_loss=0.0297\n",
      "g= 1 run=02  best_loss=0.0045\n",
      "g= 1 run=03  best_loss=0.0325\n",
      "g= 1 run=04  best_loss=0.0299\n",
      "g= 1 run=05  best_loss=0.0205\n",
      "g= 1 run=06  best_loss=0.0000\n",
      "g= 1 run=07  best_loss=0.0041\n",
      "g= 1 run=08  best_loss=0.0142\n",
      "g= 1 run=09  best_loss=0.0075\n",
      "g= 1 run=10  best_loss=0.0000\n",
      "g= 2 run=01  best_loss=0.0170\n",
      "g= 2 run=02  best_loss=0.0280\n",
      "g= 2 run=03  best_loss=0.0210\n",
      "g= 2 run=04  best_loss=0.0367\n",
      "g= 2 run=05  best_loss=0.0222\n",
      "g= 2 run=06  best_loss=0.0310\n",
      "g= 2 run=07  best_loss=0.0109\n",
      "g= 2 run=08  best_loss=0.0306\n",
      "g= 2 run=09  best_loss=0.0204\n",
      "g= 2 run=10  best_loss=0.0177\n",
      "g= 3 run=01  best_loss=0.0210\n",
      "g= 3 run=02  best_loss=0.0264\n",
      "g= 3 run=03  best_loss=0.0321\n",
      "g= 3 run=04  best_loss=0.0296\n",
      "g= 3 run=05  best_loss=0.0394\n",
      "g= 3 run=06  best_loss=0.0238\n",
      "g= 3 run=07  best_loss=0.0237\n",
      "g= 3 run=08  best_loss=0.0285\n",
      "g= 3 run=09  best_loss=0.0354\n",
      "g= 3 run=10  best_loss=0.0345\n",
      "g= 4 run=01  best_loss=0.0329\n",
      "g= 4 run=02  best_loss=0.0320\n",
      "g= 4 run=03  best_loss=0.0496\n",
      "g= 4 run=04  best_loss=0.0404\n",
      "g= 4 run=05  best_loss=0.0367\n",
      "g= 4 run=06  best_loss=0.0329\n",
      "g= 4 run=07  best_loss=0.0322\n",
      "g= 4 run=08  best_loss=0.0365\n",
      "g= 4 run=09  best_loss=0.0337\n",
      "g= 4 run=10  best_loss=0.0446\n",
      "g= 5 run=01  best_loss=0.0446\n",
      "g= 5 run=02  best_loss=0.0434\n",
      "g= 5 run=03  best_loss=0.0443\n",
      "g= 5 run=04  best_loss=0.0340\n",
      "g= 5 run=05  best_loss=0.0374\n",
      "g= 5 run=06  best_loss=0.0323\n",
      "g= 5 run=07  best_loss=0.0436\n",
      "g= 5 run=08  best_loss=0.0325\n",
      "g= 5 run=09  best_loss=0.0201\n",
      "g= 5 run=10  best_loss=0.0422\n",
      "g= 6 run=01  best_loss=0.0384\n",
      "g= 6 run=02  best_loss=0.0417\n",
      "g= 6 run=03  best_loss=0.0370\n",
      "g= 6 run=04  best_loss=0.0430\n",
      "g= 6 run=05  best_loss=0.0369\n",
      "g= 6 run=06  best_loss=0.0335\n",
      "g= 6 run=07  best_loss=0.0349\n",
      "g= 6 run=08  best_loss=0.0466\n",
      "g= 6 run=09  best_loss=0.0363\n",
      "g= 6 run=10  best_loss=0.0388\n",
      "g= 7 run=01  best_loss=0.0425\n",
      "g= 7 run=02  best_loss=0.0431\n",
      "g= 7 run=03  best_loss=0.0320\n",
      "g= 7 run=04  best_loss=0.0407\n",
      "g= 7 run=05  best_loss=0.0482\n",
      "g= 7 run=06  best_loss=0.0395\n",
      "g= 7 run=07  best_loss=0.0343\n",
      "g= 7 run=08  best_loss=0.0397\n",
      "g= 7 run=09  best_loss=0.0382\n",
      "g= 7 run=10  best_loss=0.0418\n",
      "g= 8 run=01  best_loss=0.0404\n",
      "g= 8 run=02  best_loss=0.0436\n",
      "g= 8 run=03  best_loss=0.0362\n",
      "g= 8 run=04  best_loss=0.0481\n",
      "g= 8 run=05  best_loss=0.0430\n",
      "g= 8 run=06  best_loss=0.0481\n",
      "g= 8 run=07  best_loss=0.0366\n",
      "g= 8 run=08  best_loss=0.0377\n",
      "g= 8 run=09  best_loss=0.0344\n",
      "g= 8 run=10  best_loss=0.0508\n",
      "g= 9 run=01  best_loss=0.0480\n",
      "g= 9 run=02  best_loss=0.0383\n",
      "g= 9 run=03  best_loss=0.0395\n",
      "g= 9 run=04  best_loss=0.0434\n",
      "g= 9 run=05  best_loss=0.0463\n",
      "g= 9 run=06  best_loss=0.0435\n",
      "g= 9 run=07  best_loss=0.0476\n",
      "g= 9 run=08  best_loss=0.0441\n",
      "g= 9 run=09  best_loss=0.0453\n",
      "g= 9 run=10  best_loss=0.0411\n",
      "g=10 run=01  best_loss=0.0469\n",
      "g=10 run=02  best_loss=0.0526\n",
      "g=10 run=03  best_loss=0.0412\n",
      "g=10 run=04  best_loss=0.0424\n",
      "g=10 run=05  best_loss=0.0458\n",
      "g=10 run=06  best_loss=0.0376\n",
      "g=10 run=07  best_loss=0.0514\n",
      "g=10 run=08  best_loss=0.0437\n",
      "g=10 run=09  best_loss=0.0468\n",
      "g=10 run=10  best_loss=0.0472\n",
      "g=11 run=01  best_loss=0.0476\n",
      "g=11 run=02  best_loss=0.0486\n",
      "g=11 run=03  best_loss=0.0521\n",
      "g=11 run=04  best_loss=0.0499\n",
      "g=11 run=05  best_loss=0.0478\n",
      "g=11 run=06  best_loss=0.0532\n",
      "g=11 run=07  best_loss=0.0374\n",
      "g=11 run=08  best_loss=0.0447\n",
      "g=11 run=09  best_loss=0.0511\n",
      "g=11 run=10  best_loss=0.0484\n",
      "g=12 run=01  best_loss=0.0432\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    118\u001b[39m trace_csv = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/traces/g\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_run\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m plot_png = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/plots/g\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_run\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m suffix_ids, best_loss, n_eval = \u001b[43moptimise_suffix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_csv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m plot_trace(trace_csv, plot_png)\n\u001b[32m    122\u001b[39m detail.append({\n\u001b[32m    123\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgroup_size\u001b[39m\u001b[33m\"\u001b[39m: g,\n\u001b[32m    124\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun\u001b[39m\u001b[33m\"\u001b[39m: run,\n\u001b[32m   (...)\u001b[39m\u001b[32m    132\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrace_file\u001b[39m\u001b[33m\"\u001b[39m: trace_csv\n\u001b[32m    133\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36moptimise_suffix\u001b[39m\u001b[34m(group_q, trace_csv)\u001b[39m\n\u001b[32m     95\u001b[39m     running.append(best)\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cur == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m stale >= PATIENCE\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m res = \u001b[43mdifferential_evolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoint_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpopsize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPOP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAXITER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolish\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m suffix_ids = [\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m res.x]\n\u001b[32m     99\u001b[39m pd.DataFrame({\u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m: np.arange(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(running) + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mbest_loss\u001b[39m\u001b[33m\"\u001b[39m: running}).to_csv(trace_csv, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/scipy/_lib/_util.py:440\u001b[39m, in \u001b[36m_transition_to_rng.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m     message = (\n\u001b[32m    434\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe NumPy global RNG was seeded by calling \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    435\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`np.random.seed`. Beginning in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, this \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    436\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfunction will no longer use the global RNG.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    437\u001b[39m     ) + cmn_msg\n\u001b[32m    438\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/scipy/optimize/_differentialevolution.py:501\u001b[39m, in \u001b[36mdifferential_evolution\u001b[39m\u001b[34m(func, bounds, args, strategy, maxiter, popsize, tol, mutation, recombination, rng, callback, disp, polish, init, atol, updating, workers, constraints, x0, integrality, vectorized)\u001b[39m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# using a context manager means that any created Pool objects are\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# cleared up.\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m DifferentialEvolutionSolver(func, bounds, args=args,\n\u001b[32m    487\u001b[39m                                  strategy=strategy,\n\u001b[32m    488\u001b[39m                                  maxiter=maxiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    499\u001b[39m                                  integrality=integrality,\n\u001b[32m    500\u001b[39m                                  vectorized=vectorized) \u001b[38;5;28;01mas\u001b[39;00m solver:\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m     ret = \u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/scipy/optimize/_differentialevolution.py:1185\u001b[39m, in \u001b[36mDifferentialEvolutionSolver.solve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1182\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.maxiter + \u001b[32m1\u001b[39m):\n\u001b[32m   1183\u001b[39m     \u001b[38;5;66;03m# evolve the population by a generation\u001b[39;00m\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1185\u001b[39m         \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1186\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   1187\u001b[39m         warning_flag = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/scipy/optimize/_differentialevolution.py:1600\u001b[39m, in \u001b[36mDifferentialEvolutionSolver.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1598\u001b[39m     feasible = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1599\u001b[39m     cv = np.atleast_2d([\u001b[32m0.\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m1600\u001b[39m     energy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1601\u001b[39m     \u001b[38;5;28mself\u001b[39m._nfev += \u001b[32m1\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;66;03m# compare trial and population member\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/scipy/_lib/_util.py:657\u001b[39m, in \u001b[36m_FunctionWrapper.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36moptimise_suffix.<locals>.joint_loss\u001b[39m\u001b[34m(v)\u001b[39m\n\u001b[32m     81\u001b[39m p, m = ids.clone(), msk.clone(); m[pos] = \u001b[32m1\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pos, v): p[i] = t\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28mcls\u001b[39m = \u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.last_hidden_state[:, \u001b[32m0\u001b[39m, :]\n\u001b[32m     84\u001b[39m kth = torch.topk(cos_row(\u001b[38;5;28mcls\u001b[39m, CP), \u001b[32m1\u001b[39m).values[-\u001b[32m1\u001b[39m]\n\u001b[32m     85\u001b[39m sim = torch.nn.functional.cosine_similarity(\u001b[38;5;28mcls\u001b[39m, tgt_cls)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1016\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1012\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1014\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1028\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1029\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:662\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    651\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    652\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    653\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    659\u001b[39m         output_attentions,\n\u001b[32m    660\u001b[39m     )\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    672\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:594\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    591\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    592\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/transformers/pytorch_utils.py:253\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m    606\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:519\u001b[39m, in \u001b[36mBertOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    520\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    521\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/colab_env/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os, json, math, random, warnings, numpy as np, pandas as pd, torch, tqdm, matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED           = 42\n",
    "DEVICE         = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_DOCS         = 1_000\n",
    "RANK_TARGET    = 100\n",
    "TAIL_L         = 5\n",
    "GROUP_RANGE    = range(1, 21)\n",
    "N_REPEAT       = 10\n",
    "POP            = 20\n",
    "MAXITER        = 2_000\n",
    "PATIENCE       = 20\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "VOC = tok.vocab_size\n",
    "\n",
    "def enc(txts, dev):\n",
    "    return tok(txts, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(dev)\n",
    "\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1)\n",
    "\n",
    "def is_good_q(t):\n",
    "    return \"?\" in t and 20 < len(tok(t)[\"input_ids\"]) - 2 < 500\n",
    "\n",
    "def suffix_str(ids):\n",
    "    return \" \".join(tok.convert_ids_to_tokens(ids))\n",
    "\n",
    "def detect_key(sample, keys):\n",
    "    for k in sample:\n",
    "        low = k.lower()\n",
    "        for tgt in keys:\n",
    "            if tgt in low:\n",
    "                return k\n",
    "    return list(sample.keys())[0]\n",
    "\n",
    "def prepare_corpus():\n",
    "    corpus_ds = load_dataset(\"BeIR/fiqa\", \"corpus\", split=\"corpus\")\n",
    "    docs = random.sample(list(corpus_ds), N_DOCS)\n",
    "    cpu_bert = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(\"cpu\")\n",
    "    CLS = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(0, N_DOCS, 32), desc=\"CLS\"):\n",
    "            CLS.append(cpu_bert(**enc([d[\"text\"] for d in docs[i:i+32]], \"cpu\")).last_hidden_state[:, 0, :])\n",
    "    C_CLS = torch.cat(CLS)\n",
    "    q_ds = load_dataset(\"BeIR/fiqa\", \"queries\", split=\"queries\")\n",
    "    qid_key = detect_key(q_ds[0], {\"id\", \"qid\", \"query_id\", \"number\"})\n",
    "    txt_key = detect_key(q_ds[0], {\"text\", \"query\", \"body\"})\n",
    "    pool = [{\"id\": q[qid_key], \"text\": q[txt_key]} for q in q_ds if is_good_q(q[txt_key])]\n",
    "    return docs, C_CLS, pool\n",
    "\n",
    "docs, C_CLS, pool = prepare_corpus()\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\", torch_dtype=torch.float16).to(DEVICE).eval()\n",
    "\n",
    "def optimise_suffix(group_q, trace_csv):\n",
    "    pos = list(range(512 - TAIL_L, 512))\n",
    "    bounds = [(0, VOC - 1)] * TAIL_L\n",
    "    infos = []\n",
    "    for q in group_q:\n",
    "        e = enc([q[\"text\"]], DEVICE)\n",
    "        ids, msk = e[\"input_ids\"][0], e[\"attention_mask\"][0]\n",
    "        with torch.no_grad():\n",
    "            qcls = bert(**e).last_hidden_state[:, 0, :]\n",
    "        sims = cos_row(qcls.cpu(), C_CLS)\n",
    "        tgt_idx = torch.argsort(sims, descending=True)[RANK_TARGET - 1].item()\n",
    "        tgt_cls = C_CLS[tgt_idx:tgt_idx + 1].to(DEVICE)\n",
    "        CP = C_CLS[[i for i in range(N_DOCS) if i != tgt_idx]]\n",
    "        infos.append((ids, msk, CP, tgt_cls))\n",
    "    running, best, stale = [], 1e9, 0\n",
    "    def joint_loss(v):\n",
    "        v = [int(round(x)) for x in v]\n",
    "        loss = 0.\n",
    "        for ids, msk, CP, tgt_cls in infos:\n",
    "            p, m = ids.clone(), msk.clone(); m[pos] = 1\n",
    "            for i, t in zip(pos, v): p[i] = t\n",
    "            cls = bert(input_ids=p.unsqueeze(0), attention_mask=m.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "            kth = torch.topk(cos_row(cls, CP), 1).values[-1]\n",
    "            sim = torch.nn.functional.cosine_similarity(cls, tgt_cls)[0]\n",
    "            loss += max(0., (kth - sim).item())\n",
    "        return loss / len(infos)\n",
    "    def cb(xk, _):\n",
    "        nonlocal best, stale\n",
    "        cur = joint_loss(xk)\n",
    "        if cur < best:\n",
    "            best, stale = cur, 0\n",
    "        else:\n",
    "            stale += 1\n",
    "        running.append(best)\n",
    "        return cur == 0 or stale >= PATIENCE\n",
    "    res = differential_evolution(joint_loss, bounds, popsize=POP, maxiter=MAXITER, tol=0, polish=False, seed=SEED, callback=cb)\n",
    "    suffix_ids = [int(round(x)) for x in res.x]\n",
    "    pd.DataFrame({\"eval\": np.arange(1, len(running) + 1), \"best_loss\": running}).to_csv(trace_csv, index=False)\n",
    "    return suffix_ids, best, len(running)\n",
    "\n",
    "def plot_trace(csv_path, png_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    plt.figure(figsize=(4.8, 3))\n",
    "    plt.plot(df[\"eval\"], df[\"best_loss\"], color=\"red\")\n",
    "    plt.title(\"Running Minimum Loss over Evaluations\")\n",
    "    plt.xlabel(\"function eval\"); plt.ylabel(\"min loss so far\")\n",
    "    plt.tight_layout(); plt.savefig(png_path, dpi=300); plt.close()\n",
    "\n",
    "base = \"exp_results/group_suffix2\"\n",
    "os.makedirs(f\"{base}/traces\", exist_ok=True)\n",
    "os.makedirs(f\"{base}/plots\", exist_ok=True)\n",
    "detail = []\n",
    "\n",
    "for g in GROUP_RANGE:\n",
    "    for run in range(1, N_REPEAT + 1):\n",
    "        group_q = random.sample(pool, g)\n",
    "        trace_csv = f\"{base}/traces/g{g}_run{run}.csv\"\n",
    "        plot_png = f\"{base}/plots/g{g}_run{run}.png\"\n",
    "        suffix_ids, best_loss, n_eval = optimise_suffix(group_q, trace_csv)\n",
    "        plot_trace(trace_csv, plot_png)\n",
    "        detail.append({\n",
    "            \"group_size\": g,\n",
    "            \"run\": run,\n",
    "            \"query_ids\": json.dumps([q[\"id\"] for q in group_q]),\n",
    "            \"query_texts\": json.dumps([q[\"text\"] for q in group_q]),\n",
    "            \"suffix\": suffix_str(suffix_ids),\n",
    "            \"suffix_token_ids\": json.dumps(suffix_ids),\n",
    "            \"best_loss\": best_loss,\n",
    "            \"success\": int(best_loss == 0),\n",
    "            \"n_eval\": n_eval,\n",
    "            \"trace_file\": trace_csv\n",
    "        })\n",
    "        print(f\"g={g:2d} run={run:02d}  best_loss={best_loss:.4f}\")\n",
    "\n",
    "detail_df = pd.DataFrame(detail)\n",
    "os.makedirs(base, exist_ok=True)\n",
    "detail_df.to_csv(f\"{base}/detail.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "summary = detail_df.groupby(\"group_size\").agg(\n",
    "    mean_best_loss=(\"best_loss\", \"mean\"),\n",
    "    success_rate=(\"success\", \"mean\")\n",
    ").reset_index()\n",
    "summary.to_csv(f\"{base}/summary.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(summary[\"group_size\"], summary[\"mean_best_loss\"], marker=\"o\")\n",
    "plt.xlabel(\"group size (g)\"); plt.ylabel(\"Avg best hinge loss\")\n",
    "plt.title(\"Average best loss vs. group size\"); plt.grid()\n",
    "plt.tight_layout(); plt.savefig(f\"{base}/avg_best_loss.png\", dpi=300); plt.close()\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80b2f2a9-1184-4fea-9695-f00267ee7139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已輸出：exp_results/group_suffix2/avg_trace_by_g.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 根據你的路徑調整\n",
    "TRACE_DIR = \"exp_results/group_suffix2/traces\"\n",
    "PLOT_OUT  = \"exp_results/group_suffix2/avg_trace_by_g.png\"\n",
    "\n",
    "# 找出所有 trace 檔\n",
    "paths = glob.glob(os.path.join(TRACE_DIR, \"g*_run*.csv\"))\n",
    "\n",
    "# 解析路徑拿到 g 與 run 編號\n",
    "pat = re.compile(r\"g(\\d+)_run(\\d+)\\.csv\")\n",
    "\n",
    "# 收集所有 (g, run) → DataFrame\n",
    "traces = {}\n",
    "for p in paths:\n",
    "    m = pat.search(os.path.basename(p))\n",
    "    if not m:\n",
    "        continue\n",
    "    g, run = map(int, m.groups())\n",
    "    df = pd.read_csv(p)            # 每檔只有 eval, best_loss 兩欄\n",
    "    traces.setdefault(g, {})[run] = df\n",
    "\n",
    "# 聚合：對同 g 的多次實驗做 forward-fill 再取平均\n",
    "agg = {}\n",
    "for g, run_dict in traces.items():\n",
    "    # 決定這組最長的 eval 長度\n",
    "    max_len = max(len(df) for df in run_dict.values())\n",
    "\n",
    "    # 組一個 DataFrame，index = eval (從 1 開始)\n",
    "    big = pd.DataFrame(index=np.arange(1, max_len + 1))\n",
    "\n",
    "    # 把每一次 run 貼到 big 裡；不足的 eval 先 NaN\n",
    "    for r, df in run_dict.items():\n",
    "        big[f\"run{r}\"] = pd.Series(df[\"best_loss\"].values, index=df[\"eval\"].values)\n",
    "\n",
    "    # forward-fill 讓 NaN 用前一個 best 填補，\n",
    "    # 這樣平均才不會在不同長度處突然斷掉\n",
    "    big = big.ffill()\n",
    "\n",
    "    # 計算平均曲線\n",
    "    agg[g] = big.mean(axis=1)\n",
    "\n",
    "# ======= 以下是重點：確保 20 條線各自不同顏色 =======\n",
    "\n",
    "# 1. 先取得 tab20 的 20 種顏色（它本身就是一個 ListedColormap，.colors 會回傳 20 個 RGB tuple）\n",
    "colors = plt.get_cmap(\"tab20\").colors  # 這時候 colors[0]~colors[19] 就是 20 個不重複的色票\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sorted_items = sorted(agg.items())  # 依照 key（也就是 g 值）排序\n",
    "N = len(sorted_items)\n",
    "\n",
    "# 假設你確定最多只會畫到 g=1..20，也就是 N <= 20，\n",
    "# 那麼我們這邊直接用 colors[idx] 來取對應 idx 的「固定顏色」即可。\n",
    "for idx, (g, series) in enumerate(sorted_items):\n",
    "    if idx < len(colors):\n",
    "        color = colors[idx]\n",
    "    else:\n",
    "        # 如果真的超過 20 條（不太常見），可以 fallback 回「在 0~1 區間線性插值」：\n",
    "        color = plt.get_cmap(\"tab20\")(idx / (N - 1))\n",
    "    plt.plot(\n",
    "        series.index,\n",
    "        series.values,\n",
    "        label=f\"g={g}\",\n",
    "        color=color\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"function eval (token)\")\n",
    "plt.ylabel(\"average best loss\")\n",
    "plt.title(\"Token-to-Best-Avg-Loss across query-group sizes\")\n",
    "\n",
    "# 設定圖例：小字體、四欄擺放、放在右側\n",
    "plt.legend(\n",
    "    title=\"group size\",\n",
    "    fontsize=\"x-small\",         # 圖例字體\n",
    "    title_fontsize=\"small\",     # 標題字體\n",
    "    ncol=4,                     # 四欄並排\n",
    "    frameon=False,              # 去掉邊框\n",
    "    bbox_to_anchor=(1.02, 0.5), # 右側置中\n",
    "    loc=\"center left\"\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(right=0.78)  # 給右側圖例留空\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_OUT, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"✅ 已輸出：{PLOT_OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b91f912a-e796-4331-8ffb-f5f3d55f94d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 32/32 [02:59<00:00,  5.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 20 run=01  best=0.0475  eval=46\n",
      "p= 20 run=02  best=0.0447  eval=45\n",
      "p= 20 run=03  best=0.0217  eval=33\n",
      "p= 20 run=04  best=0.0229  eval=57\n",
      "p= 20 run=05  best=0.0129  eval=48\n",
      "p= 20 run=06  best=0.0170  eval=32\n",
      "p= 20 run=07  best=0.0280  eval=22\n",
      "p= 20 run=08  best=0.0210  eval=27\n",
      "p= 20 run=09  best=0.0367  eval=23\n",
      "p= 20 run=10  best=0.0222  eval=27\n",
      "p= 20 run=11  best=0.0310  eval=29\n",
      "p= 20 run=12  best=0.0109  eval=41\n",
      "p= 20 run=13  best=0.0306  eval=26\n",
      "p= 20 run=14  best=0.0204  eval=46\n",
      "p= 20 run=15  best=0.0177  eval=28\n",
      "p= 20 run=16  best=0.0159  eval=63\n",
      "p= 20 run=17  best=0.0132  eval=37\n",
      "p= 20 run=18  best=0.0233  eval=63\n",
      "p= 20 run=19  best=0.0286  eval=64\n",
      "p= 20 run=20  best=0.0301  eval=35\n",
      "p= 40 run=01  best=0.0214  eval=82\n",
      "p= 40 run=02  best=0.0256  eval=89\n",
      "p= 40 run=03  best=0.0360  eval=73\n",
      "p= 40 run=04  best=0.0057  eval=60\n",
      "p= 40 run=05  best=0.0195  eval=88\n",
      "p= 40 run=06  best=0.0196  eval=73\n",
      "p= 40 run=07  best=0.0253  eval=73\n",
      "p= 40 run=08  best=0.0218  eval=140\n",
      "p= 40 run=09  best=0.0326  eval=49\n",
      "p= 40 run=10  best=0.0313  eval=60\n",
      "p= 40 run=11  best=0.0180  eval=89\n",
      "p= 40 run=12  best=0.0256  eval=70\n",
      "p= 40 run=13  best=0.0303  eval=89\n",
      "p= 40 run=14  best=0.0196  eval=106\n",
      "p= 40 run=15  best=0.0301  eval=56\n",
      "p= 40 run=16  best=0.0364  eval=83\n",
      "p= 40 run=17  best=0.0226  eval=145\n",
      "p= 40 run=18  best=0.0273  eval=59\n",
      "p= 40 run=19  best=0.0219  eval=117\n",
      "p= 40 run=20  best=0.0234  eval=149\n",
      "p= 60 run=01  best=0.0190  eval=88\n",
      "p= 60 run=02  best=0.0161  eval=247\n",
      "p= 60 run=03  best=0.0116  eval=97\n",
      "p= 60 run=04  best=0.0237  eval=116\n",
      "p= 60 run=05  best=0.0347  eval=72\n",
      "p= 60 run=06  best=0.0128  eval=195\n",
      "p= 60 run=07  best=0.0226  eval=149\n",
      "p= 60 run=08  best=0.0259  eval=248\n",
      "p= 60 run=09  best=0.0251  eval=108\n",
      "p= 60 run=10  best=0.0328  eval=61\n",
      "p= 60 run=11  best=0.0475  eval=116\n",
      "p= 60 run=12  best=0.0141  eval=171\n",
      "p= 60 run=13  best=0.0285  eval=120\n",
      "p= 60 run=14  best=0.0296  eval=66\n",
      "p= 60 run=15  best=0.0172  eval=133\n",
      "p= 60 run=16  best=0.0176  eval=213\n",
      "p= 60 run=17  best=0.0329  eval=179\n",
      "p= 60 run=18  best=0.0215  eval=139\n",
      "p= 60 run=19  best=0.0141  eval=129\n",
      "p= 60 run=20  best=0.0014  eval=261\n",
      "p= 80 run=01  best=0.0031  eval=285\n",
      "p= 80 run=02  best=0.0369  eval=81\n",
      "p= 80 run=03  best=0.0203  eval=90\n",
      "p= 80 run=04  best=0.0120  eval=88\n",
      "p= 80 run=05  best=0.0225  eval=124\n",
      "p= 80 run=06  best=0.0142  eval=184\n",
      "p= 80 run=07  best=0.0316  eval=133\n",
      "p= 80 run=08  best=0.0128  eval=179\n",
      "p= 80 run=09  best=0.0183  eval=201\n",
      "p= 80 run=10  best=0.0157  eval=252\n",
      "p= 80 run=11  best=0.0130  eval=89\n",
      "p= 80 run=12  best=0.0113  eval=349\n",
      "p= 80 run=13  best=0.0000  eval=97\n",
      "p= 80 run=14  best=0.0387  eval=119\n",
      "p= 80 run=15  best=0.0198  eval=90\n",
      "p= 80 run=16  best=0.0022  eval=161\n",
      "p= 80 run=17  best=0.0150  eval=104\n",
      "p= 80 run=18  best=0.0247  eval=119\n",
      "p= 80 run=19  best=0.0146  eval=109\n",
      "p= 80 run=20  best=0.0135  eval=195\n",
      "p=100 run=01  best=0.0266  eval=119\n",
      "p=100 run=02  best=0.0217  eval=114\n",
      "p=100 run=03  best=0.0166  eval=192\n",
      "p=100 run=04  best=0.0204  eval=171\n",
      "p=100 run=05  best=0.0226  eval=378\n",
      "p=100 run=06  best=0.0227  eval=299\n",
      "p=100 run=07  best=0.0279  eval=161\n",
      "p=100 run=08  best=0.0100  eval=192\n",
      "p=100 run=09  best=0.0115  eval=368\n",
      "p=100 run=10  best=0.0143  eval=175\n",
      "p=100 run=11  best=0.0107  eval=141\n",
      "p=100 run=12  best=0.0184  eval=225\n",
      "p=100 run=13  best=0.0080  eval=265\n",
      "p=100 run=14  best=0.0264  eval=172\n",
      "p=100 run=15  best=0.0168  eval=305\n",
      "p=100 run=16  best=0.0210  eval=200\n",
      "p=100 run=17  best=0.0315  eval=426\n",
      "p=100 run=18  best=0.0277  eval=217\n",
      "p=100 run=19  best=0.0070  eval=235\n",
      "p=100 run=20  best=0.0307  eval=182\n",
      "p=150 run=01  best=0.0225  eval=269\n",
      "p=150 run=02  best=0.0000  eval=10\n",
      "p=150 run=03  best=0.0167  eval=206\n",
      "p=150 run=04  best=0.0160  eval=390\n",
      "p=150 run=05  best=0.0267  eval=305\n",
      "p=150 run=06  best=0.0205  eval=221\n",
      "p=150 run=07  best=0.0208  eval=248\n",
      "p=150 run=08  best=0.0142  eval=411\n",
      "p=150 run=09  best=0.0254  eval=233\n",
      "p=150 run=10  best=0.0269  eval=238\n",
      "p=150 run=11  best=0.0097  eval=175\n",
      "p=150 run=12  best=0.0136  eval=261\n",
      "p=150 run=13  best=0.0208  eval=184\n",
      "p=150 run=14  best=0.0138  eval=258\n",
      "p=150 run=15  best=0.0095  eval=217\n",
      "p=150 run=16  best=0.0082  eval=264\n",
      "p=150 run=17  best=0.0147  eval=351\n",
      "p=150 run=18  best=0.0225  eval=569\n",
      "p=150 run=19  best=0.0189  eval=497\n",
      "p=150 run=20  best=0.0401  eval=402\n",
      "\n",
      "=== SUMMARY ===\n",
      "    patience  success_rate  mean_best_loss  mean_n_eval\n",
      "0        20          0.00        0.024815        39.60\n",
      "1        40          0.00        0.024711        87.50\n",
      "2        60          0.00        0.022437       145.40\n",
      "3        80          0.05        0.017010       152.45\n",
      "4       100          0.00        0.019615       226.85\n",
      "5       150          0.05        0.018083       285.45\n",
      "平均損失曲線已輸出 → exp_results/patience_grid2/avg_trace_by_patience.png\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "DE suffix search  (g = 2, tail = 5)  ─ PATIENCE sweep\n",
    "────────────────────────────────────────────────────────\n",
    "‣ PATIENCE  ∈ {20, 40, 80}\n",
    "‣ MAXITER   = 5 000\n",
    "產出：\n",
    "  • traces/       g2_p{patience}_run{run}.csv     ← running-min loss\n",
    "  • plots/        g2_p{patience}_run{run}.png     ← 單條曲線\n",
    "  • summary.csv   ← 各 PATIENCE 成功率 / 平均 loss / 平均評估次數\n",
    "  • p_grid_summary.png           ← 成功率 & 平均評估次數雙軸圖\n",
    "  • avg_trace_by_patience.png    ← 各 PATIENCE「平均損失曲線」\n",
    "\"\"\"\n",
    "\n",
    "import os, json, random, warnings, numpy as np, pandas as pd\n",
    "import torch, tqdm, matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ────────────── 固定參數 ──────────────\n",
    "SEED   = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "N_DOCS      = 1_000\n",
    "RANK_TARGET = 100\n",
    "TAIL_L      = 5\n",
    "GROUP_SIZE  = 2\n",
    "POP         = 20\n",
    "MAXITER     = 5_000\n",
    "PATIENCE_GRID = [20, 40, 60,80,100,150]   # ← 最小 20\n",
    "N_REPEAT    = 20               # 每個 PATIENCE 跑幾次\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "VOC = tok.vocab_size\n",
    "\n",
    "# ────────────── 小工具 ──────────────\n",
    "def enc(txts, dev):\n",
    "    return tok(txts, padding=\"max_length\", truncation=True,\n",
    "               max_length=512, return_tensors=\"pt\").to(dev)\n",
    "\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1)\n",
    "\n",
    "def is_good_q(t):\n",
    "    return \"?\" in t and 20 < len(tok(t)[\"input_ids\"]) - 2 < 500\n",
    "\n",
    "def suffix_str(ids):\n",
    "    return \" \".join(tok.convert_ids_to_tokens(ids))\n",
    "\n",
    "def detect_key(sample, keys):\n",
    "    for k in sample:\n",
    "        if any(t in k.lower() for t in keys):\n",
    "            return k\n",
    "    return list(sample.keys())[0]\n",
    "\n",
    "# ────────────── 載入語料 & 內積向量 ──────────────\n",
    "def prepare_corpus():\n",
    "    corpus_ds = load_dataset(\"BeIR/fiqa\", \"corpus\", split=\"corpus\")\n",
    "    docs  = random.sample(list(corpus_ds), N_DOCS)\n",
    "\n",
    "    cpu_bert = BertModel.from_pretrained(\"bert-base-uncased\").eval()\n",
    "    CLS = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(0, N_DOCS, 32), desc=\"CLS\"):\n",
    "            CLS.append(cpu_bert(**enc([d[\"text\"]\n",
    "                                       for d in docs[i:i+32]], \"cpu\")).last_hidden_state[:, 0, :])\n",
    "    C_CLS = torch.cat(CLS)\n",
    "\n",
    "    q_ds   = load_dataset(\"BeIR/fiqa\", \"queries\", split=\"queries\")\n",
    "    qid_k  = detect_key(q_ds[0], {\"id\", \"qid\", \"query_id\", \"number\"})\n",
    "    txt_k  = detect_key(q_ds[0], {\"text\", \"query\", \"body\"})\n",
    "    pool   = [{\"id\": q[qid_k], \"text\": q[txt_k]}\n",
    "              for q in q_ds if is_good_q(q[txt_k])]\n",
    "    return C_CLS, pool\n",
    "\n",
    "C_CLS, pool = prepare_corpus()\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\",\n",
    "                                 torch_dtype=torch.float16).to(DEVICE).eval()\n",
    "\n",
    "# ────────────── 單 run 最佳化 ──────────────\n",
    "def optimise_suffix(group_q, trace_csv, patience):\n",
    "    pos    = list(range(512 - TAIL_L, 512))\n",
    "    bounds = [(0, VOC - 1)] * TAIL_L\n",
    "    infos  = []\n",
    "\n",
    "    for q in group_q:\n",
    "        e = enc([q[\"text\"]], DEVICE)\n",
    "        ids, msk = e[\"input_ids\"][0], e[\"attention_mask\"][0]\n",
    "        with torch.no_grad():\n",
    "            qcls = bert(**e).last_hidden_state[:, 0, :]\n",
    "        sims    = cos_row(qcls.cpu(), C_CLS)\n",
    "        tgt_idx = torch.argsort(sims, descending=True)[RANK_TARGET-1].item()\n",
    "        tgt_cls = C_CLS[tgt_idx:tgt_idx+1].to(DEVICE)\n",
    "        CP = C_CLS[[i for i in range(N_DOCS) if i != tgt_idx]]\n",
    "        infos.append((ids, msk, CP, tgt_cls))\n",
    "\n",
    "    running, best, stale = [], 1e9, 0\n",
    "    def joint_loss(v):\n",
    "        v = [int(round(x)) for x in v]\n",
    "        loss = 0.\n",
    "        for ids, msk, CP, tgt in infos:\n",
    "            p, m = ids.clone(), msk.clone(); m[pos] = 1\n",
    "            for i, t in zip(pos, v): p[i] = t\n",
    "            cls = bert(input_ids=p.unsqueeze(0),\n",
    "                       attention_mask=m.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "            kth = torch.topk(cos_row(cls, CP), 1).values[-1]\n",
    "            sim = torch.nn.functional.cosine_similarity(cls, tgt)[0]\n",
    "            loss += max(0., (kth - sim).item())\n",
    "        return loss / len(infos)\n",
    "\n",
    "    def cb(xk, _):\n",
    "        nonlocal best, stale\n",
    "        cur = joint_loss(xk)\n",
    "        if cur < best: best, stale = cur, 0\n",
    "        else:          stale += 1\n",
    "        running.append(best)\n",
    "        return cur == 0 or stale >= patience\n",
    "\n",
    "    res = differential_evolution(joint_loss, bounds, popsize=POP,\n",
    "                                 maxiter=MAXITER, tol=0, polish=False,\n",
    "                                 seed=SEED, callback=cb)\n",
    "\n",
    "    suffix_ids = [int(round(x)) for x in res.x]\n",
    "    pd.DataFrame({\"eval\": np.arange(1, len(running)+1),\n",
    "                  \"best_loss\": running}).to_csv(trace_csv, index=False)\n",
    "    return suffix_ids, best, len(running)\n",
    "\n",
    "def plot_trace(csv_path, png_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    plt.figure(figsize=(4.2,3))\n",
    "    plt.plot(df[\"eval\"], df[\"best_loss\"], color=\"red\")\n",
    "    plt.xlabel(\"function eval\"); plt.ylabel(\"min loss so far\")\n",
    "    plt.title(\"Running Min Loss\"); plt.tight_layout()\n",
    "    plt.savefig(png_path, dpi=300); plt.close()\n",
    "\n",
    "# ────────────── 主實驗迴圈 ──────────────\n",
    "BASE = \"exp_results/patience_grid2\"\n",
    "os.makedirs(f\"{BASE}/traces\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE}/plots\",  exist_ok=True)\n",
    "\n",
    "rows = []\n",
    "for patience in PATIENCE_GRID:\n",
    "    for run in range(1, N_REPEAT+1):\n",
    "        qset = random.sample(pool, GROUP_SIZE)\n",
    "        t_csv = f\"{BASE}/traces/g{GROUP_SIZE}_p{patience}_run{run}.csv\"\n",
    "        p_png = f\"{BASE}/plots/g{GROUP_SIZE}_p{patience}_run{run}.png\"\n",
    "\n",
    "        suf, best_loss, n_eval = optimise_suffix(qset, t_csv, patience)\n",
    "        plot_trace(t_csv, p_png)\n",
    "\n",
    "        rows.append({\n",
    "            \"patience\": patience, \"run\": run,\n",
    "            \"best_loss\": best_loss, \"success\": int(best_loss==0),\n",
    "            \"n_eval\": n_eval, \"suffix\": suffix_str(suf)\n",
    "        })\n",
    "        print(f\"p={patience:3d} run={run:02d}  best={best_loss:.4f}  eval={n_eval}\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(f\"{BASE}/detail.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# ────────────── SUMMARY (成功率 + 平均 #eval) ──────────────\n",
    "summary = df.groupby(\"patience\").agg(\n",
    "    success_rate   = (\"success\", \"mean\"),\n",
    "    mean_best_loss = (\"best_loss\", \"mean\"),\n",
    "    mean_n_eval    = (\"n_eval\", \"mean\")\n",
    ").reset_index()\n",
    "summary.to_csv(f\"{BASE}/summary.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# 成功率 / 平均 #eval\n",
    "fig, ax1 = plt.subplots(figsize=(5,3.5))\n",
    "ax1.plot(summary[\"patience\"], summary[\"success_rate\"],\n",
    "         marker=\"o\", label=\"success rate\")\n",
    "ax1.set_xlabel(\"PATIENCE\"); ax1.set_ylabel(\"success rate\"); ax1.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(summary[\"patience\"], summary[\"mean_n_eval\"],\n",
    "         marker=\"s\", linestyle=\"--\", color=\"gray\", label=\"avg #eval\")\n",
    "ax2.set_ylabel(\"avg function evals\")\n",
    "fig.suptitle(\"Effect of PATIENCE (g = 2, tail = 5)\")\n",
    "fig.tight_layout(); fig.savefig(f\"{BASE}/p_grid_summary.png\", dpi=300); plt.close()\n",
    "\n",
    "# ────────────── ★ 新增：平均損失曲線 ★ ──────────────\n",
    "# 收集每個 patience 的所有 trace，做 forward-fill → 平均\n",
    "avg_curves = {}\n",
    "for patience in PATIENCE_GRID:\n",
    "    paths = [f\"{BASE}/traces/g{GROUP_SIZE}_p{patience}_run{r}.csv\"\n",
    "             for r in range(1, N_REPEAT+1)]\n",
    "    # 過濾掉可能因中斷而不存在的檔\n",
    "    paths = [p for p in paths if os.path.exists(p)]\n",
    "    if not paths: continue\n",
    "\n",
    "    dfs   = [pd.read_csv(p) for p in paths]\n",
    "    max_L = max(len(df) for df in dfs)\n",
    "    big   = pd.DataFrame(index=np.arange(1, max_L+1))\n",
    "    for i, df in enumerate(dfs):\n",
    "        big[f\"run{i+1}\"] = pd.Series(df[\"best_loss\"].values,\n",
    "                                     index=df[\"eval\"].values)\n",
    "    avg_curves[patience] = big.ffill().mean(axis=1)\n",
    "\n",
    "# 畫圖\n",
    "colors = plt.get_cmap(\"tab10\").colors\n",
    "plt.figure(figsize=(5,3.5))\n",
    "for idx, (pat, series) in enumerate(sorted(avg_curves.items())):\n",
    "    plt.plot(series.index, series.values, color=colors[idx],\n",
    "             label=f\"PATIENCE={pat}\")\n",
    "plt.xlabel(\"function eval\"); plt.ylabel(\"avg best loss\")\n",
    "plt.title(\"Average Running Best Loss vs. PATIENCE\")\n",
    "plt.legend(frameon=False, fontsize=\"small\"); plt.tight_layout()\n",
    "plt.savefig(f\"{BASE}/avg_trace_by_patience.png\", dpi=300); plt.close()\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\\n\", summary)\n",
    "print(\"平均損失曲線已輸出 →\", f\"{BASE}/avg_trace_by_patience.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ca0ff78-56cd-42b7-b29e-8887fda8d038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus CLS: 100%|███████████████████████████████████████████████████████████████████████| 32/32 [02:56<00:00,  5.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[R100] p= 60 run=01  best=0.04025  eval=189\n",
      "[R100] p= 60 run=02  best=0.04070  eval=108\n",
      "[R100] p= 60 run=03  best=0.02171  eval=73\n",
      "[R100] p= 60 run=04  best=0.02291  eval=97\n",
      "[R100] p= 60 run=05  best=0.00696  eval=140\n",
      "[R100] p= 60 run=06  best=0.01310  eval=181\n",
      "[R100] p= 60 run=07  best=0.01879  eval=152\n",
      "[R100] p= 60 run=08  best=0.01278  eval=128\n",
      "[R100] p= 60 run=09  best=0.03495  eval=97\n",
      "[R100] p= 60 run=10  best=0.02224  eval=67\n",
      "[R100] p= 70 run=01  best=0.02472  eval=130\n",
      "[R100] p= 70 run=02  best=0.00835  eval=151\n",
      "[R100] p= 70 run=03  best=0.02320  eval=145\n",
      "[R100] p= 70 run=04  best=0.01532  eval=135\n",
      "[R100] p= 70 run=05  best=0.01769  eval=78\n",
      "[R100] p= 70 run=06  best=0.01587  eval=113\n",
      "[R100] p= 70 run=07  best=0.01318  eval=87\n",
      "[R100] p= 70 run=08  best=0.02330  eval=113\n",
      "[R100] p= 70 run=09  best=0.02865  eval=114\n",
      "[R100] p= 70 run=10  best=0.01427  eval=256\n",
      "[R100] p= 80 run=01  best=0.02142  eval=122\n",
      "[R100] p= 80 run=02  best=0.02225  eval=197\n",
      "[R100] p= 80 run=03  best=0.02514  eval=301\n",
      "[R100] p= 80 run=04  best=0.00569  eval=100\n",
      "[R100] p= 80 run=05  best=0.01947  eval=128\n",
      "[R100] p= 80 run=06  best=0.01964  eval=113\n",
      "[R100] p= 80 run=07  best=0.02533  eval=113\n",
      "[R100] p= 80 run=08  best=0.01943  eval=296\n",
      "[R100] p= 80 run=09  best=0.02068  eval=289\n",
      "[R100] p= 80 run=10  best=0.02391  eval=260\n",
      "[R100] p= 90 run=01  best=0.01181  eval=276\n",
      "[R100] p= 90 run=02  best=0.02464  eval=170\n",
      "[R100] p= 90 run=03  best=0.02311  eval=313\n",
      "[R100] p= 90 run=04  best=0.01959  eval=156\n",
      "[R100] p= 90 run=05  best=0.03014  eval=106\n",
      "[R100] p= 90 run=06  best=0.03642  eval=133\n",
      "[R100] p= 90 run=07  best=0.02260  eval=195\n",
      "[R100] p= 90 run=08  best=0.01894  eval=347\n",
      "[R100] p= 90 run=09  best=0.01459  eval=346\n",
      "[R100] p= 90 run=10  best=0.02339  eval=199\n",
      "[R100] p=100 run=01  best=0.01903  eval=128\n",
      "[R100] p=100 run=02  best=0.01609  eval=287\n",
      "[R100] p=100 run=03  best=0.01163  eval=137\n",
      "[R100] p=100 run=04  best=0.02368  eval=156\n",
      "[R100] p=100 run=05  best=0.02464  eval=264\n",
      "[R100] p=100 run=06  best=0.01174  eval=297\n",
      "[R100] p=100 run=07  best=0.02260  eval=189\n",
      "[R100] p=100 run=08  best=0.02585  eval=288\n",
      "[R100] p=100 run=09  best=0.02514  eval=148\n",
      "[R100] p=100 run=10  best=0.03277  eval=101\n",
      "[R100] p=120 run=01  best=0.04232  eval=244\n",
      "[R100] p=120 run=02  best=0.01130  eval=357\n",
      "[R100] p=120 run=03  best=0.02458  eval=272\n",
      "[R100] p=120 run=04  best=0.01992  eval=376\n",
      "[R100] p=120 run=05  best=0.01715  eval=193\n",
      "[R100] p=120 run=06  best=0.01463  eval=477\n",
      "[R100] p=120 run=07  best=0.02782  eval=313\n",
      "[R100] p=120 run=08  best=0.01389  eval=281\n",
      "[R100] p=120 run=09  best=0.00484  eval=316\n",
      "[R100] p=120 run=10  best=0.00143  eval=321\n",
      "\n",
      "=== RANK_TARGET=100 SUMMARY ===\n",
      "    patience  success_rate  mean_best_loss  mean_n_eval\n",
      "0        60           0.0        0.023439        123.2\n",
      "1        70           0.0        0.018457        132.2\n",
      "2        80           0.0        0.020295        191.9\n",
      "3        90           0.0        0.022522        224.1\n",
      "4       100           0.0        0.021317        199.5\n",
      "5       120           0.0        0.017788        315.0 \n",
      "\n",
      "[R 10] p= 80 run=01  best=0.00000  eval=23\n",
      "[R 10] p= 80 run=02  best=0.00824  eval=164\n",
      "[R 10] p= 80 run=03  best=0.00934  eval=206\n",
      "[R 10] p= 80 run=04  best=0.00077  eval=225\n",
      "[R 10] p= 80 run=05  best=0.00955  eval=100\n",
      "[R 10] p= 80 run=06  best=0.00000  eval=134\n",
      "[R 10] p= 80 run=07  best=0.01857  eval=201\n",
      "[R 10] p= 80 run=08  best=0.00000  eval=248\n",
      "[R 10] p= 80 run=09  best=0.00672  eval=174\n",
      "[R 10] p= 80 run=10  best=0.00000  eval=53\n",
      "\n",
      "=== RANK_TARGET=10 SUMMARY ===\n",
      "    patience  success_rate  mean_best_loss  mean_n_eval\n",
      "0        80           0.4        0.005319        152.8 \n",
      "\n",
      "All experiments done ✅\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Grid experiment:  PATIENCE sweep  (g=2, tail=5)\n",
    "  • Phase-A : RANK_TARGET = 100, PATIENCE ∈ {60,70,80,90,100,120}\n",
    "  • Phase-B : RANK_TARGET =  10, PATIENCE = 80  (可自行再加)\n",
    "Outputs (per rank target):\n",
    "  traces/, plots/, detail.csv, summary.csv,\n",
    "  p_grid_summary.png, avg_trace_by_patience.png\n",
    "\"\"\"\n",
    "\n",
    "import os, random, warnings, numpy as np, pandas as pd, torch, tqdm, matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ────────────────── 固定參數 ──────────────────\n",
    "SEED          = 42\n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TAIL_L        = 5\n",
    "GROUP_SIZE    = 2\n",
    "POP           = 20\n",
    "MAXITER       = 5_000\n",
    "N_REPEAT      = 10\n",
    "PATIENCE_GRID = {100: [60,70,80,90,100,120],   # Phase-A\n",
    "                 10 : [80]}                    # Phase-B，可自行再加\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "VOC = tok.vocab_size\n",
    "\n",
    "# ────────────── 共用小工具 ──────────────\n",
    "def enc(txts, dev):\n",
    "    return tok(txts, padding=\"max_length\", truncation=True,\n",
    "               max_length=512, return_tensors=\"pt\").to(dev)\n",
    "\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1)\n",
    "\n",
    "def is_good_q(t):\n",
    "    return \"?\" in t and 20 < len(tok(t)[\"input_ids\"]) - 2 < 500\n",
    "\n",
    "def suffix_str(ids): return \" \".join(tok.convert_ids_to_tokens(ids))\n",
    "\n",
    "def detect_key(sample, keys):\n",
    "    for k in sample:\n",
    "        if any(t in k.lower() for t in keys): return k\n",
    "    return list(sample.keys())[0]\n",
    "\n",
    "# ────────────── 載入 & 向量化語料 ──────────────\n",
    "def prepare_corpus(n_docs=1_000):\n",
    "    corpus_ds = load_dataset(\"BeIR/fiqa\", \"corpus\", split=\"corpus\")\n",
    "    docs      = random.sample(list(corpus_ds), n_docs)\n",
    "\n",
    "    cpu_bert  = BertModel.from_pretrained(\"bert-base-uncased\").eval()\n",
    "    cls_vecs  = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(0, n_docs, 32), desc=\"Corpus CLS\"):\n",
    "            cls_vecs.append(cpu_bert(**enc([d[\"text\"]\n",
    "                                for d in docs[i:i+32]], \"cpu\")).last_hidden_state[:,0,:])\n",
    "    C_CLS = torch.cat(cls_vecs)\n",
    "\n",
    "    q_ds  = load_dataset(\"BeIR/fiqa\", \"queries\", split=\"queries\")\n",
    "    qid_k = detect_key(q_ds[0], {\"id\",\"qid\",\"query_id\",\"number\"})\n",
    "    txt_k = detect_key(q_ds[0], {\"text\",\"query\",\"body\"})\n",
    "    pool  = [{\"id\": q[qid_k], \"text\": q[txt_k]}\n",
    "             for q in q_ds if is_good_q(q[txt_k])]\n",
    "    return C_CLS, pool\n",
    "\n",
    "C_CLS, pool = prepare_corpus()\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\",\n",
    "                                 torch_dtype=torch.float16).to(DEVICE).eval()\n",
    "\n",
    "# ────────────── 單 run 最佳化 ──────────────\n",
    "def optimise_suffix(group_q, trace_csv, patience, rank_target):\n",
    "    pos = list(range(512 - TAIL_L, 512))\n",
    "    bounds = [(0, VOC-1)] * TAIL_L\n",
    "    infos  = []\n",
    "\n",
    "    for q in group_q:\n",
    "        e = enc([q[\"text\"]], DEVICE)\n",
    "        ids, msk = e[\"input_ids\"][0], e[\"attention_mask\"][0]\n",
    "        with torch.no_grad():\n",
    "            qcls = bert(**e).last_hidden_state[:,0,:]\n",
    "        sims    = cos_row(qcls.cpu(), C_CLS)\n",
    "        tgt_idx = torch.argsort(sims, descending=True)[rank_target-1].item()\n",
    "        tgt_cls = C_CLS[tgt_idx:tgt_idx+1].to(DEVICE)\n",
    "        CP      = C_CLS[[i for i in range(len(C_CLS)) if i != tgt_idx]]\n",
    "        infos.append((ids, msk, CP, tgt_cls))\n",
    "\n",
    "    running, best, stale = [], 1e9, 0\n",
    "    def joint_loss(v):\n",
    "        v = [int(round(x)) for x in v]\n",
    "        loss = 0.\n",
    "        for ids, msk, CP, tgt in infos:\n",
    "            p, m = ids.clone(), msk.clone(); m[pos] = 1\n",
    "            for i, t in zip(pos, v): p[i] = t\n",
    "            cls = bert(input_ids=p.unsqueeze(0),\n",
    "                       attention_mask=m.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "            kth = torch.topk(cos_row(cls, CP), 1).values[-1]\n",
    "            sim = torch.nn.functional.cosine_similarity(cls, tgt)[0]\n",
    "            loss += max(0., (kth - sim).item())\n",
    "        return loss/len(infos)\n",
    "\n",
    "    def cb(xk,_):\n",
    "        nonlocal best, stale\n",
    "        cur = joint_loss(xk)\n",
    "        if cur < best: best, stale = cur,0\n",
    "        else:          stale += 1\n",
    "        running.append(best)\n",
    "        return cur==0 or stale>=patience\n",
    "\n",
    "    _ = differential_evolution(joint_loss, bounds, popsize=POP,\n",
    "                               maxiter=MAXITER, tol=0, polish=False,\n",
    "                               seed=SEED, callback=cb)\n",
    "\n",
    "    pd.DataFrame({\"eval\": np.arange(1,len(running)+1),\n",
    "                  \"best_loss\": running}).to_csv(trace_csv, index=False)\n",
    "    return best, len(running)\n",
    "\n",
    "def plot_trace(csv_path, png_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.plot(df[\"eval\"], df[\"best_loss\"], color=\"red\")\n",
    "    plt.xlabel(\"function eval\"); plt.ylabel(\"min loss so far\")\n",
    "    plt.title(\"Running Min Loss\"); plt.tight_layout()\n",
    "    plt.savefig(png_path, dpi=300); plt.close()\n",
    "\n",
    "# ────────────── 主迴圈 (對每個 rank_target) ──────────────\n",
    "for rank_target, pat_grid in PATIENCE_GRID.items():\n",
    "    BASE = f\"exp_results/grid_r{rank_target}\"\n",
    "    os.makedirs(f\"{BASE}/traces\", exist_ok=True)\n",
    "    os.makedirs(f\"{BASE}/plots\" , exist_ok=True)\n",
    "\n",
    "    rows = []\n",
    "    for pat in pat_grid:\n",
    "        for run in range(1, N_REPEAT+1):\n",
    "            qset  = random.sample(pool, GROUP_SIZE)\n",
    "            t_csv = f\"{BASE}/traces/g{GROUP_SIZE}_p{pat}_run{run}.csv\"\n",
    "            p_png = f\"{BASE}/plots/g{GROUP_SIZE}_p{pat}_run{run}.png\"\n",
    "\n",
    "            best_loss, n_eval = optimise_suffix(qset, t_csv, pat, rank_target)\n",
    "            plot_trace(t_csv, p_png)\n",
    "\n",
    "            rows.append({\"patience\":pat,\"run\":run,\n",
    "                         \"best_loss\":best_loss,\n",
    "                         \"success\":int(best_loss==0),\n",
    "                         \"n_eval\":n_eval})\n",
    "            print(f\"[R{rank_target:3d}] p={pat:3d} run={run:02d}  \"\n",
    "                  f\"best={best_loss:.5f}  eval={n_eval}\")\n",
    "\n",
    "    # ── 整理 & 畫 summary ──\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(f\"{BASE}/detail.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    summary = df.groupby(\"patience\").agg(\n",
    "        success_rate=(\"success\",\"mean\"),\n",
    "        mean_best_loss=(\"best_loss\",\"mean\"),\n",
    "        mean_n_eval=(\"n_eval\",\"mean\")\n",
    "    ).reset_index()\n",
    "    summary.to_csv(f\"{BASE}/summary.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # 成功率 / #eval 雙軸\n",
    "    fig, ax1 = plt.subplots(figsize=(5,3.5))\n",
    "    ax1.plot(summary[\"patience\"], summary[\"success_rate\"],\n",
    "             marker=\"o\", label=\"success rate\")\n",
    "    ax1.set_xlabel(\"PATIENCE\"); ax1.set_ylabel(\"success rate\"); ax1.grid(True)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(summary[\"patience\"], summary[\"mean_n_eval\"],\n",
    "             marker=\"s\", linestyle=\"--\", color=\"gray\", label=\"avg #eval\")\n",
    "    ax2.set_ylabel(\"avg function evals\")\n",
    "    fig.suptitle(f\"RANK_TARGET={rank_target}  (g=2, tail=5)\")\n",
    "    fig.tight_layout(); fig.savefig(f\"{BASE}/p_grid_summary.png\", dpi=300); plt.close()\n",
    "\n",
    "    # 平均損失曲線\n",
    "    colors = plt.get_cmap(\"tab10\").colors\n",
    "    plt.figure(figsize=(5,3.5))\n",
    "    for idx, pat in enumerate(sorted(pat_grid)):\n",
    "        runs = [f\"{BASE}/traces/g{GROUP_SIZE}_p{pat}_run{r}.csv\"\n",
    "                for r in range(1,N_REPEAT+1) if os.path.exists(\n",
    "                    f\"{BASE}/traces/g{GROUP_SIZE}_p{pat}_run{r}.csv\")]\n",
    "        if not runs: continue\n",
    "        dfs   = [pd.read_csv(p) for p in runs]\n",
    "        maxL  = max(len(df) for df in dfs)\n",
    "        big   = pd.DataFrame(index=np.arange(1,maxL+1))\n",
    "        for j, df in enumerate(dfs):\n",
    "            big[f\"run{j}\"] = pd.Series(df[\"best_loss\"].values,\n",
    "                                       index=df[\"eval\"].values)\n",
    "        avg_curve = big.ffill().mean(axis=1)\n",
    "        plt.plot(avg_curve.index, avg_curve.values,\n",
    "                 color=colors[idx%10], label=f\"PATIENCE={pat}\")\n",
    "    plt.xlabel(\"function eval\"); plt.ylabel(\"avg best loss\")\n",
    "    plt.title(f\"Avg Running Best Loss  (R{rank_target})\")\n",
    "    plt.legend(frameon=False, fontsize=\"small\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{BASE}/avg_trace_by_patience.png\", dpi=300); plt.close()\n",
    "\n",
    "    print(f\"\\n=== RANK_TARGET={rank_target} SUMMARY ===\\n\", summary, \"\\n\")\n",
    "\n",
    "print(\"All experiments done ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7cf59-629e-4b20-bfde-9cc720299036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
