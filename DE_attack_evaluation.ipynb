{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4a8779-9bce-422b-aad4-c62b1d8c27ef",
   "metadata": {},
   "source": [
    "# DeRAG Prompt Attack Tutorial  \n",
    "**Part 1 – Retrieval-Augmented Generation (RAG)**  \n",
    "Jerry Wang  \n",
    "August 2025\n",
    "\n",
    "This notebook will demonstrate how to perform black-box prompt injection attacks on Retrieval-Augmented Generation (RAG) systems using Differential Evolution (DE). Inspired by the one-pixel attack in vision, our goal is to craft a short adversarial suffix that re-ranks retrieval results—causing a target incorrect passage to be retrieved instead of the correct one.\n",
    "\n",
    "DeRAG treats the entire RAG pipeline as a black box: we do not require gradient access, model internals, or retriever weights. Instead, we evolve a population of suffix candidates and measure their ranking success through similarity metrics, aiming to minimize the number of tokens needed for a successful attack.\n",
    "\n",
    "In theory, robust RAG models should be resistant to such small manipulations. However, our results reveal that even a few appended tokens (often ≤ 5) can reliably mislead state-of-the-art retrievers. This demonstrates a fundamental vulnerability in many LLM-based QA systems.\n",
    "\n",
    "To learn more, please refer to our paper:\n",
    "**“DeRAG: Black-box Adversarial Attacks on Retrieval-Augmented Generation Applications via Prompt Injection”**  \n",
    "Presented at KDD Workshop on Prompt Optimization (2025).  \n",
    "[GitHub Repo](https://github.com/pen9rum/Rag_attack_DeRag)\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "---\n",
    "\n",
    "##  Imports  \n",
    "Ensure that you have the following packages installed:\n",
    "```bash\n",
    "!pip install -q datasets transformers scipy  ir-datasets tqdm --upgrade \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb524e98-2aad-41dd-9e16-cf47531a8b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install datasets  ir-datasets transformers scipy tqdm --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb5052-d1cb-44cd-8d5c-244184e444b4",
   "metadata": {},
   "source": [
    "## DeRAG Attack Across Four Datasets\n",
    "\n",
    "This notebook implements and benchmarks DeRAG—a black-box prompt injection attack using Differential Evolution (DE)—across four benchmark datasets: **MS MARCO**, **FiQA**, **FEVER**, and **SciFact**. Each dataset is drawn from the [BEIR benchmark suite](https://arxiv.org/abs/2104.08663), covering domains from financial QA to fact verification.\n",
    "\n",
    "For each dataset, we randomly sample 1,000 documents and 100 queries. Then, for every query, we select a target \"incorrect\" document from the non-relevant set and attempt to promote it into the Top-1, Top-10, or Top-20 retrieved results using several attack methods.\n",
    "\n",
    "The evaluated attack methods include:\n",
    "- `none`: Baseline retrieval without perturbation.\n",
    "- `random`: Uniformly sampling suffix tokens.\n",
    "- `greedy`: HotFlip-style greedy search.\n",
    "- `ggpp`: Gradient-Guided Prompt Perturbation (white-box).\n",
    "- `DE_fixed`: Fixed-length DE without early stopping.\n",
    "- `DE_seq`: Sequential DE with incremental suffix lengths.\n",
    "- `DE_fixed_stop`: Fixed-length DE with early stopping.\n",
    "- `DE_seq_stop`: Sequential DE with early stopping (default method).\n",
    "\n",
    "Each method is evaluated using multiple metrics:\n",
    "- `Success@K`: Whether the target document appears in the top-K results.\n",
    "- `Token Used`: Number of tokens in the adversarial suffix.\n",
    "- `Iteration Count`: Optimization steps required.\n",
    "- `ΔMRR`: Change in Mean Reciprocal Rank.\n",
    "- `ΔnDCG@20`: Change in ranking quality at cutoff 20.\n",
    "- `ΔCosine`: Change in semantic similarity between the query and the target document.\n",
    "\n",
    "To begin running the experiments, simply execute the `run_all()` function for your desired ranking threshold:\n",
    "\n",
    "```python\n",
    "for K in (1, 10, 20):\n",
    "    run_all(topk_loss=K, save_dir=f\"results_top{K}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd159326-2a82-4954-be0d-d6d60895c14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 63/63 [00:13<00:00,  4.58it/s]\n",
      "Top-1:  53%|█████████████████████████████████████                                 | 53/100 [7:23:39<4:48:51, 368.76s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Top-1: 100%|██████████████████████████████████████████████████████████████████████| 100/100 [13:45:01<00:00, 495.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ results_top1  rows = 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 63/63 [00:16<00:00,  3.78it/s]\n",
      "Top-10:  10%|███████                                                                | 10/100 [48:14<7:12:15, 288.18s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n",
      "Top-10: 100%|██████████████████████████████████████████████████████████████████████| 100/100 [8:25:42<00:00, 303.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ results_top10  rows = 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 63/63 [00:17<00:00,  3.69it/s]\n",
      "Top-20:  36%|████████████████████████▊                                            | 36/100 [2:43:09<2:36:58, 147.16s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (899 > 512). Running this sequence through the model will result in indexing errors\n",
      "Top-20: 100%|██████████████████████████████████████████████████████████████████████| 100/100 [6:48:30<00:00, 245.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ results_top20  rows = 736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, random, math, tqdm, warnings, numpy as np, pandas as pd, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_DOCS     = 1_000\n",
    "N_Q        = 100\n",
    "TAIL_L     = 5         \n",
    "BUDGET     = 150       \n",
    "PATIENCE   = 20       \n",
    "BATCH_CLS  = 16     \n",
    "\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1)\n",
    "\n",
    "def dcg(rank): return 1 / math.log2(rank + 1)\n",
    "\n",
    "def load_subset():\n",
    "    c = load_dataset(\"BeIR/scifact\", \"corpus\",  split=\"corpus\")\n",
    "    q = load_dataset(\"BeIR/scifact\", \"queries\", split=\"queries\")\n",
    "    docs = random.sample(list(c), N_DOCS)\n",
    "    qs   = random.sample(list(q), N_Q)\n",
    "    return [d[\"text\"] for d in docs], [x[\"text\"] for x in qs]\n",
    "\n",
    "def ggpp_full(tok, bert, ids, msk, CP, tgt_cls, tgt_txt,\n",
    "              cap_L=5, max_epoch=150, topk_loss=20):\n",
    "    body = tok(tgt_txt, add_special_tokens=False,\n",
    "               truncation=True, max_length=510)[\"input_ids\"]\n",
    "    body += [tok.unk_token_id] * max(0, cap_L - len(body))\n",
    "\n",
    "    base = bert(**tok(tgt_txt, return_tensors=\"pt\",\n",
    "                      truncation=True, max_length=512).to(DEVICE)\n",
    "               ).last_hidden_state[0, 0]\n",
    "    imp = []\n",
    "    for i in range(cap_L):\n",
    "        tmp = body.copy(); tmp[i] = tok.mask_token_id\n",
    "        tens = torch.tensor([tok.cls_token_id]+tmp+[tok.sep_token_id]\n",
    "                           ).unsqueeze(0).to(DEVICE)\n",
    "        emb = bert(input_ids=tens,\n",
    "                   attention_mask=torch.ones_like(tens)).last_hidden_state[0, 0]\n",
    "        imp.append(1 - torch.nn.functional.cosine_similarity(base, emb, dim=0).item())\n",
    "    prefix = [body[i] for i in np.argsort(imp)[-cap_L:]]\n",
    "\n",
    "    pos   = list(range(502, 502+cap_L))\n",
    "    patch = ids.clone()\n",
    "    for p, v in zip(pos, prefix): patch[p] = v\n",
    "    am = msk.clone(); am[pos] = 1\n",
    "    W  = bert.embeddings.word_embeddings.weight\n",
    "\n",
    "    def loss_fn(cls_vec):\n",
    "        kth=torch.topk(cos_row(cls_vec,CP),topk_loss).values[-1]\n",
    "        sim=torch.nn.functional.cosine_similarity(cls_vec,tgt_cls)[0]\n",
    "        return torch.relu(kth-sim)\n",
    "\n",
    "    best_cls=bert(input_ids=patch.unsqueeze(0),\n",
    "                  attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "    best_loss=loss_fn(best_cls).item(); used_iter=0\n",
    "\n",
    "    for _ in range(max_epoch):\n",
    "        used_iter+=1\n",
    "        emb=bert.embeddings.word_embeddings(\n",
    "            patch.unsqueeze(0)).detach().clone().requires_grad_(True)\n",
    "        cls=bert(inputs_embeds=emb,\n",
    "                 attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "        loss=loss_fn(cls); loss.backward()\n",
    "        if loss.item()==0: best_cls=cls; break\n",
    "        grad=emb.grad[0,pos]; score=torch.matmul(grad,W.t())\n",
    "        cand_ids=score.topk(5,largest=False,dim=1).indices.cpu()\n",
    "        improved=False\n",
    "        for mask in range(1,1<<cap_L):\n",
    "            cand=patch.clone()\n",
    "            for i in range(cap_L):\n",
    "                if mask&(1<<i): cand[pos[i]]=random.choice(cand_ids[i]).item()\n",
    "            cls2=bert(input_ids=cand.unsqueeze(0),\n",
    "                      attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "            l2=loss_fn(cls2).item()\n",
    "            if l2<best_loss:\n",
    "                best_loss=l2; patch=cand; best_cls=cls2; improved=True\n",
    "            if l2==0: break\n",
    "        if not improved: break\n",
    "    return best_cls,cap_L,used_iter\n",
    "\n",
    "\n",
    "def run_all(topk_loss, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    tok  = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(DEVICE)\n",
    "    VOC  = tok.vocab_size\n",
    "    def enc(t): return tok(t, padding=\"max_length\", truncation=True,\n",
    "                           max_length=512, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    docs, qs = load_subset()\n",
    "    CLS=[]\n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm.tqdm(range(0, len(docs), BATCH_CLS), desc=\"CLS\"):\n",
    "            bt = enc(docs[i:i+BATCH_CLS])\n",
    "            cls= bert(**bt).last_hidden_state[:, 0, :].cpu()\n",
    "            CLS.append(cls)\n",
    "            del bt; torch.cuda.empty_cache()\n",
    "    C_CLS = torch.cat(CLS)\n",
    "\n",
    "    METHODS = (\"none\", \"random\", \"greedy\", \"ggpp\",\n",
    "               \"DE_fixed\", \"DE_seq\", \"DE_fixed_stop\", \"DE_seq_stop\")\n",
    "    rec=[]\n",
    "\n",
    "    for qtxt in tqdm.tqdm(qs, desc=f\"Top-{topk_loss}\"):\n",
    "        tgt = random.randrange(len(docs))\n",
    "        tgt_txt = docs[tgt]\n",
    "        if len(tok(tgt_txt)[\"input_ids\"]) > 510: continue\n",
    "        tgt_cls = C_CLS[tgt:tgt+1].to(DEVICE)\n",
    "        CP = C_CLS[[i for i in range(len(docs)) if i != tgt]]\n",
    "\n",
    "        qenc = enc(qtxt); ids = qenc[\"input_ids\"][0]; msk = qenc[\"attention_mask\"][0]\n",
    "        with torch.no_grad():\n",
    "            qcls = bert(**qenc).last_hidden_state[:, 0, :]\n",
    "\n",
    "        base = torch.cat([\n",
    "            cos_row(qcls, CP).cpu(),    \n",
    "            torch.nn.functional.cosine_similarity(qcls.cpu(), tgt_cls.cpu())\n",
    "        ])\n",
    "        rank_b = (base > base[-1]).sum().item() + 1\n",
    "\n",
    "        for mtd in METHODS:\n",
    "            success=False; used_L=0; used_iter=0; adv_cls=qcls\n",
    "            if mtd==\"random\":\n",
    "                best_loss=1e9\n",
    "                for _ in range(BUDGET):\n",
    "                    patch=ids.clone()\n",
    "                    for p in range(502, 502+TAIL_L):\n",
    "                        patch[p] = random.randrange(VOC)\n",
    "                    am = msk.clone(); am[502:502+TAIL_L] = 1\n",
    "                    cls = bert(input_ids=patch.unsqueeze(0),\n",
    "                               attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "                    kth = torch.topk(cos_row(cls, CP), topk_loss).values[-1]\n",
    "                    loss= max(0., (kth - torch.nn.functional.cosine_similarity(\n",
    "                                        cls, tgt_cls)[0]).item())\n",
    "                    used_iter += 1\n",
    "                    if loss < best_loss:\n",
    "                        best_loss=loss; adv_cls=cls; success=(loss==0)\n",
    "                    if success: break\n",
    "                used_L=TAIL_L\n",
    "            elif mtd==\"greedy\":\n",
    "                pos=list(range(502, 502+TAIL_L)); patch=ids.clone()\n",
    "                best_loss=1e9\n",
    "                for _ in range(BUDGET):\n",
    "                    used_iter += 1; improved=False\n",
    "                    for p in pos:\n",
    "                        best_id=patch[p].item()\n",
    "                        for cand in random.sample(range(VOC),512):\n",
    "                            patch[p]=cand\n",
    "                            am = msk.clone(); am[pos]=1\n",
    "                            cls=bert(input_ids=patch.unsqueeze(0),\n",
    "                                     attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "                            kth=torch.topk(cos_row(cls,CP),topk_loss).values[-1]\n",
    "                            loss=max(0.,(kth-torch.nn.functional.cosine_similarity(\n",
    "                                              cls,tgt_cls)[0]).item())\n",
    "                            if loss<best_loss:\n",
    "                                best_loss=loss; best_id=cand; adv_cls=cls; improved=True\n",
    "                            if loss==0: success=True; break\n",
    "                        patch[p]=best_id\n",
    "                        if success: break\n",
    "                    if success or not improved: break\n",
    "                used_L=TAIL_L\n",
    "            elif mtd==\"ggpp\":\n",
    "                adv_cls, used_L, used_iter = ggpp_full(\n",
    "                    tok, bert, ids, msk, CP, tgt_cls, tgt_txt,\n",
    "                    cap_L=TAIL_L, max_epoch=BUDGET, topk_loss=topk_loss)\n",
    "                kth = torch.topk(cos_row(adv_cls, CP), topk_loss).values[-1]\n",
    "                success = (kth <= torch.nn.functional.cosine_similarity(\n",
    "                                      adv_cls, tgt_cls)[0])\n",
    "            def de_run(L, max_iter, plateau):\n",
    "                pos=list(range(502, 502+L)); bounds=[(0, VOC-1)]*L\n",
    "                gens = max_iter//20 if max_iter else 1000\n",
    "                stop = [0]; best=[1e9]\n",
    "                def obj(v):\n",
    "                    v=[int(round(x)) for x in v]\n",
    "                    patch=ids.clone()\n",
    "                    for p,t in zip(pos,v): patch[p]=t\n",
    "                    am=msk.clone(); am[pos]=1\n",
    "                    cls=bert(input_ids=patch.unsqueeze(0),\n",
    "                             attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "                    kth=torch.topk(cos_row(cls,CP),topk_loss).values[-1]\n",
    "                    sim=torch.nn.functional.cosine_similarity(cls,tgt_cls)[0]\n",
    "                    return max(0.,(kth-sim).item())\n",
    "                def cb(xk,_):\n",
    "                    if not plateau: return False\n",
    "                    cur=obj(xk)\n",
    "                    stop[0] = 0 if cur < best[0] else stop[0]+1\n",
    "                    best[0] = min(best[0], cur)\n",
    "                    return stop[0] >= PATIENCE or cur == 0\n",
    "                res = differential_evolution(obj, bounds, popsize=20,\n",
    "                                             maxiter=gens, tol=0,\n",
    "                                             polish=False, seed=SEED,\n",
    "                                             callback=cb)\n",
    "                v=[int(round(x)) for x in res.x]\n",
    "                patch=ids.clone()\n",
    "                for p,t in zip(pos,v): patch[p]=t\n",
    "                am=msk.clone(); am[pos]=1\n",
    "                cls=bert(input_ids=patch.unsqueeze(0),\n",
    "                         attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "                return cls, res.nfev, res.fun==0\n",
    "\n",
    "            if mtd==\"DE_fixed\":\n",
    "                adv_cls, used_iter, success = de_run(TAIL_L, BUDGET, plateau=False); used_L=TAIL_L\n",
    "            if mtd==\"DE_fixed_stop\":\n",
    "                adv_cls, used_iter, success = de_run(TAIL_L, None, plateau=True); used_L=TAIL_L\n",
    "            if mtd==\"DE_seq\":\n",
    "                for L in range(1, TAIL_L+1):\n",
    "                    cls,iters,ok = de_run(L, BUDGET, plateau=False)\n",
    "                    used_iter += iters\n",
    "                    if ok: adv_cls=cls; used_L=L; success=True; break\n",
    "                else: used_L=TAIL_L\n",
    "            if mtd==\"DE_seq_stop\":\n",
    "                for L in range(1, TAIL_L+1):\n",
    "                    cls,iters,ok = de_run(L, None, plateau=True)\n",
    "                    used_iter += iters\n",
    "                    if ok: adv_cls=cls; used_L=L; success=True; break\n",
    "                else: used_L=TAIL_L-\n",
    "            sims = torch.cat([\n",
    "                cos_row(adv_cls.cpu(), CP),      \n",
    "                torch.nn.functional.cosine_similarity(\n",
    "                    adv_cls.cpu(), tgt_cls.cpu())\n",
    "            ])\n",
    "            rank_a = (sims > sims[-1]).sum().item() + 1\n",
    "            d_mrr  = 1/rank_a - 1/rank_b\n",
    "            d_ndcg = ((dcg(rank_a) if rank_a<=20 else 0) -\n",
    "                      (dcg(rank_b) if rank_b<=20 else 0))\n",
    "            d_cos  = (torch.nn.functional.cosine_similarity(\n",
    "                        adv_cls, tgt_cls)[0] -\n",
    "                      torch.nn.functional.cosine_similarity(\n",
    "                        qcls, tgt_cls)[0]).item()\n",
    "\n",
    "            rec.append(dict(\n",
    "                top_k      = topk_loss,\n",
    "                method     = mtd,\n",
    "                success    = int(rank_a <= topk_loss),\n",
    "                token_used = used_L,\n",
    "                iter_used  = used_iter,\n",
    "                delta_mrr  = d_mrr,\n",
    "                delta_ndcg = d_ndcg,\n",
    "                delta_cos  = d_cos\n",
    "            ))\n",
    "        torch.cuda.empty_cache()  \n",
    "\n",
    "    pd.DataFrame(rec).to_csv(f\"{save_dir}/records.csv\", index=False)\n",
    "    print(f\"✓ {save_dir}  rows = {len(rec)}\")\n",
    "for K in (1, 10, 20):\n",
    "    run_all(topk_loss=K, save_dir=f\"results_top{K}\")\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcbd1d5a-d05c-47d1-a21b-a66678756844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 63/63 [00:12<00:00,  4.89it/s]\n",
      "Top-1:   0%|                                                                                    | 0/100 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1585 > 512). Running this sequence through the model will result in indexing errors\n",
      "Top-1: 100%|██████████████████████████████████████████████████████████████████████| 100/100 [10:01:57<00:00, 361.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ results_top1  rows = 665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 63/63 [00:12<00:00,  4.87it/s]\n",
      "Top-10:  73%|██████████████████████████████████████████████████▎                  | 73/100 [5:35:33<2:01:09, 269.25s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (664 > 512). Running this sequence through the model will result in indexing errors\n",
      "Top-10: 100%|██████████████████████████████████████████████████████████████████████| 100/100 [7:19:14<00:00, 263.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ results_top10  rows = 686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 63/63 [00:13<00:00,  4.79it/s]\n",
      "Top-20:  62%|██████████████████████████████████████████▊                          | 62/100 [4:15:20<2:21:34, 223.55s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Top-20: 100%|██████████████████████████████████████████████████████████████████████| 100/100 [7:03:33<00:00, 254.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ results_top20  rows = 693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, random, math, tqdm, warnings, numpy as np, pandas as pd, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_DOCS     = 1_000\n",
    "N_Q        = 100\n",
    "TAIL_L     = 5          \n",
    "BUDGET     = 150        \n",
    "PATIENCE   = 20         \n",
    "BATCH_CLS  = 16         \n",
    "\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1)\n",
    "\n",
    "def dcg(rank): return 1 / math.log2(rank + 1)\n",
    "\n",
    "def load_subset():\n",
    "    c = load_dataset(\"BeIR/fiqa\", \"corpus\",  split=\"corpus\")\n",
    "    q = load_dataset(\"BeIR/fiqa\", \"queries\", split=\"queries\")\n",
    "    docs = random.sample(list(c), N_DOCS)\n",
    "    qs   = random.sample(list(q), N_Q)\n",
    "    return [d[\"text\"] for d in docs], [x[\"text\"] for x in qs]\n",
    "\n",
    "def ggpp_full(tok, bert, ids, msk, CP, tgt_cls, tgt_txt,\n",
    "              cap_L=5, max_epoch=150, topk_loss=20):\n",
    "    body = tok(tgt_txt, add_special_tokens=False,\n",
    "               truncation=True, max_length=510)[\"input_ids\"]\n",
    "    body += [tok.unk_token_id] * max(0, cap_L - len(body))\n",
    "\n",
    "    base = bert(**tok(tgt_txt, return_tensors=\"pt\",\n",
    "                      truncation=True, max_length=512).to(DEVICE)\n",
    "               ).last_hidden_state[0, 0]\n",
    "    imp = []\n",
    "    for i in range(cap_L):\n",
    "        tmp = body.copy(); tmp[i] = tok.mask_token_id\n",
    "        tens = torch.tensor([tok.cls_token_id]+tmp+[tok.sep_token_id]\n",
    "                           ).unsqueeze(0).to(DEVICE)\n",
    "        emb = bert(input_ids=tens,\n",
    "                   attention_mask=torch.ones_like(tens)).last_hidden_state[0, 0]\n",
    "        imp.append(1 - torch.nn.functional.cosine_similarity(base, emb, dim=0).item())\n",
    "    prefix = [body[i] for i in np.argsort(imp)[-cap_L:]]\n",
    "\n",
    "    pos   = list(range(502, 502+cap_L))\n",
    "    patch = ids.clone()\n",
    "    for p, v in zip(pos, prefix): patch[p] = v\n",
    "    am = msk.clone(); am[pos] = 1\n",
    "    W  = bert.embeddings.word_embeddings.weight\n",
    "\n",
    "    def loss_fn(cls_vec):\n",
    "        kth=torch.topk(cos_row(cls_vec,CP),topk_loss).values[-1]\n",
    "        sim=torch.nn.functional.cosine_similarity(cls_vec,tgt_cls)[0]\n",
    "        return torch.relu(kth-sim)\n",
    "\n",
    "    best_cls=bert(input_ids=patch.unsqueeze(0),\n",
    "                  attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "    best_loss=loss_fn(best_cls).item(); used_iter=0\n",
    "\n",
    "    for _ in range(max_epoch):\n",
    "        used_iter+=1\n",
    "        emb=bert.embeddings.word_embeddings(\n",
    "            patch.unsqueeze(0)).detach().clone().requires_grad_(True)\n",
    "        cls=bert(inputs_embeds=emb,\n",
    "                 attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "        loss=loss_fn(cls); loss.backward()\n",
    "        if loss.item()==0: best_cls=cls; break\n",
    "        grad=emb.grad[0,pos]; score=torch.matmul(grad,W.t())\n",
    "        cand_ids=score.topk(5,largest=False,dim=1).indices.cpu()\n",
    "        improved=False\n",
    "        for mask in range(1,1<<cap_L):\n",
    "            cand=patch.clone()\n",
    "            for i in range(cap_L):\n",
    "                if mask&(1<<i): cand[pos[i]]=random.choice(cand_ids[i]).item()\n",
    "            cls2=bert(input_ids=cand.unsqueeze(0),\n",
    "                      attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "            l2=loss_fn(cls2).item()\n",
    "            if l2<best_loss:\n",
    "                best_loss=l2; patch=cand; best_cls=cls2; improved=True\n",
    "            if l2==0: break\n",
    "        if not improved: break\n",
    "    return best_cls,cap_L,used_iter\n",
    "\n",
    "def run_all(topk_loss, save_dir):\n",
    "    os.makedirs(f\"{save_dir}/fiqa\", exist_ok=True)\n",
    "\n",
    "    tok  = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(DEVICE)\n",
    "    VOC  = tok.vocab_size\n",
    "    def enc(t): return tok(t, padding=\"max_length\", truncation=True,\n",
    "                           max_length=512, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    docs, qs = load_subset()\n",
    "    CLS=[]\n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm.tqdm(range(0, len(docs), BATCH_CLS), desc=\"CLS\"):\n",
    "            bt = enc(docs[i:i+BATCH_CLS])\n",
    "            cls= bert(**bt).last_hidden_state[:, 0, :].cpu()\n",
    "            CLS.append(cls)\n",
    "            del bt; torch.cuda.empty_cache()\n",
    "    C_CLS = torch.cat(CLS)\n",
    "\n",
    "    METHODS = (\"none\", \"random\", \"ggpp\",\n",
    "               \"DE_fixed\", \"DE_seq\", \"DE_fixed_stop\", \"DE_seq_stop\") \n",
    "    rec=[]\n",
    "    for qtxt in tqdm.tqdm(qs, desc=f\"Top-{topk_loss}\"):\n",
    "        tgt = random.randrange(len(docs))\n",
    "        tgt_txt = docs[tgt]\n",
    "        if len(tok(tgt_txt)[\"input_ids\"]) > 510: continue\n",
    "        tgt_cls = C_CLS[tgt:tgt+1].to(DEVICE)\n",
    "        CP = C_CLS[[i for i in range(len(docs)) if i != tgt]]\n",
    "\n",
    "        qenc = enc(qtxt); ids = qenc[\"input_ids\"][0]; msk = qenc[\"attention_mask\"][0]\n",
    "        with torch.no_grad():\n",
    "            qcls = bert(**qenc).last_hidden_state[:, 0, :]\n",
    "        base = torch.cat([\n",
    "            cos_row(qcls, CP).cpu(),     # ★ 移到 CPU\n",
    "            torch.nn.functional.cosine_similarity(qcls.cpu(), tgt_cls.cpu())\n",
    "        ])\n",
    "        rank_b = (base > base[-1]).sum().item() + 1\n",
    "\n",
    "        for mtd in METHODS:\n",
    "            success=False; used_L=0; used_iter=0; adv_cls=qcls\n",
    "            if mtd==\"random\":\n",
    "                best_loss=1e9\n",
    "                for _ in range(BUDGET):\n",
    "                    patch=ids.clone()\n",
    "                    for p in range(502, 502+TAIL_L):\n",
    "                        patch[p] = random.randrange(VOC)\n",
    "                    am = msk.clone(); am[502:502+TAIL_L] = 1\n",
    "                    cls = bert(input_ids=patch.unsqueeze(0),\n",
    "                               attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "                    kth = torch.topk(cos_row(cls, CP), topk_loss).values[-1]\n",
    "                    loss= max(0., (kth - torch.nn.functional.cosine_similarity(\n",
    "                                        cls, tgt_cls)[0]).item())\n",
    "                    used_iter += 1\n",
    "                    if loss < best_loss:\n",
    "                        best_loss=loss; adv_cls=cls; success=(loss==0)\n",
    "                    if success: break\n",
    "                used_L=TAIL_L\n",
    "            elif mtd==\"greedy\":\n",
    "                pos=list(range(502, 502+TAIL_L)); patch=ids.clone()\n",
    "                best_loss=1e9\n",
    "                for _ in range(BUDGET):\n",
    "                    used_iter += 1; improved=False\n",
    "                    for p in pos:\n",
    "                        best_id=patch[p].item()\n",
    "                        for cand in random.sample(range(VOC),512):\n",
    "                            patch[p]=cand\n",
    "                            am = msk.clone(); am[pos]=1\n",
    "                            cls=bert(input_ids=patch.unsqueeze(0),\n",
    "                                     attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "                            kth=torch.topk(cos_row(cls,CP),topk_loss).values[-1]\n",
    "                            loss=max(0.,(kth-torch.nn.functional.cosine_similarity(\n",
    "                                              cls,tgt_cls)[0]).item())\n",
    "                            if loss<best_loss:\n",
    "                                best_loss=loss; best_id=cand; adv_cls=cls; improved=True\n",
    "                            if loss==0: success=True; break\n",
    "                        patch[p]=best_id\n",
    "                        if success: break\n",
    "                    if success or not improved: break\n",
    "                used_L=TAIL_L\n",
    "            elif mtd==\"ggpp\":\n",
    "                adv_cls, used_L, used_iter = ggpp_full(\n",
    "                    tok, bert, ids, msk, CP, tgt_cls, tgt_txt,\n",
    "                    cap_L=TAIL_L, max_epoch=BUDGET, topk_loss=topk_loss)\n",
    "                kth = torch.topk(cos_row(adv_cls, CP), topk_loss).values[-1]\n",
    "                success = (kth <= torch.nn.functional.cosine_similarity(\n",
    "                                      adv_cls, tgt_cls)[0])\n",
    "            def de_run(L, max_iter, plateau):\n",
    "                pos=list(range(502, 502+L)); bounds=[(0, VOC-1)]*L\n",
    "                gens = max_iter//20 if max_iter else 1000\n",
    "                stop = [0]; best=[1e9]\n",
    "                def obj(v):\n",
    "                    v=[int(round(x)) for x in v]\n",
    "                    patch=ids.clone()\n",
    "                    for p,t in zip(pos,v): patch[p]=t\n",
    "                    am=msk.clone(); am[pos]=1\n",
    "                    cls=bert(input_ids=patch.unsqueeze(0),\n",
    "                             attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "                    kth=torch.topk(cos_row(cls,CP),topk_loss).values[-1]\n",
    "                    sim=torch.nn.functional.cosine_similarity(cls,tgt_cls)[0]\n",
    "                    return max(0.,(kth-sim).item())\n",
    "                def cb(xk,_):\n",
    "                    if not plateau: return False\n",
    "                    cur=obj(xk)\n",
    "                    stop[0] = 0 if cur < best[0] else stop[0]+1\n",
    "                    best[0] = min(best[0], cur)\n",
    "                    return stop[0] >= PATIENCE or cur == 0\n",
    "                res = differential_evolution(obj, bounds, popsize=20,\n",
    "                                             maxiter=gens, tol=0,\n",
    "                                             polish=False, seed=SEED,\n",
    "                                             callback=cb)\n",
    "                v=[int(round(x)) for x in res.x]\n",
    "                patch=ids.clone()\n",
    "                for p,t in zip(pos,v): patch[p]=t\n",
    "                am=msk.clone(); am[pos]=1\n",
    "                cls=bert(input_ids=patch.unsqueeze(0),\n",
    "                         attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "                return cls, res.nfev, res.fun==0\n",
    "\n",
    "            if mtd==\"DE_fixed\":\n",
    "                adv_cls, used_iter, success = de_run(TAIL_L, BUDGET, plateau=False); used_L=TAIL_L\n",
    "            if mtd==\"DE_fixed_stop\":\n",
    "                adv_cls, used_iter, success = de_run(TAIL_L, None, plateau=True); used_L=TAIL_L\n",
    "            if mtd==\"DE_seq\":\n",
    "                for L in range(1, TAIL_L+1):\n",
    "                    cls,iters,ok = de_run(L, BUDGET, plateau=False)\n",
    "                    used_iter += iters\n",
    "                    if ok: adv_cls=cls; used_L=L; success=True; break\n",
    "                else: used_L=TAIL_L\n",
    "            if mtd==\"DE_seq_stop\":\n",
    "                for L in range(1, TAIL_L+1):\n",
    "                    cls,iters,ok = de_run(L, None, plateau=True)\n",
    "                    used_iter += iters\n",
    "                    if ok: adv_cls=cls; used_L=L; success=True; break\n",
    "                else: used_L=TAIL_L\n",
    "            sims = torch.cat([\n",
    "                cos_row(adv_cls.cpu(), CP),       \n",
    "                torch.nn.functional.cosine_similarity(\n",
    "                    adv_cls.cpu(), tgt_cls.cpu())\n",
    "            ])\n",
    "            rank_a = (sims > sims[-1]).sum().item() + 1\n",
    "            d_mrr  = 1/rank_a - 1/rank_b\n",
    "            d_ndcg = ((dcg(rank_a) if rank_a<=20 else 0) -\n",
    "                      (dcg(rank_b) if rank_b<=20 else 0))\n",
    "            d_cos  = (torch.nn.functional.cosine_similarity(\n",
    "                        adv_cls, tgt_cls)[0] -\n",
    "                      torch.nn.functional.cosine_similarity(\n",
    "                        qcls, tgt_cls)[0]).item()\n",
    "\n",
    "            rec.append(dict(\n",
    "                top_k      = topk_loss,\n",
    "                method     = mtd,\n",
    "                success    = int(rank_a <= topk_loss),\n",
    "                token_used = used_L,\n",
    "                iter_used  = used_iter,\n",
    "                delta_mrr  = d_mrr,\n",
    "                delta_ndcg = d_ndcg,\n",
    "                delta_cos  = d_cos\n",
    "            ))\n",
    "        torch.cuda.empty_cache()   \n",
    "    pd.DataFrame(rec).to_csv(f\"{save_dir}/fiqa/records.csv\", index=False)\n",
    "    print(f\"✓ {save_dir}  rows = {len(rec)}\")\n",
    "\n",
    "for K in (1, 10, 20):\n",
    "    run_all(topk_loss=K, save_dir=f\"results_top{K}\")\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43701963-300b-442d-8bea-a41f8a9a0696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 63/63 [00:09<00:00,  6.38it/s]\n",
      "Top-1: 100%|████████████████████████████████████████████████████████████████████████| 100/100 [1:31:07<00:00, 54.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ results_top1  rows = 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 63/63 [00:12<00:00,  4.89it/s]\n",
      "Top-10: 100%|███████████████████████████████████████████████████████████████████████| 100/100 [1:03:15<00:00, 37.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ results_top10  rows = 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 63/63 [00:09<00:00,  6.66it/s]\n",
      "Top-20: 100%|███████████████████████████████████████████████████████████████████████| 100/100 [1:09:40<00:00, 41.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ results_top20  rows = 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, random, math, tqdm, warnings, numpy as np, pandas as pd, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 41\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_DOCS     = 1_000\n",
    "N_Q        = 100\n",
    "TAIL_L     = 5         \n",
    "BUDGET     = 150      \n",
    "PATIENCE   = 20        \n",
    "BATCH_CLS  = 16       \n",
    "\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1)\n",
    "\n",
    "def dcg(rank): return 1 / math.log2(rank + 1)\n",
    "def load_subset():\n",
    "    ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\", split=\"validation\")  # ~10 k 条\n",
    "    all_passages, all_queries = [], []\n",
    "    for ex in ds:\n",
    "        all_queries.append(ex[\"query\"])\n",
    "        all_passages.extend(ex[\"passages\"])\n",
    "\n",
    "    docs = random.sample(all_passages, N_DOCS)\n",
    "    qs   = random.sample(all_queries, N_Q)\n",
    "    return docs, qs\n",
    "def ggpp_full(tok, bert, ids, msk, CP, tgt_cls, tgt_txt,\n",
    "              cap_L=5, max_epoch=150, topk_loss=20):\n",
    "    body = tok(tgt_txt, add_special_tokens=False,\n",
    "               truncation=True, max_length=510)[\"input_ids\"]\n",
    "    body += [tok.unk_token_id] * max(0, cap_L - len(body))\n",
    "\n",
    "    base = bert(**tok(tgt_txt, return_tensors=\"pt\",\n",
    "                      truncation=True, max_length=512).to(DEVICE)\n",
    "               ).last_hidden_state[0, 0]\n",
    "    imp = []\n",
    "    for i in range(cap_L):\n",
    "        tmp = body.copy(); tmp[i] = tok.mask_token_id\n",
    "        tens = torch.tensor([tok.cls_token_id]+tmp+[tok.sep_token_id]\n",
    "                           ).unsqueeze(0).to(DEVICE)\n",
    "        emb = bert(input_ids=tens,\n",
    "                   attention_mask=torch.ones_like(tens)).last_hidden_state[0, 0]\n",
    "        imp.append(1 - torch.nn.functional.cosine_similarity(base, emb, dim=0).item())\n",
    "    prefix = [body[i] for i in np.argsort(imp)[-cap_L:]]\n",
    "\n",
    "    pos   = list(range(502, 502+cap_L))\n",
    "    patch = ids.clone()\n",
    "    for p, v in zip(pos, prefix): patch[p] = v\n",
    "    am = msk.clone(); am[pos] = 1\n",
    "    W  = bert.embeddings.word_embeddings.weight\n",
    "    def loss_fn(cls_vec):\n",
    "        kth=torch.topk(cos_row(cls_vec,CP),topk_loss).values[-1]\n",
    "        sim=torch.nn.functional.cosine_similarity(cls_vec,tgt_cls)[0]\n",
    "        return torch.relu(kth-sim)\n",
    "\n",
    "    best_cls=bert(input_ids=patch.unsqueeze(0),\n",
    "                  attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "    best_loss=loss_fn(best_cls).item(); used_iter=0\n",
    "\n",
    "    for _ in range(max_epoch):\n",
    "        used_iter+=1\n",
    "        emb=bert.embeddings.word_embeddings(\n",
    "            patch.unsqueeze(0)).detach().clone().requires_grad_(True)\n",
    "        cls=bert(inputs_embeds=emb,\n",
    "                 attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "        loss=loss_fn(cls); loss.backward()\n",
    "        if loss.item()==0: best_cls=cls; break\n",
    "        grad=emb.grad[0,pos]; score=torch.matmul(grad,W.t())\n",
    "        cand_ids=score.topk(5,largest=False,dim=1).indices.cpu()\n",
    "        improved=False\n",
    "        for mask in range(1,1<<cap_L):\n",
    "            cand=patch.clone()\n",
    "            for i in range(cap_L):\n",
    "                if mask&(1<<i): cand[pos[i]]=random.choice(cand_ids[i]).item()\n",
    "            cls2=bert(input_ids=cand.unsqueeze(0),\n",
    "                      attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "            l2=loss_fn(cls2).item()\n",
    "            if l2<best_loss:\n",
    "                best_loss=l2; patch=cand; best_cls=cls2; improved=True\n",
    "            if l2==0: break\n",
    "        if not improved: break\n",
    "    return best_cls,cap_L,used_iter\n",
    "def run_all(topk_loss, save_dir):\n",
    "    os.makedirs(f\"{save_dir}/msmarco\", exist_ok=True)\n",
    "\n",
    "    tok  = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(DEVICE)\n",
    "    VOC  = tok.vocab_size\n",
    "    def enc(t): return tok(t, padding=\"max_length\", truncation=True,\n",
    "                           max_length=512, return_tensors=\"pt\").to(DEVICE)\n",
    "    docs, qs = load_subset()\n",
    "    CLS=[]\n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm.tqdm(range(0, len(docs), BATCH_CLS), desc=\"CLS\"):\n",
    "            bt = enc(docs[i:i+BATCH_CLS])\n",
    "            cls= bert(**bt).last_hidden_state[:, 0, :].cpu()\n",
    "            CLS.append(cls)\n",
    "            del bt; torch.cuda.empty_cache()\n",
    "    C_CLS = torch.cat(CLS)\n",
    "\n",
    "    METHODS = (\"none\", \"random\", \"ggpp\",\n",
    "               \"DE_fixed\", \"DE_seq\", \"DE_fixed_stop\", \"DE_seq_stop\") #\"greedy\"\n",
    "    rec=[]\n",
    "    for qtxt in tqdm.tqdm(qs, desc=f\"Top-{topk_loss}\"):\n",
    "        tgt = random.randrange(len(docs))\n",
    "        tgt_txt = docs[tgt]\n",
    "        if len(tok(tgt_txt)[\"input_ids\"]) > 510: continue\n",
    "        tgt_cls = C_CLS[tgt:tgt+1].to(DEVICE)\n",
    "        CP = C_CLS[[i for i in range(len(docs)) if i != tgt]]\n",
    "\n",
    "        qenc = enc(qtxt); ids = qenc[\"input_ids\"][0]; msk = qenc[\"attention_mask\"][0]\n",
    "        with torch.no_grad():\n",
    "            qcls = bert(**qenc).last_hidden_state[:, 0, :]-\n",
    "        base = torch.cat([\n",
    "            cos_row(qcls, CP).cpu(),     # ★ 移到 CPU\n",
    "            torch.nn.functional.cosine_similarity(qcls.cpu(), tgt_cls.cpu())\n",
    "        ])\n",
    "        rank_b = (base > base[-1]).sum().item() + 1\n",
    "\n",
    "        for mtd in METHODS:\n",
    "            success=False; used_L=0; used_iter=0; adv_cls=qcls\n",
    "            if mtd==\"random\":\n",
    "                best_loss=1e9\n",
    "                for _ in range(BUDGET):\n",
    "                    patch=ids.clone()\n",
    "                    for p in range(502, 502+TAIL_L):\n",
    "                        patch[p] = random.randrange(VOC)\n",
    "                    am = msk.clone(); am[502:502+TAIL_L] = 1\n",
    "                    cls = bert(input_ids=patch.unsqueeze(0),\n",
    "                               attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "                    kth = torch.topk(cos_row(cls, CP), topk_loss).values[-1]\n",
    "                    loss= max(0., (kth - torch.nn.functional.cosine_similarity(\n",
    "                                        cls, tgt_cls)[0]).item())\n",
    "                    used_iter += 1\n",
    "                    if loss < best_loss:\n",
    "                        best_loss=loss; adv_cls=cls; success=(loss==0)\n",
    "                    if success: break\n",
    "                used_L=TAIL_L\n",
    "            elif mtd==\"greedy\":\n",
    "                pos=list(range(502, 502+TAIL_L)); patch=ids.clone()\n",
    "                best_loss=1e9\n",
    "                for _ in range(BUDGET):\n",
    "                    used_iter += 1; improved=False\n",
    "                    for p in pos:\n",
    "                        best_id=patch[p].item()\n",
    "                        for cand in random.sample(range(VOC),512):\n",
    "                            patch[p]=cand\n",
    "                            am = msk.clone(); am[pos]=1\n",
    "                            cls=bert(input_ids=patch.unsqueeze(0),\n",
    "                                     attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "                            kth=torch.topk(cos_row(cls,CP),topk_loss).values[-1]\n",
    "                            loss=max(0.,(kth-torch.nn.functional.cosine_similarity(\n",
    "                                              cls,tgt_cls)[0]).item())\n",
    "                            if loss<best_loss:\n",
    "                                best_loss=loss; best_id=cand; adv_cls=cls; improved=True\n",
    "                            if loss==0: success=True; break\n",
    "                        patch[p]=best_id\n",
    "                        if success: break\n",
    "                    if success or not improved: break\n",
    "                used_L=TAIL_L\n",
    "            elif mtd==\"ggpp\":\n",
    "                adv_cls, used_L, used_iter = ggpp_full(\n",
    "                    tok, bert, ids, msk, CP, tgt_cls, tgt_txt,\n",
    "                    cap_L=TAIL_L, max_epoch=BUDGET, topk_loss=topk_loss)\n",
    "                kth = torch.topk(cos_row(adv_cls, CP), topk_loss).values[-1]\n",
    "                success = (kth <= torch.nn.functional.cosine_similarity(\n",
    "                                      adv_cls, tgt_cls)[0])\n",
    "            def de_run(L, max_iter, plateau):\n",
    "                pos=list(range(502, 502+L)); bounds=[(0, VOC-1)]*L\n",
    "                gens = max_iter//20 if max_iter else 1000\n",
    "                stop = [0]; best=[1e9]\n",
    "                def obj(v):\n",
    "                    v=[int(round(x)) for x in v]\n",
    "                    patch=ids.clone()\n",
    "                    for p,t in zip(pos,v): patch[p]=t\n",
    "                    am=msk.clone(); am[pos]=1\n",
    "                    cls=bert(input_ids=patch.unsqueeze(0),\n",
    "                             attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "                    kth=torch.topk(cos_row(cls,CP),topk_loss).values[-1]\n",
    "                    sim=torch.nn.functional.cosine_similarity(cls,tgt_cls)[0]\n",
    "                    return max(0.,(kth-sim).item())\n",
    "                def cb(xk,_):\n",
    "                    if not plateau: return False\n",
    "                    cur=obj(xk)\n",
    "                    stop[0] = 0 if cur < best[0] else stop[0]+1\n",
    "                    best[0] = min(best[0], cur)\n",
    "                    return stop[0] >= PATIENCE or cur == 0\n",
    "                res = differential_evolution(obj, bounds, popsize=20,\n",
    "                                             maxiter=gens, tol=0,\n",
    "                                             polish=False, seed=SEED,\n",
    "                                             callback=cb)\n",
    "                v=[int(round(x)) for x in res.x]\n",
    "                patch=ids.clone()\n",
    "                for p,t in zip(pos,v): patch[p]=t\n",
    "                am=msk.clone(); am[pos]=1\n",
    "                cls=bert(input_ids=patch.unsqueeze(0),\n",
    "                         attention_mask=am.unsqueeze(0)).last_hidden_state[:,0,:]\n",
    "                return cls, res.nfev, res.fun==0\n",
    "\n",
    "            if mtd==\"DE_fixed\":\n",
    "                adv_cls, used_iter, success = de_run(TAIL_L, BUDGET, plateau=False); used_L=TAIL_L\n",
    "            if mtd==\"DE_fixed_stop\":\n",
    "                adv_cls, used_iter, success = de_run(TAIL_L, None, plateau=True); used_L=TAIL_L\n",
    "            if mtd==\"DE_seq\":\n",
    "                for L in range(1, TAIL_L+1):\n",
    "                    cls,iters,ok = de_run(L, BUDGET, plateau=False)\n",
    "                    used_iter += iters\n",
    "                    if ok: adv_cls=cls; used_L=L; success=True; break\n",
    "                else: used_L=TAIL_L\n",
    "            if mtd==\"DE_seq_stop\":\n",
    "                for L in range(1, TAIL_L+1):\n",
    "                    cls,iters,ok = de_run(L, None, plateau=True)\n",
    "                    used_iter += iters\n",
    "                    if ok: adv_cls=cls; used_L=L; success=True; break\n",
    "                else: used_L=TAIL_L\n",
    "            sims = torch.cat([\n",
    "                cos_row(adv_cls.cpu(), CP),       # ★ 移到 CPU\n",
    "                torch.nn.functional.cosine_similarity(\n",
    "                    adv_cls.cpu(), tgt_cls.cpu())\n",
    "            ])\n",
    "            rank_a = (sims > sims[-1]).sum().item() + 1\n",
    "            d_mrr  = 1/rank_a - 1/rank_b\n",
    "            d_ndcg = ((dcg(rank_a) if rank_a<=20 else 0) -\n",
    "                      (dcg(rank_b) if rank_b<=20 else 0))\n",
    "            d_cos  = (torch.nn.functional.cosine_similarity(\n",
    "                        adv_cls, tgt_cls)[0] -\n",
    "                      torch.nn.functional.cosine_similarity(\n",
    "                        qcls, tgt_cls)[0]).item()\n",
    "\n",
    "            rec.append(dict(\n",
    "                top_k      = topk_loss,\n",
    "                method     = mtd,\n",
    "                success    = int(rank_a <= topk_loss),\n",
    "                token_used = used_L,\n",
    "                iter_used  = used_iter,\n",
    "                delta_mrr  = d_mrr,\n",
    "                delta_ndcg = d_ndcg,\n",
    "                delta_cos  = d_cos\n",
    "            ))\n",
    "        torch.cuda.empty_cache()   # 每 query 釋放\n",
    "\n",
    "    pd.DataFrame(rec).to_csv(f\"{save_dir}/msmarco/records.csv\", index=False)\n",
    "    print(f\"✓ {save_dir}  rows = {len(rec)}\")\n",
    "for K in (1, 10, 20):\n",
    "    run_all(topk_loss=K, save_dir=f\"results_top{K}\")\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00914820-2293-4f01-acca-a94a213c1abf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/colab_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 63/63 [00:15<00:00,  3.97it/s]\n",
      "Top-1:  14%|█████████▋                                                           | 14/100 [1:35:10<10:07:33, 423.88s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
      "Top-1: 100%|███████████████████████████████████████████████████████████████████████| 100/100 [9:43:08<00:00, 349.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ results_top1  rows=686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 63/63 [00:16<00:00,  3.90it/s]\n",
      "Top-10:  21%|██████████████▍                                                      | 21/100 [1:31:07<6:17:37, 286.81s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n",
      "Top-10: 100%|██████████████████████████████████████████████████████████████████████| 100/100 [7:30:15<00:00, 270.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ results_top10  rows=693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLS: 100%|██████████████████████████████████████████████████████████████████████████████| 63/63 [00:15<00:00,  3.98it/s]\n",
      "Top-20:  37%|█████████████████████████▌                                           | 37/100 [2:07:32<2:00:56, 115.18s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n",
      "Top-20: 100%|██████████████████████████████████████████████████████████████████████| 100/100 [6:13:48<00:00, 224.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ results_top20  rows=686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, random, math, warnings, tqdm, numpy as np, pandas as pd, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_DOCS     = 1_000\n",
    "N_Q        = 100\n",
    "TAIL_L     = 5\n",
    "BUDGET     = 150\n",
    "PATIENCE   = 20\n",
    "BATCH_CLS  = 16\n",
    "def cos_row(x, Y):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        x.expand_as(Y.to(x.device)), Y.to(x.device), dim=1)\n",
    "\n",
    "def dcg(rank): \n",
    "    return 1 / math.log2(rank + 1)\n",
    "def load_subset():\n",
    "    corpus  = load_dataset(\"BeIR/fever\",   \"corpus\",  split=\"corpus\")\n",
    "    queries = load_dataset(\"BeIR/fever\",   \"queries\", split=\"queries\")\n",
    "\n",
    "    docs = random.sample(list(corpus),  N_DOCS)\n",
    "    qs   = random.sample(list(queries), N_Q)\n",
    "\n",
    "    return [d[\"text\"] for d in docs], [q[\"text\"] for q in qs]\n",
    "\n",
    "def ggpp_full(tok, bert, ids, msk, CP, tgt_cls, tgt_txt,\n",
    "              cap_L=5, max_epoch=150, topk_loss=20):\n",
    "\n",
    "    body = tok(tgt_txt, add_special_tokens=False,\n",
    "               truncation=True, max_length=510)[\"input_ids\"]\n",
    "    body += [tok.unk_token_id] * max(0, cap_L - len(body))\n",
    "\n",
    "    base = bert(**tok(tgt_txt, return_tensors=\"pt\",\n",
    "                      truncation=True, max_length=512).to(DEVICE)\n",
    "               ).last_hidden_state[0, 0]\n",
    "    imp = []\n",
    "    for i in range(cap_L):\n",
    "        tmp = body.copy(); tmp[i] = tok.mask_token_id\n",
    "        tens = torch.tensor([tok.cls_token_id] + tmp + [tok.sep_token_id]\n",
    "                           ).unsqueeze(0).to(DEVICE)\n",
    "        emb = bert(input_ids=tens,\n",
    "                   attention_mask=torch.ones_like(tens)).last_hidden_state[0, 0]\n",
    "        imp.append(1 - torch.nn.functional.cosine_similarity(base, emb, dim=0).item())\n",
    "\n",
    "    prefix = [body[i] for i in np.argsort(imp)[-cap_L:]]\n",
    "\n",
    "    pos   = list(range(502, 502 + cap_L))\n",
    "    patch = ids.clone()\n",
    "    for p, v in zip(pos, prefix):\n",
    "        patch[p] = v\n",
    "    am = msk.clone(); am[pos] = 1\n",
    "    W  = bert.embeddings.word_embeddings.weight\n",
    "\n",
    "    def loss_fn(cls_vec):\n",
    "        kth = torch.topk(cos_row(cls_vec, CP), topk_loss).values[-1]\n",
    "        sim = torch.nn.functional.cosine_similarity(cls_vec, tgt_cls)[0]\n",
    "        return torch.relu(kth - sim)\n",
    "\n",
    "    best_cls  = bert(input_ids=patch.unsqueeze(0),\n",
    "                     attention_mask=am.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "    best_loss = loss_fn(best_cls).item()\n",
    "    used_iter = 0\n",
    "\n",
    "    for _ in range(max_epoch):\n",
    "        used_iter += 1\n",
    "        emb = bert.embeddings.word_embeddings(\n",
    "            patch.unsqueeze(0)).detach().clone().requires_grad_(True)\n",
    "        cls = bert(inputs_embeds=emb,\n",
    "                   attention_mask=am.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "        loss = loss_fn(cls)\n",
    "        loss.backward()\n",
    "        if loss.item() == 0:\n",
    "            best_cls = cls\n",
    "            break\n",
    "        grad  = emb.grad[0, pos]\n",
    "        score = torch.matmul(grad, W.t())\n",
    "        cand_ids = score.topk(5, largest=False, dim=1).indices.cpu()\n",
    "        improved = False\n",
    "        for mask in range(1, 1 << cap_L):\n",
    "            cand = patch.clone()\n",
    "            for i in range(cap_L):\n",
    "                if mask & (1 << i):\n",
    "                    cand[pos[i]] = random.choice(cand_ids[i]).item()\n",
    "            cls2 = bert(input_ids=cand.unsqueeze(0),\n",
    "                        attention_mask=am.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "            l2 = loss_fn(cls2).item()\n",
    "            if l2 < best_loss:\n",
    "                best_loss = l2\n",
    "                patch     = cand\n",
    "                best_cls  = cls2\n",
    "                improved  = True\n",
    "            if l2 == 0:\n",
    "                break\n",
    "        if not improved:\n",
    "            break\n",
    "    return best_cls, cap_L, used_iter\n",
    "def run_all(topk_loss: int, save_dir: str):\n",
    "\n",
    "    os.makedirs(f\"{save_dir}/fever\", exist_ok=True)\n",
    "\n",
    "    tok  = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\").eval().to(DEVICE)\n",
    "    VOC  = tok.vocab_size\n",
    "    enc  = lambda t: tok(t, padding=\"max_length\", truncation=True,\n",
    "                         max_length=512, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    docs, qs = load_subset()\n",
    "    CLS = []\n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm.tqdm(range(0, len(docs), BATCH_CLS), desc=\"CLS\"):\n",
    "            bt  = enc(docs[i:i + BATCH_CLS])\n",
    "            cls = bert(**bt).last_hidden_state[:, 0, :].cpu()\n",
    "            CLS.append(cls)\n",
    "            del bt\n",
    "            torch.cuda.empty_cache()\n",
    "    C_CLS = torch.cat(CLS)\n",
    "\n",
    "    METHODS = (\n",
    "        \"none\", \"random\", \"ggpp\",\n",
    "        \"DE_fixed\", \"DE_seq\", \"DE_fixed_stop\", \"DE_seq_stop\"\n",
    "    )\n",
    "\n",
    "    rec = []\n",
    "    for qtxt in tqdm.tqdm(qs, desc=f\"Top-{topk_loss}\"):\n",
    "        tgt      = random.randrange(len(docs))\n",
    "        tgt_txt  = docs[tgt]\n",
    "        if len(tok(tgt_txt)[\"input_ids\"]) > 510:\n",
    "            continue\n",
    "        tgt_cls  = C_CLS[tgt:tgt + 1].to(DEVICE)\n",
    "        CP       = C_CLS[[i for i in range(len(docs)) if i != tgt]]\n",
    "\n",
    "        qenc = enc(qtxt)\n",
    "        ids, msk = qenc[\"input_ids\"][0], qenc[\"attention_mask\"][0]\n",
    "        with torch.no_grad():\n",
    "            qcls = bert(**qenc).last_hidden_state[:, 0, :]\n",
    "\n",
    "        base = torch.cat([\n",
    "            cos_row(qcls, CP).cpu(),\n",
    "            torch.nn.functional.cosine_similarity(qcls.cpu(), tgt_cls.cpu())\n",
    "        ])\n",
    "        rank_b = (base > base[-1]).sum().item() + 1\n",
    "        for mtd in METHODS:\n",
    "            success   = False\n",
    "            used_L    = 0\n",
    "            used_iter = 0\n",
    "            adv_cls   = qcls\n",
    "\n",
    "            # ----- none -----\n",
    "            if mtd == \"none\":\n",
    "                used_L  = 0\n",
    "                success = rank_b <= topk_loss\n",
    "\n",
    "            # ----- random -----\n",
    "            elif mtd == \"random\":\n",
    "                best_loss = 1e9\n",
    "                for _ in range(BUDGET):\n",
    "                    patch = ids.clone()\n",
    "                    for p in range(502, 502 + TAIL_L):\n",
    "                        patch[p] = random.randrange(VOC)\n",
    "                    am  = msk.clone(); am[502:502 + TAIL_L] = 1\n",
    "                    cls = bert(input_ids=patch.unsqueeze(0),\n",
    "                               attention_mask=am.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "                    kth  = torch.topk(cos_row(cls, CP), topk_loss).values[-1]\n",
    "                    loss = max(0., (kth - torch.nn.functional.cosine_similarity(\n",
    "                        cls, tgt_cls)[0]).item())\n",
    "                    used_iter += 1\n",
    "                    if loss < best_loss:\n",
    "                        best_loss = loss\n",
    "                        adv_cls   = cls\n",
    "                        success   = (loss == 0)\n",
    "                    if success:\n",
    "                        break\n",
    "                used_L = TAIL_L\n",
    "\n",
    "            elif mtd == \"ggpp\":\n",
    "                adv_cls, used_L, used_iter = ggpp_full(\n",
    "                    tok, bert, ids, msk, CP, tgt_cls, tgt_txt,\n",
    "                    cap_L=TAIL_L, max_epoch=BUDGET, topk_loss=topk_loss\n",
    "                )\n",
    "                kth     = torch.topk(cos_row(adv_cls, CP), topk_loss).values[-1]\n",
    "                success = (kth <= torch.nn.functional.cosine_similarity(\n",
    "                    adv_cls, tgt_cls)[0])-\n",
    "            def de_run(L, max_iter, plateau):\n",
    "                pos    = list(range(502, 502 + L))\n",
    "                bounds = [(0, VOC - 1)] * L\n",
    "                gens   = max_iter // 20 if max_iter else 1000\n",
    "                stop   = [0]; best = [1e9]\n",
    "\n",
    "                def obj(v):\n",
    "                    v = [int(round(x)) for x in v]\n",
    "                    patch = ids.clone()\n",
    "                    for p, t in zip(pos, v):\n",
    "                        patch[p] = t\n",
    "                    am  = msk.clone(); am[pos] = 1\n",
    "                    cls = bert(input_ids=patch.unsqueeze(0),\n",
    "                               attention_mask=am.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "                    kth = torch.topk(cos_row(cls, CP), topk_loss).values[-1]\n",
    "                    sim = torch.nn.functional.cosine_similarity(cls, tgt_cls)[0]\n",
    "                    return max(0., (kth - sim).item())\n",
    "\n",
    "                def cb(xk, _):\n",
    "                    if not plateau:\n",
    "                        return False\n",
    "                    cur = obj(xk)\n",
    "                    stop[0] = 0 if cur < best[0] else stop[0] + 1\n",
    "                    best[0] = min(best[0], cur)\n",
    "                    return stop[0] >= PATIENCE or cur == 0\n",
    "\n",
    "                res = differential_evolution(\n",
    "                    obj, bounds, popsize=20, maxiter=gens,\n",
    "                    tol=0, polish=False, seed=SEED, callback=cb\n",
    "                )\n",
    "\n",
    "                v = [int(round(x)) for x in res.x]\n",
    "                patch = ids.clone()\n",
    "                for p, t in zip(pos, v):\n",
    "                    patch[p] = t\n",
    "                am  = msk.clone(); am[pos] = 1\n",
    "                cls = bert(input_ids=patch.unsqueeze(0),\n",
    "                           attention_mask=am.unsqueeze(0)).last_hidden_state[:, 0, :]\n",
    "                return cls, res.nfev, res.fun == 0\n",
    "            if mtd == \"DE_fixed\":\n",
    "                adv_cls, used_iter, success = de_run(TAIL_L, BUDGET, plateau=False)\n",
    "                used_L = TAIL_L\n",
    "            elif mtd == \"DE_fixed_stop\":\n",
    "                adv_cls, used_iter, success = de_run(TAIL_L, None, plateau=True)\n",
    "                used_L = TAIL_L\n",
    "            elif mtd == \"DE_seq\":\n",
    "                for L in range(1, TAIL_L + 1):\n",
    "                    cls, iters, ok = de_run(L, BUDGET, plateau=False)\n",
    "                    used_iter += iters\n",
    "                    if ok:\n",
    "                        adv_cls = cls; used_L = L; success = True; break\n",
    "                else:\n",
    "                    used_L = TAIL_L\n",
    "            elif mtd == \"DE_seq_stop\":\n",
    "                for L in range(1, TAIL_L + 1):\n",
    "                    cls, iters, ok = de_run(L, None, plateau=True)\n",
    "                    used_iter += iters\n",
    "                    if ok:\n",
    "                        adv_cls = cls; used_L = L; success = True; break\n",
    "                else:\n",
    "                    used_L = TAIL_L\n",
    "            sims = torch.cat([\n",
    "                cos_row(adv_cls.cpu(), CP),\n",
    "                torch.nn.functional.cosine_similarity(\n",
    "                    adv_cls.cpu(), tgt_cls.cpu())\n",
    "            ])\n",
    "            rank_a = (sims > sims[-1]).sum().item() + 1\n",
    "            d_mrr  = 1 / rank_a - 1 / rank_b\n",
    "            d_ndcg = ((dcg(rank_a) if rank_a <= 20 else 0) -\n",
    "                      (dcg(rank_b) if rank_b <= 20 else 0))\n",
    "            d_cos  = (torch.nn.functional.cosine_similarity(\n",
    "                adv_cls, tgt_cls)[0] -\n",
    "                      torch.nn.functional.cosine_similarity(\n",
    "                qcls, tgt_cls)[0]).item()\n",
    "\n",
    "            rec.append(dict(\n",
    "                top_k      = topk_loss,\n",
    "                method     = mtd,\n",
    "                success    = int(rank_a <= topk_loss),\n",
    "                token_used = used_L,\n",
    "                iter_used  = used_iter,\n",
    "                delta_mrr  = d_mrr,\n",
    "                delta_ndcg = d_ndcg,\n",
    "                delta_cos  = d_cos\n",
    "            ))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    pd.DataFrame(rec).to_csv(f\"{save_dir}/fever/records.csv\", index=False)\n",
    "    print(f\"✓ {save_dir}  rows={len(rec)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for K in (1, 10, 20):\n",
    "        run_all(topk_loss=K, save_dir=f\"results_top{K}\")\n",
    "        torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
